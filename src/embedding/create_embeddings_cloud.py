# src/embedding/create_embeddings_cloud.py

import pandas as pd
import numpy as np
import gc
import torch
import time
import os
import sys
import glob
from tqdm.auto import tqdm
from pathlib import Path
import torch.multiprocessing as mp
from azure.storage.blob import BlobServiceClient
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- PATH KONFIGUR√ÅCI√ì ---
# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

try:
    from configs import config
except ImportError as e:
    print(f"HIBA: configs modul import sikertelen: {e}")
    sys.exit(1)

# ==============================================================================
# === WORKER LOGIKA (A F≈ê SZKRIPTBE INTEGR√ÅLVA) ===
# ==============================================================================

class DocumentProcessor:
    """Felel≈ës egy dokumentum sz√∂veg√©nek darabol√°s√°√©rt (chunking)."""
    def __init__(self, chunk_size, chunk_overlap):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )
    def split_document(self, doc_id, text):
        if not isinstance(text, str) or not text.strip():
            return []
        chunks_text = self.text_splitter.split_text(text)
        return [{'doc_id': doc_id, 'text_chunk': chunk} for chunk in chunks_text]

class EmbeddingGenerator:
    """Felel≈ës az embedding modell bet√∂lt√©s√©√©rt √©s a sz√∂vegek embeddingj√©√©rt."""
    def __init__(self, model_name, batch_size, device):
        self.model = None
        self.model_name = model_name
        self.batch_size = batch_size
        self.device = device
    
    def load_model(self):
        if self.model is None:
            print(f"[{self.device}] Modell bet√∂lt√©se: {self.model_name}...")
            self.model = SentenceTransformer(self.model_name, device=self.device, trust_remote_code=True)
            print(f"[{self.device}] Modell bet√∂ltve.")
    
    def generate_embeddings(self, texts):
        with torch.autocast(device_type="cuda", dtype=torch.float16):
            return self.model.encode(
                texts,
                batch_size=self.batch_size,
                normalize_embeddings=True,
                show_progress_bar=False,
                convert_to_numpy=True
            ).astype(np.float32)

def process_data_chunk_on_worker(args):
    """
    Ez a f≈ë worker f√ºggv√©ny. Beolvas egy adatdarabot tartalmaz√≥ f√°jlt,
    legener√°lja az embeddingeket, √©s az eredm√©nyt egy kimeneti f√°jlba menti.
    """
    worker_id, device, model_name, batch_size, chunk_size, chunk_overlap, input_file_path, output_file_path = args
    
    try:
        print(f"[Worker {worker_id}]: Indul, feldolgozza: {input_file_path}")
        processor = DocumentProcessor(chunk_size, chunk_overlap)
        generator = EmbeddingGenerator(model_name, batch_size, device)
        generator.load_model()

        df_input = pd.read_parquet(input_file_path)
        
        all_chunks = []; original_docs_info = {}
        for _, row in df_input.iterrows():
            chunks = processor.split_document(row['doc_id'], row['text'])
            if chunks:
                all_chunks.extend(chunks)
                meta_cols = {col: row.get(col) for col in df_input.columns if col != 'text'}
                original_docs_info[row['doc_id']] = meta_cols

        if not all_chunks: return None

        df_chunks = pd.DataFrame(all_chunks)
        df_chunks['embedding'] = list(generator.generate_embeddings(df_chunks['text_chunk'].tolist()))
        agg_embeddings = df_chunks.groupby('doc_id')['embedding'].apply(lambda x: np.mean(np.vstack(x), axis=0))
        
        final_data = []
        for doc_id, doc_embedding in agg_embeddings.items():
            doc_info = original_docs_info.get(doc_id, {})
            doc_info['embedding'] = doc_embedding
            final_data.append(doc_info)
        
        result_df = pd.DataFrame(final_data)
        result_df.to_parquet(output_file_path)
        
        print(f"[Worker {worker_id}]: ‚úÖ Befejezte, eredm√©ny mentve: {output_file_path}")
        return output_file_path

    except Exception as e:
        import traceback
        print(f"\n\nCRITICAL ERROR IN WORKER {worker_id}: {e}")
        traceback.print_exc()
        return None
    finally:
        del processor, generator
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# ==============================================================================
# === F≈ê VEZ√âRL≈ê LOGIKA ===
# ==============================================================================

# === 1. FELH≈êS KONFIGUR√ÅCI√ì ===
# --- Azure ---
AZURE_CONNECTION_STRING = os.getenv("AZURE_CONNECTION_STRING")
AZURE_CONTAINER_NAME = "bhgy"  # CSER√âLD LE a saj√°t kont√©nered nev√©re!
INPUT_BLOB_NAME = config.CLEANED_PARQUET_DATA_PATH.name
OUTPUT_BLOB_NAME = "documents_with_embeddings_final.parquet"

# --- Modell √©s Feldolgoz√°s ---
MODEL_NAME = "Qwen/Qwen3-Embedding-0.6B"
BATCH_SIZE = 96
CHUNK_SIZE_CHARS = 5000
CHUNK_OVERLAP_CHARS = 500

# --- Lok√°lis √∫tvonalak a RunPodon bel√ºl ---
LOCAL_TEMP_DIR = Path("/workspace/embedding_job_temp")
LOCAL_INPUT_FILE = LOCAL_TEMP_DIR / INPUT_BLOB_NAME

# === 2. F≈ê VEZ√âRL≈ê F√úGGV√âNYEK ===

def download_from_azure(conn_str, container, blob_name, local_path):
    """Let√∂lt egy f√°jlt az Azure Blob Storage-b√≥l."""
    if not conn_str:
        raise ValueError("Azure connection string nincs be√°ll√≠tva (AZURE_CONNECTION_STRING)!")
    
    if local_path.exists():
        print(f"‚úÖ A bemeneti f√°jl m√°r l√©tezik lok√°lisan: {local_path}")
        return
    
    print(f"‚¨áÔ∏è F√°jl let√∂lt√©se: az://{container}/{blob_name} -> {local_path}")
    local_path.parent.mkdir(exist_ok=True, parents=True)
    
    blob_service_client = BlobServiceClient.from_connection_string(conn_str)
    blob_client = blob_service_client.get_blob_client(container=container, blob=blob_name)
    
    with open(local_path, "wb") as download_file:
        downloader = blob_client.download_blob(max_concurrency=4)
        download_file.write(downloader.readall())
    print("‚úÖ Let√∂lt√©s sikeres.")

def upload_to_azure(conn_str, container, blob_name, local_path):
    """Felt√∂lt egy f√°jlt az Azure Blob Storage-ba."""
    if not conn_str:
        raise ValueError("Azure connection string nincs be√°ll√≠tva (AZURE_CONNECTION_STRING)!")
        
    print(f"‚¨ÜÔ∏è F√°jl felt√∂lt√©se: {local_path} -> az://{container}/{blob_name}")
    blob_service_client = BlobServiceClient.from_connection_string(conn_str)
    blob_client = blob_service_client.get_blob_client(container=container, blob=blob_name)
    
    with open(local_path, "rb") as data:
        blob_client.upload_blob(data, overwrite=True, max_concurrency=4)
    print("‚úÖ Felt√∂lt√©s sikeres.")

# === 3. F≈ê FELDOLGOZ√ÅSI LOGIKA ===
def main():
    if not torch.cuda.is_available():
        print("‚ùå HIBA: CUDA nem el√©rhet≈ë. A szkript csak GPU-s k√∂rnyezetben futtathat√≥.")
        return
        
    NUM_GPUS = torch.cuda.device_count()
    print(f"üî• Tal√°lt GPU-k sz√°ma: {NUM_GPUS}")
    
    # 1. Bemeneti f√°jl let√∂lt√©se
    try:
        download_from_azure(AZURE_CONNECTION_STRING, AZURE_CONTAINER_NAME, INPUT_BLOB_NAME, LOCAL_INPUT_FILE)
    except Exception as e:
        print(f"‚ùå Hiba a let√∂lt√©s sor√°n: {e}")
        return
        
    # 2. F≈ë feldolgoz√°si ciklus
    print("\n--- Feldolgoz√°s ind√≠t√°sa t√∂bb GPU-n (F√°jl-alap√∫ IPC) ---")
    main_start_time = time.time()
    
    mp.set_start_method('spawn', force=True)
    
    df_full = pd.read_parquet(LOCAL_INPUT_FILE)
    total_rows = len(df_full)
    
    # Az adatok egyenl≈ë darabokra oszt√°sa a GPU-k k√∂z√∂tt
    # Ez a teljes adathalmazra vonatkozik, nem daraboljuk tov√°bb.
    # Minden worker egy nagy szeletet kap.
    df_chunks_for_gpus = np.array_split(df_full, NUM_GPUS)
    del df_full; gc.collect()

    worker_args = []
    temp_files_to_clean = []
    
    print("Ideiglenes f√°jlok l√©trehoz√°sa a workereknek...")
    for i, df_worker_chunk in enumerate(df_chunks_for_gpus):
        if not df_worker_chunk.empty:
            input_path = LOCAL_TEMP_DIR / f"input_worker_{i}.parquet"
            output_path = LOCAL_TEMP_DIR / f"output_worker_{i}.parquet"
            
            df_worker_chunk.to_parquet(input_path)
            temp_files_to_clean.append(input_path)
            
            worker_args.append((
                i, f'cuda:{i}', MODEL_NAME, BATCH_SIZE, 
                CHUNK_SIZE_CHARS, CHUNK_OVERLAP_CHARS, 
                input_path, output_path
            ))

    # 3. P√°rhuzamos feldolgoz√°s
    print("Worker processzek ind√≠t√°sa...")
    with mp.Pool(processes=NUM_GPUS) as pool:
        result_paths = pool.map(process_data_chunk_on_worker, worker_args)
    
    # 4. Eredm√©nyek √∂sszef≈±z√©se
    print("\nAdatok √∂sszef≈±z√©se az ideiglenes f√°jlokb√≥l...")
    valid_result_paths = [p for p in result_paths if p is not None]
    
    if valid_result_paths:
        final_df = pd.concat(
            [pd.read_parquet(f) for f in valid_result_paths], 
            ignore_index=True
        )
        
        # 5. V√©gs≈ë felt√∂lt√©s
        final_output_path = LOCAL_TEMP_DIR / "final_embeddings.parquet"
        final_df.to_parquet(final_output_path)
        temp_files_to_clean.extend(valid_result_paths)
        temp_files_to_clean.append(final_output_path)
        
        try:
            upload_to_azure(AZURE_CONNECTION_STRING, AZURE_CONTAINER_NAME, OUTPUT_BLOB_NAME, final_output_path)
        except Exception as e:
            print(f"‚ùå Hiba a v√©gs≈ë f√°jl felt√∂lt√©se sor√°n: {e}")
        
        print(f"\nüìä √ñsszesen {len(final_df):,} / {total_rows} dokumentum embeddingje j√∂tt l√©tre.")
    else:
        print("‚ö†Ô∏è Nem lett feldolgozva adat, egyetlen worker sem t√©rt vissza eredm√©nnyel.")

    # 6. Takar√≠t√°s
    print("Ideiglenes f√°jlok t√∂rl√©se...")
    for f in temp_files_to_clean:
        try:
            os.remove(f)
        except OSError:
            pass
    if LOCAL_TEMP_DIR.exists(): LOCAL_TEMP_DIR.rmdir()

    total_time_minutes = (time.time() - main_start_time) / 60
    print(f"\nüéâ Feldolgoz√°s befejezve {total_time_minutes:.2f} perc alatt.")

if __name__ == "__main__":
    main() 