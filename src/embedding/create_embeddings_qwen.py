#!/usr/bin/env python3
"""
Dense Index Builder for CourtRankRL
FAISS index √©p√≠t√©se Qwen3-Embedding-0.6B alap√∫ embeddingekkel.
MEGJEGYZ√âS: Ez a verzi√≥ egyszer≈±s√≠tett a stabilit√°s √©rdek√©ben.
"""

import json
import numpy as np
import faiss
import sys
import time
import logging
from pathlib import Path
from typing import List, Dict, Optional
import torch
from sentence_transformers import SentenceTransformer

# Projekt gy√∂k√©r hozz√°ad√°sa az import √∫thoz
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# Egyszer≈±s√≠tett config - ker√ºlj√ºk a komplex importokat
class SimpleConfig:
    """Egyszer≈±s√≠tett config oszt√°ly tesztel√©s c√©lj√°b√≥l."""
    PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
    DATA_DIR = PROJECT_ROOT / "data"
    PROCESSED_DATA_DIR = DATA_DIR / "processed"
    INDEX_DIR = DATA_DIR / "index"
    MODELS_DIR = DATA_DIR / "models"

    # F√°jlok
    CHUNKS_JSONL = PROCESSED_DATA_DIR / "chunks.jsonl"
    FAISS_INDEX_PATH = INDEX_DIR / "faiss_index_qwen.bin"
    CHUNK_ID_MAP_PATH = INDEX_DIR / "chunk_id_map_qwen.json"

    # Modell be√°ll√≠t√°sok
    MODEL_NAME = "Qwen/Qwen3-Embedding-0.6B"
    EMBEDDING_DIMENSION = 1024

    # Retrieval be√°ll√≠t√°sok
    LOGGING_LEVEL = logging.WARNING

# Haszn√°ljuk az egyszer≈±s√≠tett config-ot
config = SimpleConfig()

class DenseIndexBuilder:
    """FAISS index √©p√≠t√©se Qwen3-Embedding-0.6B embeddingekkel."""

    def __init__(self):
        self.logger = self._setup_logging()
        self.device = self._get_device()
        self.model = self._init_qwen_model()
        self.stats = {
            'total_chunks': 0,
            'processed_chunks': 0,
            'failed_chunks': 0,
            'start_time': time.time()
        }

    def _setup_logging(self) -> logging.Logger:
        """Logging be√°ll√≠t√°sa config alapj√°n."""
        logger = logging.getLogger('DenseIndexBuilder')
        level_str = str(config.LOGGING_LEVEL).upper()
        logger.setLevel(getattr(logging, level_str, logging.WARNING))

        # Egyszer≈±s√≠tett logging tesztel√©s c√©lj√°b√≥l
        if not logger.handlers:
            handler = logging.StreamHandler()
            # Egyszer≈± form√°z√°s
            formatter = logging.Formatter('%(levelname)s: %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _get_device(self) -> str:
        """Eszk√∂z kiv√°laszt√°sa (GPU ha el√©rhet≈ë, k√ºl√∂nben CPU)."""
        if torch.cuda.is_available():
            device = 'cuda'
            self.logger.info("CUDA el√©rhet≈ë, GPU haszn√°lata")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = 'mps'
            self.logger.info("MPS el√©rhet≈ë, Apple Silicon GPU haszn√°lata")
        else:
            device = 'cpu'
            self.logger.info("CPU haszn√°lata")

        return device

    def _init_qwen_model(self) -> SentenceTransformer:
        """Qwen3-Embedding-0.6B modell inicializ√°l√°sa."""
        model_name = "Qwen/Qwen3-Embedding-0.6B"

        try:
            self.logger.info(f"Qwen3-Embedding-0.6B modell bet√∂lt√©se: {model_name}")
            model = SentenceTransformer(model_name, trust_remote_code=True)

            # Embedding dimenzi√≥ ellen≈ërz√©se
            if hasattr(model, 'encode'):
                # Teszt encoding hogy m≈±k√∂dik-e
                test_embedding = model.encode("teszt sz√∂veg", normalize_embeddings=True)
                actual_dim = len(test_embedding)
                self.logger.info(f"Modell bet√∂ltve, embedding dimenzi√≥: {actual_dim}")

                # Ellen≈ërizz√ºk hogy egyezik-e a config-gal
                if actual_dim != config.EMBEDDING_DIMENSION:
                    self.logger.warning(f"Modell embedding dimenzi√≥ ({actual_dim}) nem egyezik a config-ban be√°ll√≠tottal ({config.EMBEDDING_DIMENSION})")
                    self.logger.info(f"Config friss√≠t√©se sz√ºks√©ges: EMBEDDING_DIMENSION = {actual_dim}")

            return model

        except Exception as e:
            raise RuntimeError(f"Qwen3-Embedding-0.6B modell bet√∂lt√©si hiba: {e}")

    def _validate_chunk_data(self, chunk: Dict) -> bool:
        """Chunk adat valid√°l√°sa."""
        required_fields = ['chunk_id', 'text']
        for field in required_fields:
            if field not in chunk:
                self.logger.warning(f"Hi√°nyz√≥ k√∂telez≈ë mez≈ë '{field}' a chunk-ban")
                return False

        if not chunk['text'].strip():
            self.logger.warning(f"√úres sz√∂veg a chunk-ban: {chunk['chunk_id']}")
            return False

        return True

    def _validate_embedding(self, embedding: np.ndarray, chunk_id: str) -> bool:
        """Embedding valid√°l√°sa."""
        if not isinstance(embedding, np.ndarray) or embedding.size == 0:
            self.logger.warning(f"√ârv√©nytelen embedding a chunk-hoz {chunk_id}: nem ndarray vagy √ºres")
            return False

        if len(embedding) != config.EMBEDDING_DIMENSION:
            self.logger.warning(f"√ârv√©nytelen embedding dimenzi√≥ a chunk-hoz {chunk_id}: elv√°rt {config.EMBEDDING_DIMENSION}, kapott {len(embedding)}")
            return False

        if not np.isfinite(embedding).all():
            self.logger.warning(f"√ârv√©nytelen embedding √©rt√©kek a chunk-hoz {chunk_id}: nem v√©ges sz√°mok")
            return False

        return True

    def _generate_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[np.ndarray]:
        """Embeddingek gener√°l√°sa batch-ben."""
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            self.logger.debug(f"Batch feldolgoz√°sa: {i}-{min(i + batch_size, len(texts))}")

            try:
                # Embedding gener√°l√°s normaliz√°l√°ssal koszinusz hasonl√≥s√°ghoz
                batch_embeddings = self.model.encode(
                    batch_texts,
                    normalize_embeddings=True,
                    convert_to_numpy=True,
                    device=self.device
                )

                # Valid√°l√°s
                for j, emb in enumerate(batch_embeddings):
                    if not self._validate_embedding(emb, f"batch_{i+j}"):
                        self.logger.warning(f"√ârv√©nytelen embedding a batch {i+j}. poz√≠ci√≥ban")
                        # Helyettes√≠tj√ºk null vektorral
                        batch_embeddings[j] = np.zeros(config.EMBEDDING_DIMENSION, dtype=np.float32)

                embeddings.extend(batch_embeddings)

            except Exception as e:
                self.logger.error(f"Batch embedding gener√°l√°si hiba: {e}")
                # Hib√°s batch eset√©n null vektorok hozz√°ad√°sa
                for _ in batch_texts:
                    embeddings.append(np.zeros(config.EMBEDDING_DIMENSION, dtype=np.float32))

        return embeddings

    def build_index(self, chunks_jsonl: Path, resume_from: Optional[int] = None):
        """FAISS index √©p√≠t√©se batch embeddingekkel."""
        print("=== DENSE INDEX √âP√çT√âS ===")
        print(f"Chunks f√°jl: {chunks_jsonl}")
        print(f"Modell: Qwen3-Embedding-0.6B")
        print(f"Eszk√∂z: {self.device}")
        print(f"Embedding dimenzi√≥: {config.EMBEDDING_DIMENSION}")

        # Batch m√©ret be√°ll√≠t√°sa
        batch_size = 32  # Qwen eset√©ben kisebb batch m√©ret optim√°lis
        print(f"Batch m√©ret: {batch_size}")

        # FAISS index el≈ëk√©sz√≠t√©s vagy bet√∂lt√©s (resume eset√©n)
        index = None
        chunk_id_map = {}

        if resume_from is not None and config.FAISS_INDEX_PATH.exists():
            print(f"Folytat√°s a {resume_from}. sort√≥l")
            index = faiss.read_index(str(config.FAISS_INDEX_PATH))
            with open(config.CHUNK_ID_MAP_PATH, 'r', encoding='utf-8') as f:
                chunk_id_map = json.load(f)
            current_row = len(chunk_id_map)
        else:
            current_row = 0

        batch_items: List[Dict] = []
        processed_lines = 0

        def flush_batch(items: List[Dict]):
            nonlocal index, chunk_id_map, current_row
            if not items:
                return

            valid_items = [it for it in items if self._validate_chunk_data(it)]
            if not valid_items:
                self.logger.warning("Nincs √©rv√©nyes elem a batch-ben")
                return

            # Embeddingek gener√°l√°sa
            texts = [it['text'] for it in valid_items]
            embeddings = self._generate_embeddings_batch(texts, batch_size=batch_size)

            # Align √©s valid√°ci√≥
            aligned: List[np.ndarray] = []
            aligned_ids: List[str] = []
            failed_embeddings = 0

            for i, (it, emb) in enumerate(zip(valid_items, embeddings)):
                cid = it['chunk_id']

                if not self._validate_embedding(emb, cid):
                    self.stats['failed_chunks'] += 1
                    failed_embeddings += 1
                    continue

                # M√°r normaliz√°lva van a model.encode-b√≥l (normalize_embeddings=True)
                aligned.append(emb.astype(np.float32))
                aligned_ids.append(cid)

            if not aligned:
                self.logger.warning(f"Nincs √©rv√©nyes embedding a batch-ben (sikertelen: {failed_embeddings})")
                return

            arr = np.vstack(aligned).astype(np.float32)
            if index is None:
                # IndexFlatIP haszn√°lata koszinusz hasonl√≥s√°ghoz (m√°r normaliz√°lt vektorok)
                # Egyszer≈±bb index t√≠pus tesztel√©s c√©lj√°b√≥l
                try:
                    index = faiss.IndexFlatIP(arr.shape[1])  # Inner product koszinusz hasonl√≥s√°ghoz
                    self.logger.info("IndexFlatIP haszn√°lata")
                except Exception as e:
                    self.logger.warning(f"IndexFlatIP hiba, IndexFlatL2 haszn√°lata: {e}")
                    index = faiss.IndexFlatL2(arr.shape[1])  # L2 distance fallback

            index.add(arr)

            for cid in aligned_ids:
                chunk_id_map[str(current_row)] = cid  # String kulcsok haszn√°lata konzisztenci√°√©rt
                current_row += 1

            self.stats['processed_chunks'] += len(aligned)

            # Progress riport
            elapsed = time.time() - self.stats['start_time']
            progress = self.stats['processed_chunks'] / max(1, self.stats['total_chunks']) * 100
            print(f"Progress: {self.stats['processed_chunks']:,}/{self.stats['total_chunks']:,} "
                  f"({progress:.1f}%) - Elapsed: {elapsed:.1f}s")

        # Total chunks sz√°mol√°sa
        with open(chunks_jsonl, 'r', encoding='utf-8') as f:
            self.stats['total_chunks'] = sum(1 for _ in f)

        print(f"√ñsszes feldolgozand√≥ chunk: {self.stats['total_chunks']:,}")

        # K√©r√©sek k√©sz√≠t√©se streamelve a chunks.jsonl-b≈ël
        with open(chunks_jsonl, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if resume_from is not None and line_num <= resume_from:
                    continue

                try:
                    d = json.loads(line.strip())
                    if not self._validate_chunk_data(d):
                        self.stats['failed_chunks'] += 1
                        continue

                    batch_items.append({
                        'chunk_id': d['chunk_id'],
                        'text': d['text'],
                    })

                    if len(batch_items) >= batch_size:
                        flush_batch(batch_items)
                        batch_items = []

                except json.JSONDecodeError as e:
                    self.logger.error(f"JSON dek√≥dol√°si hiba a {line_num}. sorban: {e}")
                    self.stats['failed_chunks'] += 1
                    continue

        # Marad√©k flush
        if batch_items:
            flush_batch(batch_items)

        if index is None or self.stats['processed_chunks'] == 0:
            raise ValueError("Nem k√©sz√ºlt el egyetlen embedding sem. Ellen≈ërizze a bemeneti adatokat.")

        # Ment√©s
        print(f"\nMent√©s: {config.FAISS_INDEX_PATH}")
        faiss.write_index(index, str(config.FAISS_INDEX_PATH))

        print(f"Chunk ID mapping ment√©s: {config.CHUNK_ID_MAP_PATH}")
        with open(config.CHUNK_ID_MAP_PATH, 'w', encoding='utf-8') as f:
            json.dump(chunk_id_map, f, ensure_ascii=False, indent=2)

        # Statisztik√°k
        elapsed = time.time() - self.stats['start_time']
        print("\n=== FELDOLGOZ√ÅS EREDM√âNYE ===")
        print(f"Sikeres embeddingek: {self.stats['processed_chunks']:,}")
        print(f"Sikertelen chunkok: {self.stats['failed_chunks']:,}")
        print(f"Teljes id≈ë: {elapsed:.2f}s")
        print(f"√Åtlagos feldolgoz√°si sebess√©g: {self.stats['processed_chunks'] / elapsed:.1f} chunk/s")

def main():
    """Build dense index from chunks with command line options."""
    import argparse

    parser = argparse.ArgumentParser(description='CourtRankRL Dense Index Builder - Qwen3')
    parser.add_argument('--resume-from', type=int, default=None,
                       help='Folytat√°s adott sorsz√°mt√≥l a chunks f√°jlban')
    parser.add_argument('--reset', action='store_true',
                       help='Reset √©s teljes √∫jj√°√©p√≠t√©s')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='Batch m√©ret fel√ºl√≠r√°sa (alap√©rtelmezett: 32)')
    parser.add_argument('--test-only', action='store_true',
                       help='Csak teszt futtat√°s, nem √©p√≠ti fel az indexet')

    args = parser.parse_args()

    if args.test_only:
        print("üß™ QWEN3 TESZT MOD")
        print("=" * 50)

        try:
            print("1. Modell bet√∂lt√©se...")
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B', trust_remote_code=True)
            print("‚úÖ Modell bet√∂ltve")

            print("2. Teszt embedding gener√°l√°s...")
            test_texts = [
                "Ez egy teszt mondat magyar b√≠r√≥s√°gi hat√°rozatokhoz.",
                "A szerz≈ëd√©s √©rv√©nytelen, mert hi√°nyzik az al√°√≠r√°s."
            ]
            embeddings = model.encode(test_texts, normalize_embeddings=True)
            print(f"‚úÖ Embeddingek gener√°lva: {len(embeddings)} db, dimenzi√≥: {len(embeddings[0])}")

            print("3. FAISS teszt...")
            import faiss
            import numpy as np
            arr = np.array(embeddings, dtype=np.float32)
            index = faiss.IndexFlatIP(arr.shape[1])
            index.add(arr)
            print("‚úÖ FAISS index l√©trehozva √©s felt√∂ltve")

            print("\n‚úÖ MINDEN TESZT SIKERES!")
            print("A Qwen3-Embedding-0.6B modell haszn√°latra k√©sz.")

        except Exception as e:
            print(f"‚ùå TESZT HIBA: {e}")
            import traceback
            traceback.print_exc()

        return

    if not config.CHUNKS_JSONL.exists():
        print(f"‚ùå HIBA: Chunks f√°jl nem tal√°lhat√≥: {config.CHUNKS_JSONL}")
        return

    # Reset m√≥d
    if args.reset:
        if config.FAISS_INDEX_PATH.exists():
            print(f"Reset: {config.FAISS_INDEX_PATH} t√∂rl√©se...")
            config.FAISS_INDEX_PATH.unlink(missing_ok=True)
        if config.CHUNK_ID_MAP_PATH.exists():
            print(f"Reset: {config.CHUNK_ID_MAP_PATH} t√∂rl√©se...")
            config.CHUNK_ID_MAP_PATH.unlink(missing_ok=True)
        print("Reset k√©sz.\n")

    try:
        builder = DenseIndexBuilder()
        builder.build_index(config.CHUNKS_JSONL, resume_from=args.resume_from)

        print("\n‚úÖ DENSE INDEX SIKERESEN FEL√âP√çTVE!")
        print(f"FAISS index: {config.FAISS_INDEX_PATH}")
        print(f"Chunk mapping: {config.CHUNK_ID_MAP_PATH}")

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  FELHASZN√ÅL√ìI MEGSZAK√çT√ÅS")
        print("Haszn√°lja --resume-from kapcsol√≥t a folytat√°shoz")
        print(f"P√©lda: python create_embeddings_qwen.py --resume-from {builder.stats['processed_chunks'] if 'builder' in locals() else 0}")
    except Exception as e:
        print(f"\n‚ùå HIBA: {e}")
        if 'builder' in locals():
            print(f"Hib√°s chunkok: {builder.stats['failed_chunks']}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()