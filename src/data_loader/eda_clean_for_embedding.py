import pandas as pd
import logging
import os
import sys
from pathlib import Path
from tqdm import tqdm

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Debug: config import ellen≈ërz√©se
configs_path = Path(project_root) / "configs"
if not configs_path.exists():
    print(f"HIBA: configs mappa nem tal√°lhat√≥: {configs_path}")
    print(f"Project root: {project_root}")
    print(f"Working directory: {os.getcwd()}")
    sys.exit(1)

# ===== CHUNKED CLEANING KONFIGUR√ÅCI√ìJA =====
ENABLE_UNIFIED_CSV = True  # Egyes√≠tett cleaned CSV l√©trehoz√°sa backwards compatibility-√©rt
USE_CHUNKED_INPUT = True   # Chunked input haszn√°lata (ha el√©rhet≈ë)

# --- Debugging ---
# print("--- Debugging sys.path ---") # Elt√°vol√≠tva
# print(f"Calculated project_root: {project_root}") # Elt√°vol√≠tva
# print("Current sys.path:") # Elt√°vol√≠tva
# for p in sys.path: # Elt√°vol√≠tva
#     print(f"- {p}") # Elt√°vol√≠tva
# print("--- End Debugging ---") # Elt√°vol√≠tva
# --- End Debugging ---

# Konfigur√°ci√≥ import√°l√°sa
try:
    from configs import config
    # logging.info("A 'configs.config' sikeresen import√°lva.") # Ez ink√°bb debug print volt
except ModuleNotFoundError as e:
    # Haszn√°ljunk logging-ot itt is, ha m√°r be√°ll√≠tottuk, b√°r itt m√©g nincs konfigb√≥l
    print(f"HIBA: Nem siker√ºlt import√°lni a 'configs.config'-ot. Hiba: {e}")
    print("Gy≈ëz≈ëdj meg r√≥la, hogy a 'configs' k√∂nyvt√°r l√©tezik a projekt gy√∂ker√©ben √©s tartalmazza a '__init__.py' √©s 'config.py' f√°jlokat.")
    sys.exit(1)

# ------------------------------------------------------------------
# Konfigur√°ci√≥ bet√∂lt√©se
# ------------------------------------------------------------------
IN_CSV_PATH = config.RAW_CSV_DATA_PATH # Bemeneti "nyers" CSV
OUT_CSV_PATH = config.CLEANED_CSV_DATA_PATH # Kimeneti "tiszt√≠tott" CSV
CSV_ENCODING = config.CSV_ENCODING # CSV k√≥dol√°s
CSV_INDEX = config.CSV_INDEX # √çrjuk-e az indexet a CSV-be
LOGGING_LEVEL = config.LOGGING_LEVEL # Loggol√°si szint
LOGGING_FORMAT = config.LOGGING_FORMAT # Loggol√°si form√°tum
MIN_TEXT_LENGTH = config.CLEANING_MIN_TEXT_LENGTH # Minim√°lis sz√∂veghossz a tiszt√≠t√°shoz

# Loggol√°s be√°ll√≠t√°sa a k√∂zponti konfigur√°ci√≥b√≥l
logging.basicConfig(
    level=LOGGING_LEVEL,
    format=LOGGING_FORMAT
)

def clean_single_chunk(chunk_df):
    """
    Egyetlen chunk adattiszt√≠t√°sa (az eredeti tiszt√≠t√°si logika alapj√°n).
    
    Returns:
        tuple: (cleaned_df, stats_dict)
    """
    initial_rows = len(chunk_df)
    
    if 'text' not in chunk_df.columns:
        logging.error("A chunk nem tartalmaz 'text' oszlopot!")
        return pd.DataFrame(), {'initial': initial_rows, 'final': 0, 'removed': initial_rows}
    
    # 1. NaN √©rt√©kek elt√°vol√≠t√°sa a 'text' oszlopb√≥l
    df_cleaned = chunk_df.dropna(subset=['text'])
    rows_after_nan = len(df_cleaned)
    nan_removed = initial_rows - rows_after_nan
    
    # 2. √úres vagy csak whitespace-t tartalmaz√≥ stringek elt√°vol√≠t√°sa
    df_cleaned = df_cleaned[df_cleaned['text'].astype(str).str.strip().astype(bool)]
    rows_after_empty = len(df_cleaned)
    empty_removed = rows_after_nan - rows_after_empty
    
    # 3. T√∫l r√∂vid sz√∂vegek elt√°vol√≠t√°sa
    df_cleaned = df_cleaned[df_cleaned['text'].str.len() >= MIN_TEXT_LENGTH]
    rows_after_short = len(df_cleaned)
    short_removed = rows_after_empty - rows_after_short
    
    # 4. Duplik√°lt doc_id-k elt√°vol√≠t√°sa (az els≈ë el≈ëfordul√°s megtart√°sa)
    duplicate_removed = 0
    if 'doc_id' in df_cleaned.columns:
        df_cleaned = df_cleaned.drop_duplicates(subset=['doc_id'], keep='first')
        rows_after_duplicates = len(df_cleaned)
        duplicate_removed = rows_after_short - rows_after_duplicates
    
    final_rows = len(df_cleaned)
    total_removed = initial_rows - final_rows
    
    stats = {
        'initial': initial_rows,
        'final': final_rows,
        'removed': total_removed,
        'nan_removed': nan_removed,
        'empty_removed': empty_removed,
        'short_removed': short_removed,
        'duplicate_removed': duplicate_removed
    }
    
    return df_cleaned.reset_index(drop=True), stats

def save_cleaned_chunk(cleaned_df, chunk_idx):
    """
    Cleaned chunk ment√©se CSV f√°jlba.
    """
    if cleaned_df.empty:
        return None
    
    chunk_filename = f"cleaned_chunk_{chunk_idx:04d}.csv"
    chunk_dir = config.OUT_DIR / "chunked_cleaned"
    chunk_dir.mkdir(parents=True, exist_ok=True)
    chunk_path = chunk_dir / chunk_filename
    
    cleaned_df.to_csv(chunk_path, index=CSV_INDEX, encoding=CSV_ENCODING, errors='replace')
    
    logging.info(f"Cleaned chunk mentve: {chunk_filename} ({len(cleaned_df):,} rekord)")
    return chunk_path

def process_chunked_input():
    """
    Chunked input f√°jlok feldolgoz√°sa (ha el√©rhet≈ëk).
    
    Returns:
        tuple: (success, cleaned_chunk_files, total_stats)
    """
    chunked_raw_dir = config.OUT_DIR / "chunked_raw"
    
    if not chunked_raw_dir.exists():
        return False, [], {}
    
    raw_chunk_files = sorted(list(chunked_raw_dir.glob("raw_chunk_*.csv")))
    
    if not raw_chunk_files:
        return False, [], {}
    
    logging.info(f"Chunked input feldolgoz√°s: {len(raw_chunk_files)} raw chunk tal√°lva")
    
    cleaned_chunk_files = []
    total_stats = {
        'total_initial': 0,
        'total_final': 0, 
        'total_removed': 0,
        'chunks_processed': 0
    }
    
    for chunk_idx, raw_chunk_path in enumerate(tqdm(raw_chunk_files, desc="Raw chunk-ok tiszt√≠t√°sa")):
        try:
            # Raw chunk bet√∂lt√©se
            raw_df = pd.read_csv(raw_chunk_path, encoding=CSV_ENCODING)
            
            # Tiszt√≠t√°s
            cleaned_df, chunk_stats = clean_single_chunk(raw_df)
            
            # Statisztik√°k friss√≠t√©se
            total_stats['total_initial'] += chunk_stats['initial']
            total_stats['total_final'] += chunk_stats['final']
            total_stats['total_removed'] += chunk_stats['removed']
            total_stats['chunks_processed'] += 1
            
            # Cleaned chunk ment√©se
            if not cleaned_df.empty:
                cleaned_chunk_path = save_cleaned_chunk(cleaned_df, chunk_idx)
                if cleaned_chunk_path:
                    cleaned_chunk_files.append(cleaned_chunk_path)
            else:
                logging.warning(f"Chunk {chunk_idx} teljesen √ºres a tiszt√≠t√°s ut√°n")
                
        except Exception as e:
            logging.error(f"Hiba a raw chunk feldolgoz√°s√°ban ({raw_chunk_path}): {e}")
            continue
    
    return True, cleaned_chunk_files, total_stats

def process_unified_input():
    """
    Egyes√≠tett CSV input feldolgoz√°sa (fallback vagy ha nincs chunked input).
    
    Returns:
        tuple: (success, cleaned_data_df, total_stats)
    """
    if not IN_CSV_PATH.exists():
        return False, pd.DataFrame(), {}
    
    logging.info("Egyes√≠tett CSV feldolgoz√°s (fallback mode)")
    
    try:
        df = pd.read_csv(IN_CSV_PATH, encoding=CSV_ENCODING)
        cleaned_df, stats = clean_single_chunk(df)
        
        total_stats = {
            'total_initial': stats['initial'],
            'total_final': stats['final'],
            'total_removed': stats['removed'],
            'chunks_processed': 1
        }
        
        return True, cleaned_df, total_stats
        
    except Exception as e:
        logging.error(f"Hiba az egyes√≠tett CSV bet√∂lt√©s√©ben: {e}")
        return False, pd.DataFrame(), {}

# ------------------------------------------------------------------
# F≈ë v√©grehajt√°si blokk
# ------------------------------------------------------------------
def main():
    """
    F≈ë f√ºggv√©ny az adatok tiszt√≠t√°s√°hoz embedding gener√°l√°s el≈ëtt.
    
    √öJDONS√ÅG: Chunked input t√°mogat√°ssal √©s memory-safe feldolgoz√°ssal.
    """
    logging.info("Chunked adattiszt√≠t√≥ szkript ind√≠t√°sa...")
    
    cleaned_chunk_files = []
    unified_cleaned_df = pd.DataFrame()
    total_stats = {}
    
    # ===== 1. CHUNKED INPUT FELDOLGOZ√ÅS (PRIORIT√ÅS) =====
    if USE_CHUNKED_INPUT:
        success, cleaned_chunk_files, total_stats = process_chunked_input()
        
        if success:
            logging.info("‚úÖ Chunked input feldolgoz√°s sikeres")
        else:
            logging.info("‚ö†Ô∏è  Nincs chunked input - fallback egyes√≠tett CSV-re")
    
    # ===== 2. EGYES√çTETT INPUT FELDOLGOZ√ÅS (FALLBACK) =====
    if not cleaned_chunk_files:  # Ha nincs chunked input vagy nem siker√ºlt
        success, unified_cleaned_df, total_stats = process_unified_input()
        
        if not success:
            logging.error("Nincs el√©rhet≈ë input adat (sem chunked, sem egyes√≠tett)")
            logging.error("K√©rlek, el≈ësz√∂r futtasd a `preprocess_documents.py` szkriptet!")
            raise SystemExit("Hiba: Nincs bemeneti adat")
    
    # ===== 3. √ñSSZEGZ≈ê STATISZTIK√ÅK =====
    if total_stats:
        removal_percentage = (total_stats['total_removed'] / total_stats['total_initial'] * 100) if total_stats['total_initial'] > 0 else 0
        
        logging.info("üìä TISZT√çT√ÅSI √ñSSZEFOGLAL√ì:")
        logging.info(f"  Input rekordok: {total_stats['total_initial']:,}")
        logging.info(f"  Output rekordok: {total_stats['total_final']:,}")
        logging.info(f"  Elt√°vol√≠tott rekordok: {total_stats['total_removed']:,} ({removal_percentage:.1f}%)")
        logging.info(f"  Feldolgozott chunk-ok: {total_stats['chunks_processed']}")
    
    # ===== 4. OPCION√ÅLIS EGYES√çTETT CLEANED CSV =====
    unified_csv_created = False
    if ENABLE_UNIFIED_CSV:
        logging.info("Egyes√≠tett cleaned CSV l√©trehoz√°sa backwards compatibility-√©rt...")
        
        try:
            if cleaned_chunk_files:
                # Chunk-okb√≥l egyes√≠t√©s
                all_cleaned_dfs = []
                for chunk_path in tqdm(cleaned_chunk_files, desc="Cleaned chunk-ok egyes√≠t√©se"):
                    chunk_df = pd.read_csv(chunk_path, encoding=CSV_ENCODING)
                    if not chunk_df.empty:
                        all_cleaned_dfs.append(chunk_df)
                
                if all_cleaned_dfs:
                    final_cleaned_df = pd.concat(all_cleaned_dfs, ignore_index=True)
                else:
                    final_cleaned_df = pd.DataFrame()
                    
            elif not unified_cleaned_df.empty:
                # M√°r van egyes√≠tett DataFrame
                final_cleaned_df = unified_cleaned_df
            else:
                final_cleaned_df = pd.DataFrame()
            
            # Egyes√≠tett CSV ment√©se
            if not final_cleaned_df.empty:
                OUT_CSV_PATH.parent.mkdir(parents=True, exist_ok=True)
                final_cleaned_df.to_csv(OUT_CSV_PATH, encoding=CSV_ENCODING, index=CSV_INDEX)
                unified_csv_created = True
                logging.info(f"Egyes√≠tett cleaned CSV mentve: {OUT_CSV_PATH} ({len(final_cleaned_df):,} sor)")
            else:
                logging.warning("Nincs tiszt√≠tott adat az egyes√≠tett CSV-hez")
                
        except Exception as e:
            logging.error(f"Hiba az egyes√≠tett cleaned CSV l√©trehoz√°s√°ban: {e}")
    
    # ===== 5. V√âGS≈ê √úZENETEK =====
    print(f"\n‚úÖ CHUNKED CLEANING BEFEJEZVE!")
    
    if cleaned_chunk_files:
        print(f"üìÅ Cleaned chunk f√°jlok ({len(cleaned_chunk_files)} db): {config.OUT_DIR / 'chunked_cleaned'}")
    
    if unified_csv_created:
        print(f"üìÑ Egyes√≠tett cleaned CSV: {OUT_CSV_PATH}")
        print(f"üí° K√∂vetkez≈ë scriptek haszn√°lhatj√°k az egyes√≠tett CSV-t vagy a chunk-okat")
    else:
        print(f"‚ö†Ô∏è  Nincs egyes√≠tett cleaned CSV - csak chunk-ok (mem√≥ria k√≠m√©l√©s)")
    
    if total_stats:
        print(f"üìä {total_stats['total_final']:,} tiszt√≠tott rekord ({total_stats['total_removed']:,} elt√°vol√≠tva)")
        print(f"üöÄ K√∂vetkez≈ë l√©p√©s: embedding gener√°l√°s chunked m√≥dban")
    
    logging.info("Chunked adattiszt√≠t√≥ szkript befejezve.")

if __name__ == '__main__':
    main()