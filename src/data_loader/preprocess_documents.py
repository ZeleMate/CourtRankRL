# Ez a szkript felel≈ës a nyers dokumentumok (RTF, DOCX) √©s a hozz√°juk tartoz√≥
# JSON metaadatok feldolgoz√°s√°√©rt, majd egyetlen CSV f√°jlba t√∂rt√©n≈ë ment√©s√©√©rt.
import pandas as pd
import json
import re
from pathlib import Path
from tqdm import tqdm
import sys
import os
import logging
from striprtf.striprtf import rtf_to_text
import csv
from docx import Document
from bs4 import BeautifulSoup
import html
from typing import Set

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Konfigur√°ci√≥ import√°l√°sa
try:
    from configs import config
except ImportError as e:
    print(f"HIBA: configs modul import sikertelen: {e}")
    sys.exit(1)

# Loggol√°s be√°ll√≠t√°sa
logging.basicConfig(level=config.LOGGING_LEVEL, format=config.LOGGING_FORMAT)

# Magyar stopword lista
HUNGARIAN_STOPWORDS = {
    # Alapvet≈ë stopword√∂k
    'a', 'az', '√©s', 'vagy', 'de', 'hogy', 'mint', 'mely', 'aki', 'ami', 'ez', 'az',
    'ezek', 'azok', 'ezt', 'azt', 'ennek', 'annak', 'erre', 'arra', 'ide', 'oda',
    'itt', 'ott', 'itt', 'ott', 'itt', 'ott', 'itt', 'ott', 'itt', 'ott',
    
    # N√©vel≈ëk √©s determin√°nsok
    'egy', 'minden', 'valamennyi', 'n√©h√°ny', 'sok', 'kev√©s', 't√∂bb', 'kevesebb',
    'annyi', 'ennyi', 'annyi', 'ennyi', 'annyi', 'ennyi', 'annyi', 'ennyi',
    
    # N√©vm√°sok
    '√©n', 'te', '≈ë', 'mi', 'ti', '≈ëk', 'magam', 'magad', 'maga', 'magunk', 'magatok', 'maguk',
    '√∂n', '√∂n√∂k', '√∂nmaga', '√∂nmaguk', 'mindenki', 'valaki', 'senki', 'b√°rki',
    
    # El√∂lj√°r√≥k √©s k√∂t≈ësz√≥k
    'alatt', '√°ltal', 'bel√ºl', 'ellen', 'el≈ëtt', 'felett', 'helyett', 'kereszt√ºl',
    'k√≠v√ºl', 'mellett', 'm√∂g√∂tt', 'n√©lk√ºl', 'szerint', 'ut√°n', 'valamint', 'v√©g√ºl',
    
    # Hat√°roz√≥sz√≥k
    'itt', 'ott', 'hol', 'mikor', 'hogyan', 'mi√©rt', 'milyen', 'mennyi', 'ahol',
    'amikor', 'ahogy', 'am√≠g', 'm√≠g', 'mialatt', 'mi√≥ta', 'mi√≥ta', 'mivel',
    
    # Seg√©dig√©k √©s m√≥dos√≠t√≥sz√≥k
    'van', 'vannak', 'volt', 'voltak', 'lesz', 'lesznek', 'lenni', 'lenni',
    'fog', 'fognak', 'tud', 'tudnak', 'akar', 'akarnak', 'kell', 'kellenek',
    'szabad', 'szabadnak', 'musz√°j', 'musz√°jnak', 'lehet', 'lehetnek',
    
    # Gyakori ig√©k
    'csin√°l', 'csin√°lnak', 'tesz', 'tennek', 'ad', 'adnak', 'vesz', 'vesznek',
    'j√∂n', 'j√∂nnek', 'megy', 'mennek', '√°ll', '√°llnak', '√ºl', '√ºlnek',
    'fekszik', 'fekszenek', 'alszik', 'alszanak', 'eszik', 'esznek',
    
    # M√≥dos√≠t√≥sz√≥k √©s k√∂t≈ësz√≥k
    'is', 'sem', 'csak', 'm√©g', 'm√°r', 'most', 'akkor', 'soha', 'mindig',
    'n√©ha', 'gyakran', 'ritk√°n', 'hamar', 'k√©s≈ën', 'kor√°n', 'k√©s≈ëbb',
    'el≈ëbb', 'ut√≥bb', 'azut√°n', 'azel≈ëtt', 'ekkor', 'akkor', 'mikor',
    
    # Sz√°mok √©s mennyis√©gek
    'egy', 'kett≈ë', 'h√°rom', 'n√©gy', '√∂t', 'hat', 'h√©t', 'nyolc', 'kilenc', 't√≠z',
    'sz√°z', 'ezer', 'milli√≥', 'milli√°rd', 'els≈ë', 'm√°sodik', 'harmadik',
    
    # Id≈ëtartamok
    'ma', 'tegnap', 'holnap', 'id√©n', 'tavaly', 'j√∂v≈ëre', 'h√©t', 'h√≥nap', '√©v',
    'perc', '√≥ra', 'nap', 'h√©t', 'h√≥nap', '√©v', '√©vsz√°zad', '√©vezred',
    
    # Helyek √©s ir√°nyok
    'itt', 'ott', 'hol', 'ahol', 'ide', 'oda', 'erre', 'arra', 'fel', 'le',
    'fel√ºl', 'alul', 'kint', 'bent', 'k√ºls≈ë', 'bels≈ë', 'jobb', 'bal',
    
    # M√≥dos√≠t√≥sz√≥k √©s jelz≈ëk
    'nagy', 'kicsi', 'hossz√∫', 'r√∂vid', 'sz√©les', 'keskeny', 'magas', 'alacsony',
    'vastag', 'v√©kony', 'er≈ës', 'gyenge', 'kem√©ny', 'puha', 'hideg', 'meleg',
    '√∫j', 'r√©gi', 'fiatal', '√∂reg', 'sz√©p', 'cs√∫nya', 'j√≥', 'rossz',
    
    # Gyakori szavak a jogi sz√∂vegekben
    'szerint', 'alapj√°n', '√©rtelm√©ben', 'megfelel≈ëen', 'szem el≈ëtt', 'figyelembe',
    'v√©ve', 'tekintettel', 'kifoly√≥lag', 'ok√°n', 'miatt', 'folyt√°n', 'eredm√©nyek√©nt',
    'k√∂vetkezt√©ben', 'alapj√°n', 'szerint', 'megfelel≈ëen', 'megfelel≈ë', 'megfelel',
    'megfelelnek', 'megfelel≈ë', 'megfelel≈ëen', 'megfelel≈ë', 'megfelel≈ëen',
    
    # √úres szavak √©s gyakori kifejez√©sek
    'sz√≥val', 'vagyis', 'teh√°t', 'ugyanis', 'mivel', 'mert', 'mert', 'mert',
    'ugyanakkor', 'azonban', 'viszont', 'ellenben', 'ellenkez≈ëleg', 'ford√≠tva',
    'egy√©bk√©nt', 'egy√©bk√©nt', 'egy√©bk√©nt', 'egy√©bk√©nt', 'egy√©bk√©nt',
    
    # R√∂vid√≠t√©sek √©s gyakori kifejez√©sek
    'stb', 'st', 'v√∂', 'l√°sd', 'ld', 'pl', 'p√©ld√°ul', 'p√©ld√°ul', 'p√©ld√°ul',
    'stb', 'st', 'v√∂', 'l√°sd', 'ld', 'pl', 'p√©ld√°ul', 'p√©ld√°ul', 'p√©ld√°ul'
}

def remove_hungarian_stopwords(text: str, stopwords: Set[str] | None = None) -> tuple[str, int]:
    """
    Magyar stopword√∂k elt√°vol√≠t√°sa a sz√∂vegb≈ël.
    
    Args:
        text: A tiszt√≠tand√≥ sz√∂veg
        stopwords: Stopword halmaz (alap√©rtelmezett: HUNGARIAN_STOPWORDS)
    
    Returns:
        Tuple: (A stopword√∂k n√©lk√ºli sz√∂veg, elt√°vol√≠tott stopword√∂k sz√°ma)
    """
    if not isinstance(text, str) or not text.strip():
        return "", 0
    
    if stopwords is None:
        stopwords = HUNGARIAN_STOPWORDS
    
    # Sz√∂veg szavakra bont√°sa, megtartva a sz√≥k√∂z√∂ket √©s √≠r√°sjeleket
    words = text.split()
    filtered_words = []
    removed_count = 0
    
    for word in words:
        # Sz√≥ tiszt√≠t√°sa (√≠r√°sjelek elt√°vol√≠t√°sa a hasonl√≠t√°s el≈ëtt)
        clean_word = re.sub(r'[^\w√°√©√≠√≥√∂≈ë√∫√º≈±√Å√â√ç√ì√ñ≈ê√ö√ú≈∞]', '', word.lower())
        
        # Ha a tiszt√≠tott sz√≥ nincs a stopword list√°ban, megtartjuk
        if clean_word and clean_word not in stopwords:
            filtered_words.append(word)
        elif clean_word:
            removed_count += 1
    
    return ' '.join(filtered_words), removed_count

def clean_text_for_embedding(text: str, remove_stopwords: bool = True) -> tuple[str, int]:
    """
    Sz√∂veg alapos tiszt√≠t√°sa embedding gener√°l√°s el≈ëtt.
    Elt√°vol√≠tja a HTML tageket, speci√°lis karaktereket, URL-eket, normaliz√°lja a whitespace-t,
    √©s opcion√°lisan kisz≈±ri a magyar stopword√∂ket.
    
    Args:
        text: A tiszt√≠tand√≥ sz√∂veg
        remove_stopwords: Ha True, elt√°vol√≠tja a magyar stopword√∂ket
    
    Returns:
        Tuple: (A tiszt√≠tott sz√∂veg, elt√°vol√≠tott stopword√∂k sz√°ma)
    """
    if not isinstance(text, str) or not text.strip():
        return "", 0
    
    # HTML entit√°sok dek√≥dol√°sa (pl. &amp; -> &)
    try:
        text = html.unescape(text)
    except Exception:
        pass # Ha hiba t√∂rt√©nik, a nyers sz√∂veggel megy√ºnk tov√°bb

    # HTML tagek elt√°vol√≠t√°sa BeautifulSoup-pal
    try:
        soup = BeautifulSoup(text, 'html.parser')
        text = soup.get_text(separator=' ')
    except Exception:
        # Ha a BeautifulSoup hib√°t dob, egyszer≈± regex-szel pr√≥b√°ljuk
        text = re.sub(r'<[^>]+>', '', text)
    
    # URL-ek, email c√≠mek elt√°vol√≠t√°sa
    text = re.sub(r'http\S+|www\S+|https\S+|\S+@\S+', '', text, flags=re.MULTILINE)

    # Null byte √©s egy√©b nem sz√∂veges vez√©rl≈ë karakterek elt√°vol√≠t√°sa
    text = text.replace('\x00', '')
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # RTF specifikus maradv√°nyok elt√°vol√≠t√°sa, amik a konverzi√≥ ut√°n maradhatnak
    text = re.sub(r'\\[a-zA-Z]+\d*\s?', '', text)
    text = re.sub(r'[{}]', '', text)
    
    # Egys√©ges kisbet≈±re alak√≠t√°s
    text = text.lower()
    
    # T√∂bbsz√∂r√∂s sz√≥k√∂z√∂k, tabul√°torok, √∫j sorok cser√©je egyetlen sz√≥k√∂zre
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Magyar stopword√∂k elt√°vol√≠t√°sa (opcion√°lis)
    removed_count = 0
    if remove_stopwords:
        text, removed_count = remove_hungarian_stopwords(text)
        # √öjabb whitespace normaliz√°l√°s a stopword elt√°vol√≠t√°s ut√°n
        text = re.sub(r'\s+', ' ', text).strip()
    
    return text, removed_count

# Adat k√∂nyvt√°r el√©r√©si √∫tja a konfigur√°ci√≥b√≥l
root_dir_to_scan = project_root / 'data'
paths = list(root_dir_to_scan.rglob('*'))

# A feldolgozott rekordokat egyetlen list√°ban gy≈±jtj√ºk
all_records = []
total_records = 0
stopwords_removed_count = 0  # Statisztika a stopword elt√°vol√≠t√°sr√≥l

# T√°mogatott sz√∂vegf√°jl kiterjeszt√©sek
SUPPORTED_EXTENSIONS = tuple(ext.lower() for ext in config.SUPPORTED_TEXT_EXTENSIONS)

logging.info(f"Feldolgoz√°s kezd√©se, c√©l: egyetlen CSV f√°jl.")
logging.info(f"Tal√°lva {len(paths):,} potenci√°lis f√°jl")
if config.REMOVE_HUNGARIAN_STOPWORDS:
    logging.info("üá≠üá∫ Magyar stopword sz≈±r√©s AKT√çV - a stopword√∂k elt√°vol√≠t√°sra ker√ºlnek a sz√∂vegekb≈ël")
else:
    logging.info("üá≠üá∫ Magyar stopword sz≈±r√©s INAKT√çV - a stopword√∂k megmaradnak a sz√∂vegekben")

for path in tqdm(paths, desc="Dokumentumf√°jlok feldolgoz√°sa"):
    if path.is_file() and path.suffix.lower() in SUPPORTED_EXTENSIONS:
        base_filename = path.stem
        text_path = path
        json_filename = base_filename + '.RTF_OBH.JSON'
        json_path = path.with_name(json_filename)

        text_content = ""
        if text_path.suffix.lower() == '.rtf':
            try:
                with open(text_path, 'r', encoding='utf-8', errors='ignore') as f:
                    rtf_content = f.read()
                text_content = rtf_to_text(rtf_content, errors="ignore")
            except Exception as e:
                logging.warning(f"Nem siker√ºlt kinyerni a sz√∂veget az RTF f√°jlb√≥l ({text_path}): {e}")
        elif text_path.suffix.lower() == '.docx':
            try:
                doc = Document(str(text_path))
                text_content = ' \n'.join(para.text for para in doc.paragraphs if para.text.strip())
            except Exception as e:
                logging.warning(f"Nem siker√ºlt kinyerni a sz√∂veget a DOCX f√°jlb√≥l ({text_path}): {e}")
        
        # A kinyert nyers sz√∂veg azonnali tiszt√≠t√°sa
        cleaned_text_content, removed_stopwords = clean_text_for_embedding(
            text_content, 
            remove_stopwords=config.REMOVE_HUNGARIAN_STOPWORDS
        )
        stopwords_removed_count += removed_stopwords

        # Csak akkor dolgozzuk fel a rekordot, ha a tiszt√≠t√°s ut√°n is maradt √©rt√©kelhet≈ë sz√∂veg
        if len(cleaned_text_content) < config.CLEANING_MIN_TEXT_LENGTH:
            logging.debug(f"Dokumentum √°tugorva, mert a tiszt√≠tott sz√∂veg t√∫l r√∂vid: {path.name}")
            continue

        extracted_metadata = {}
        all_related_ugyszam = []
        all_related_birosag = []

        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as jf:
                    metadata_dict = json.load(jf)
                    if 'List' in metadata_dict and isinstance(metadata_dict['List'], list) and len(metadata_dict['List']) > 0:
                        extracted_metadata = metadata_dict['List'][0]
                        if 'KapcsolodoHatarozatok' in extracted_metadata and isinstance(extracted_metadata['KapcsolodoHatarozatok'], list):
                            for related_case in extracted_metadata['KapcsolodoHatarozatok']:
                                if isinstance(related_case, dict):
                                    all_related_ugyszam.append(related_case.get('KapcsolodoUgyszam'))
                                    all_related_birosag.append(related_case.get('KapcsolodoBirosag'))
                                else:
                                    logging.warning(f"A KapcsolodoHatarozatok lista egyik eleme nem sz√≥t√°r a {json_path} f√°jlban.")
                                    all_related_ugyszam.append(None)
                                    all_related_birosag.append(None)
                        if 'Jogszabalyhelyek' in extracted_metadata and not isinstance(extracted_metadata['Jogszabalyhelyek'], (str, int, float, bool)):
                            extracted_metadata['Jogszabalyhelyek'] = json.dumps(extracted_metadata['Jogszabalyhelyek'], ensure_ascii=False)
                        if 'KapcsolodoHatarozatok' in extracted_metadata and not isinstance(extracted_metadata['KapcsolodoHatarozatok'], (str, int, float, bool)):
                            extracted_metadata['KapcsolodoHatarozatok'] = json.dumps(extracted_metadata['KapcsolodoHatarozatok'], ensure_ascii=False)
            except json.JSONDecodeError:
                logging.warning(f"Nem siker√ºlt dek√≥dolni a JSON f√°jlt: {json_path}")
            except Exception as e:
                logging.warning(f"Hiba a JSON f√°jl feldolgoz√°sa k√∂zben ({json_path}): {e}")

        birosag_from_path = None
        try:
            abs_root_dir = root_dir_to_scan.resolve()
            abs_path = path.resolve()
            if abs_path.is_relative_to(abs_root_dir):
                 rel_parts = abs_path.relative_to(abs_root_dir).parts
                 if len(rel_parts) > 1:
                    birosag_from_path = rel_parts[0]
        except Exception as e_path:
             logging.warning(f"V√°ratlan hiba a b√≠r√≥s√°g nev√©nek √∫tvonalb√≥l t√∂rt√©n≈ë kinyer√©se k√∂zben ({path}): {e_path}")

        record = {
            'text': cleaned_text_content,
            **extracted_metadata,
            'AllKapcsolodoUgyszam': json.dumps(all_related_ugyszam, ensure_ascii=False) if all_related_ugyszam else None,
            'AllKapcsolodoBirosag': json.dumps(all_related_birosag, ensure_ascii=False) if all_related_birosag else None,
        }
        record['doc_id'] = extracted_metadata.get('Azonosito', base_filename)
        record['birosag'] = extracted_metadata.get('MeghozoBirosag', birosag_from_path)
        
        record.pop('Szoveg', None)
        record.pop('RezumeSzovegKornyezet', None)
        record.pop('DownloadLink', None)
        record.pop('metadata', None)

        all_records.append(record)
        total_records += 1

# ===== EGYES√çTETT, TISZT√çTOTT PARQUET L√âTREHOZ√ÅSA √âS MENT√âSE =====
logging.info("Feldolgoz√°s befejezve, egys√©ges DataFrame l√©trehoz√°sa...")

if all_records:
    try:
        df = pd.DataFrame(all_records)
        
        # A 'birosag' oszlop felt√∂lt√©se, ha hi√°nyzik (fontos a konzisztenci√°hoz)
        df['birosag'] = df['birosag'].fillna('ISMERETLEN')

        # Oszlopok sorrendj√©nek biztos√≠t√°sa a jobb √°tl√°that√≥s√°g√©rt
        expected_cols = [
            'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',
            'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',
            'KapcsolodoHatarozatok', 'Jogszabalyhelyek'
        ]
        
        final_cols = [col for col in expected_cols if col in df.columns]
        other_cols = [col for col in df.columns if col not in final_cols]
        df = df[final_cols + other_cols]

        # A kimeneti √∫tvonal most a Parquet f√°jlra mutat
        out_path = config.CLEANED_PARQUET_DATA_PATH
        out_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Ment√©s egyetlen, t√∂m√∂r√≠tett Parquet f√°jlba
        df.to_parquet(
            path=out_path,
            engine='pyarrow',
            compression='snappy',
            index=False,
        )
        
        logging.info(f"Tiszt√≠tott Parquet f√°jl sikeresen mentve: {out_path} ({len(df):,} sor)")

    except Exception as e:
        logging.error(f"Hiba a Parquet f√°jl l√©trehoz√°s√°ban vagy ment√©s√©ben: {e}", exc_info=True)

# ===== V√âGS≈ê √úZENETEK =====
print(f"\n‚úÖ PREPROCESSING BEFEJEZVE!")
print(f"üìä Feldolgozott rekordok: {total_records:,}")
if config.REMOVE_HUNGARIAN_STOPWORDS:
    print(f"üá≠üá∫ Elt√°vol√≠tott magyar stopword√∂k: {stopwords_removed_count:,}")
print(f"üìÑ Kimeneti f√°jl: {config.CLEANED_PARQUET_DATA_PATH}")