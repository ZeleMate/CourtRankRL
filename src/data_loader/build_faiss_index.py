# src/data_loader/build_faiss_index.py
"""
FAISS index √©p√≠t≈ë szkript a jogi dokumentumok embeddingjeihez.

√öJDONS√ÅG: Chunked parquet t√°mogat√°s memory-safe FAISS index √©p√≠t√©shez.
Ez a szkript el≈ësz√∂r chunked parquet f√°jlokat keres, majd fallback az egyes√≠tett parquet-re.
"""
import pandas as pd
import numpy as np
import faiss
import logging
import os
import sys
import gc
import time
import pickle
import glob
from pathlib import Path
from typing import Tuple, Dict, Any, List

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Konfigur√°ci√≥ import√°l√°sa
from configs import config

# ------------------------------------------------------------------
# Konfigur√°ci√≥ bet√∂lt√©se
# ------------------------------------------------------------------
OUT_DIR = config.OUT_DIR
PROCESSED_PARQUET_DATA_PATH = config.PROCESSED_PARQUET_DATA_PATH
EMBEDDING_DIMENSION = config.EMBEDDING_DIMENSION

# FAISS specifikus el√©r√©si utak √©s param√©terek a konfigur√°ci√≥b√≥l
FAISS_INDEX_PATH = OUT_DIR / "faiss_index.bin"
FAISS_MAPPING_PATH = OUT_DIR / "faiss_id_mapping.pkl"
FAISS_NLIST = config.FAISS_INDEX_NLIST
FAISS_NPROBE = config.FAISS_INDEX_NPROBE

# Kimeneti k√∂nyvt√°r l√©trehoz√°sa, ha nem l√©tezik
OUT_DIR.mkdir(parents=True, exist_ok=True)

# Loggol√°s be√°ll√≠t√°sa
logging.basicConfig(
    level=config.LOGGING_LEVEL,
    format=config.LOGGING_FORMAT
)

def create_faiss_index(vectors: np.ndarray) -> Any:
    """
    L√©trehoz egy FAISS indexet a megadott vektorokb√≥l.
    A vektorok sz√°ma alapj√°n v√°laszt IndexFlatL2 vagy IndexIVFFlat t√≠pust.

    Args:
        vectors: A be√°gyaz√°si vektorok NumPy t√∂mbje (float32).
    
    Returns:
        A l√©trehozott FAISS index (faiss.Index objektum).
    """
    vector_dimension = vectors.shape[1]
    vector_count = vectors.shape[0]
    logging.info(f"FAISS index l√©trehoz√°sa {vector_count} vektorral, dimenzi√≥: {vector_dimension}")
    
    if not vectors.flags.c_contiguous:
        logging.warning("A bemeneti vektorok nem C-folyamatosak (C-contiguous), √°talak√≠t√°s...")
        vectors = np.ascontiguousarray(vectors, dtype=np.float32)
    
    if vector_count < 10000:
        logging.info(f"Kis adathalmaz ({vector_count} vektor), IndexFlatL2 haszn√°lata.")
        index = faiss.IndexFlatL2(vector_dimension)
    else:
        logging.info(f"Nagyobb adathalmaz ({vector_count} vektor), IndexIVFFlat l√©trehoz√°sa {FAISS_NLIST} klaszterrel.")
        quantizer = faiss.IndexFlatL2(vector_dimension)
        index = faiss.IndexIVFFlat(quantizer, vector_dimension, FAISS_NLIST, faiss.METRIC_L2)
        
        logging.info("IndexIVFFlat betan√≠t√°sa...")
        if not index.is_trained and vector_count > 0:
            index.train(vectors)  # type: ignore
        else:
            logging.info("Az index m√°r betan√≠tott vagy nincs adat a tan√≠t√°shoz.")
            
        index.nprobe = FAISS_NPROBE
    
    logging.info("Vektorok hozz√°ad√°sa az indexhez...")
    start_time = time.time()
    if vector_count > 0:
        index.add(vectors)  # type: ignore
    else:
        logging.warning("Nincsenek vektorok az indexhez ad√°shoz.")
    logging.info(f"Vektorok indexel√©se befejezve. Feldolgozott vektorok: {index.ntotal if vector_count > 0 else 0}, id≈ë: {time.time() - start_time:.2f} mp")
    
    return index

def load_chunked_embeddings() -> Tuple[bool, pd.DataFrame]:
    """
    Chunked parquet f√°jlokb√≥l embeddings bet√∂lt√©se memory-safe m√≥don.
    
    Returns:
        Tuple[bool, pd.DataFrame]: (success, combined_dataframe)
    """
    # Chunked parquet f√°jlok keres√©se
    chunked_pattern = str(OUT_DIR / "*_with_embeddings.parquet")
    chunk_files = glob.glob(chunked_pattern)
    
    if not chunk_files:
        logging.info("Nincs chunked parquet f√°jl tal√°lva")
        return False, pd.DataFrame()
    
    logging.info(f"üéØ CHUNKED PARQUET BET√ñLT√âS: {len(chunk_files)} chunk f√°jl tal√°lhat√≥")
    
    # Chunk f√°jlok rendezett bet√∂lt√©se (konzisztens sorrend)
    chunk_files.sort()
    all_chunks = []
    total_docs = 0
    
    for i, chunk_file in enumerate(chunk_files):
        try:
            logging.info(f"Chunk bet√∂lt√©se ({i+1}/{len(chunk_files)}): {os.path.basename(chunk_file)}")
            
            # Csak sz√ºks√©ges oszlopok bet√∂lt√©se (mem√≥ria optimaliz√°l√°s)
            chunk_df = pd.read_parquet(chunk_file, columns=['doc_id', 'embedding'])
            
            # Alapvet≈ë valid√°ci√≥
            if chunk_df.empty:
                logging.warning(f"√úres chunk: {chunk_file}")
                continue
            
            # Hi√°nyz√≥ embeddings elt√°vol√≠t√°sa
            missing_before = chunk_df['embedding'].isna().sum()
            if missing_before > 0:
                logging.warning(f"Chunk {i+1}: {missing_before} hi√°nyz√≥ embedding elt√°vol√≠tva")
                chunk_df = chunk_df.dropna(subset=['embedding'])
            
            if not chunk_df.empty:
                all_chunks.append(chunk_df)
                total_docs += len(chunk_df)
                logging.info(f"Chunk {i+1} bet√∂ltve: {len(chunk_df):,} √©rv√©nyes rekord")
            
            # Rendszeres mem√≥ria tiszt√≠t√°s
            if i % 5 == 0:
                gc.collect()
                
        except Exception as e:
            logging.error(f"Hiba chunk bet√∂lt√©s√©ben ({chunk_file}): {e}")
            continue
    
    if not all_chunks:
        logging.error("Nincs √©rv√©nyes chunk adat")
        return False, pd.DataFrame()
    
    # Chunk-ok egyes√≠t√©se
    logging.info("Chunk-ok egyes√≠t√©se...")
    combined_df = pd.concat(all_chunks, ignore_index=True)
    
    # Mem√≥ria felszabad√≠t√°s
    del all_chunks
    gc.collect()
    
    logging.info(f"‚úÖ Chunked bet√∂lt√©s sikeres:")
    logging.info(f"  üìÅ Chunk f√°jlok: {len(chunk_files)}")
    logging.info(f"  üìÑ √ñsszesen dokumentumok: {len(combined_df):,}")
    logging.info(f"  üöÄ Memory-optimaliz√°lt feldolgoz√°s")
    
    return True, combined_df

def load_unified_embeddings() -> pd.DataFrame:
    """
    Unified parquet f√°jl bet√∂lt√©se (fallback mode).
    """
    if not PROCESSED_PARQUET_DATA_PATH.exists():
        raise FileNotFoundError(f"Unified parquet nem tal√°lhat√≥: {PROCESSED_PARQUET_DATA_PATH}")
    
    logging.info("üìÑ UNIFIED PARQUET BET√ñLT√âS (fallback mode)")
    logging.info(f"Embeddings bet√∂lt√©se: {PROCESSED_PARQUET_DATA_PATH}")
    
    df = pd.read_parquet(PROCESSED_PARQUET_DATA_PATH, columns=['doc_id', 'embedding'])
    
    logging.info(f"Unified parquet bet√∂ltve: {len(df):,} dokumentum")
    return df

def test_search(index: Any, vectors: np.ndarray, id_mapping: Dict[int, Any], k: int = 5) -> None:
    """
    Leteszteli a FAISS indexet egy egyszer≈± keres√©ssel az els≈ë vektor alapj√°n.
    
    Args:
        index: A FAISS index.
        vectors: A vektorok, amelyekb≈ël az index k√©sz√ºlt (csak az els≈ët haszn√°lja a teszthez).
        id_mapping: Az FAISS index ID-k √©s az eredeti dokumentum ID-k k√∂z√∂tti lek√©pez√©s.
        k: A keresend≈ë legk√∂zelebbi szomsz√©dok sz√°ma.
    """
    if vectors.shape[0] == 0:
        logging.warning("Nincsenek vektorok a keres√©s tesztel√©s√©hez.")
        return

    logging.info(f"FAISS index tesztel√©se: {k} legk√∂zelebbi szomsz√©d keres√©se az els≈ë vektorhoz...")
    query_vector = np.ascontiguousarray(vectors[0:1], dtype=np.float32)
    
    start_time = time.time()
    distances, indices = index.search(query_vector, k)
    search_time = time.time() - start_time
    
    logging.info(f"Keres√©si id≈ë: {search_time*1000:.2f} ms")
    if indices.size > 0:
        logging.info(f"Tal√°latok FAISS indexei: {indices[0].tolist()}")
        try:
            doc_ids = [id_mapping[idx] for idx in indices[0].tolist() if idx in id_mapping]
            logging.info(f"Tal√°latok eredeti dokumentum ID-jai: {doc_ids}")
        except KeyError as e:
            logging.error(f"Hiba a dokumentum ID-k visszakeres√©sekor a lek√©pez√©sb≈ël: hi√°nyz√≥ FAISS index {e}")
    else:
        logging.info("A tesztkeres√©s nem adott vissza tal√°latot.")

def main():
    """
    F≈ë f√ºggv√©ny a FAISS index l√©trehoz√°s√°hoz.
    
    √öJDONS√ÅG: Chunked parquet t√°mogat√°s memory-safe FAISS index √©p√≠t√©shez.
    El≈ësz√∂r chunked parquet f√°jlokat keres, fallback az egyes√≠tett parquet-re.
    """
    logging.info("üöÄ CHUNKED-KOMPATIBILIS FAISS INDEX √âP√çT√âS")
    
    # ===== 1. CHUNKED PARQUET BET√ñLT√âS (PRIORIT√ÅS) =====
    success, df = load_chunked_embeddings()
    
    # ===== 2. UNIFIED PARQUET FALLBACK =====
    if not success:
        logging.info("Chunked parquet nem el√©rhet≈ë, fallback unified parquet-re...")
        
        if not PROCESSED_PARQUET_DATA_PATH.exists():
            logging.error(f"Nincs el√©rhet≈ë embedding adat!")
            logging.error(f"Sem chunked parquet ({OUT_DIR}/*_with_embeddings.parquet)")
            logging.error(f"Sem unified parquet ({PROCESSED_PARQUET_DATA_PATH})")
            raise SystemExit("El≈ësz√∂r futtasd az embedding gener√°l√°st!")
        
        df = load_unified_embeddings()

    try:
        # ===== 3. ADATOK VALID√ÅL√ÅSA √âS TISZT√çT√ÅSA =====
        logging.info(f"Embedding adatok valid√°l√°sa: {len(df):,} dokumentum")
        
        # Hi√°nyz√≥ embeddinges sorok kezel√©se (ha m√©g vannak)
        missing_count = df['embedding'].isna().sum()
        if missing_count:
            logging.warning(f"{missing_count} sorban nincs embedding, ezek kisz≈±r√©se...")
            df = df.dropna(subset=['embedding']).reset_index(drop=True)
            logging.info(f"Sz≈±r√©s ut√°n maradt {len(df):,} dokumentum")
        
        # Dimenzi√≥ ellen≈ërz√©s √©s sz≈±r√©s a stackel√©s el≈ëtt
        if not df.empty:
            original_count = len(df)
            # Biztons√°gos hossz ellen≈ërz√©s: csak list√°kra/t√∂mb√∂kre alkalmazzuk
            df['embedding_len'] = df['embedding'].apply(lambda x: len(x) if isinstance(x, (list, np.ndarray)) else 0)
            
            mismatched_dims = df[df['embedding_len'] != EMBEDDING_DIMENSION]
            if not mismatched_dims.empty:
                logging.warning(f"{len(mismatched_dims)} sor elt√°vol√≠tva a nem megfelel≈ë embedding dimenzi√≥ miatt.")
                logging.debug(f"P√©ld√°k a hib√°s dimenzi√≥j√∫ sorokb√≥l (doc_id, embedding hossza): {mismatched_dims[['doc_id', 'embedding_len']].head().to_dict('records')}")
                df = df[df['embedding_len'] == EMBEDDING_DIMENSION]

            df = df.drop(columns=['embedding_len'])

        if df.empty:
            logging.error("Nincsenek √©rv√©nyes embeddingek a sz≈±r√©s ut√°n. Le√°ll√°s.")
            raise SystemExit("Nincs feldolgozhat√≥ adat a dimenzi√≥ellen≈ërz√©s ut√°n.")

        # ID-lek√©pez√©s l√©trehoz√°sa
        id_mapping = dict(enumerate(df['doc_id']))
        
        # Vektorok konvert√°l√°sa numpy t√∂mbb√©
        vectors = np.stack(df['embedding'].tolist()).astype(np.float32)
        
        # Mem√≥ria felszabad√≠t√°sa
        del df
        gc.collect()
        
        # Dimenzi√≥sz√°m ellen≈ërz√©se
        if vectors.shape[1] != EMBEDDING_DIMENSION:
            logging.warning(f"A vektorok dimenzi√≥ja {vectors.shape[1]}, konfigur√°ci√≥ban: {EMBEDDING_DIMENSION}")

        # FAISS index l√©trehoz√°sa
        logging.info("FAISS index l√©trehoz√°sa...")
        index = create_faiss_index(vectors)
        
        # Index √©s ID-lek√©pez√©s ment√©se
        logging.info(f"Index ment√©se: {FAISS_INDEX_PATH}")
        faiss.write_index(index, str(FAISS_INDEX_PATH))
        
        logging.info(f"ID-lek√©pez√©s ment√©se: {FAISS_MAPPING_PATH}")
        with open(FAISS_MAPPING_PATH, 'wb') as f:
            pickle.dump(id_mapping, f)
        
        # Index tesztel√©se
        test_search(index, vectors, id_mapping)
        
        # ===== V√âGS≈ê √ñSSZEFOGLAL√ì =====
        input_mode = "CHUNKED" if success else "UNIFIED"
        
        print(f"\n‚úÖ CHUNKED-KOMPATIBILIS FAISS INDEX L√âTREHOZVA!")
        print(f"üìä Feldolgozott dokumentumok: {len(id_mapping):,}")
        print(f"üìÅ Input m√≥d: {input_mode}")
        print(f"üóÇÔ∏è  FAISS index: {FAISS_INDEX_PATH}")
        print(f"üîó ID mapping: {FAISS_MAPPING_PATH}")
        print(f"üìè Embedding dimenzi√≥: {vectors.shape[1]}")
        print(f"üîç Szemantikai kereshet≈ës√©g: MEG≈êRIZVE")
        if success:
            print(f"üöÄ Memory-optimaliz√°lt chunked feldolgoz√°s haszn√°lva!")
        
        logging.info(f"FAISS index l√©trehozva ({input_mode} mode): {len(id_mapping):,} dokumentum")
        print(f"‚úÖ ID-lek√©pez√©s mentve: {FAISS_MAPPING_PATH}")
        
    except Exception as e:
        logging.error(f"Hiba t√∂rt√©nt: {e}")
        import traceback
        logging.error(traceback.format_exc())
        raise SystemExit("Hiba a FAISS index √©p√≠t√©se sor√°n!")

if __name__ == '__main__':
    main()