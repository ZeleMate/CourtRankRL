#!/usr/bin/env python3
"""
GRPO Training Slate Preparation (OptimalizÃ¡lt verziÃ³)
Agents.md specifikÃ¡ciÃ³ alapjÃ¡n.

FÅ‘ feladatok:
1. Query filtering: csak olyan query-ket tartunk meg ahol a retrieval talÃ¡l relevÃ¡ns dokumentumot
2. Chunk-level slate kÃ©szÃ­tÃ©s teljes szÃ¶vegekkel
3. Metadata export (court, domain, year)
4. Baseline fusion ranking tÃ¡rolÃ¡sa (slate order)

OptimalizÃ¡ciÃ³k:
- Unified memory cache (text + metadata): ~1.5-1.8 GB RAM, egyszeri beolvasÃ¡s
- tqdm progress bar: becsÃ¼lt hÃ¡tralÃ©vÅ‘ idÅ‘, throughput tracking
"""

import argparse
import json
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List

from tqdm import tqdm

# Config
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from configs import config
from scripts.hybrid_retrieval import HybridRetriever

# Slate konfigurÃ¡ciÃ³ (UPDATED: tiszta RRF sorrend, nincs relevancia-bÃ³nusz, nincs qrels-alapÃº filtering)
SLATE_SIZE_CHUNKS = 300  # TOP-300 CHUNK CANDIDATE POOL (fixÃ¡lt, BM25+FAISS fusion)
SLATE_SIZE_TRAINING = 30  # Training prompt: 30 chunk (reprezentatÃ­v minta)


def build_chunk_cache(chunks_path: Path) -> Dict[str, Dict[str, str]]:
    """
    Egyszeri betÃ¶ltÃ©s: chunks.jsonl â†’ unified memory cache.
    
    Cache struktÃºra: {chunk_id: {"text": str, "court": str, "domain": str, "year": str}}
    
    Returns:
        Chunk cache dictionary
    """
    import pandas as pd
    
    print("\nğŸ“¦ Unified chunk cache Ã©pÃ­tÃ©se (text + metadata)...")
    print(f"   ForrÃ¡s: {chunks_path}")
    
    chunk_cache = {}
    
    # Egyszeri teljes beolvasÃ¡s pandas-szal (gyorsabb mint chunked reading erre a cÃ©lra)
    df = pd.read_json(chunks_path, lines=True)
    
    print(f"   FeldolgozÃ¡s: {len(df):,} chunk...")
    
    for _, row in tqdm(df.iterrows(), total=len(df), desc="Cache Ã©pÃ­tÃ©s", unit="chunk"):
        chunk_id = row.get('chunk_id')
        if chunk_id:
            chunk_cache[chunk_id] = {
                "text": row.get('text', ''),
                "court": row.get('court', ''),
                "domain": row.get('domain', ''),
                "year": row.get('year', '')
            }
    
    cache_size_mb = sum(len(str(v)) for v in chunk_cache.values()) / (1024 * 1024)
    print(f"   âœ… Cache kÃ©sz: {len(chunk_cache):,} chunk (~{cache_size_mb:.1f} MB)")
    
    return chunk_cache


def load_qrels(qrels_path: Path) -> Dict[str, Dict[str, int]]:
    """
    BetÃ¶lti a qrels fÃ¡jlt.
    
    Returns:
        {query_id: {doc_id: relevance}}
    """
    qrels = defaultdict(dict)
    
    with open(qrels_path, "r", encoding="utf-8") as f:
        next(f)  # Skip header
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            parts = line.split("\t")
            if len(parts) != 3:
                continue
            
            query_id, doc_id, relevance_str = parts
            try:
                relevance = int(relevance_str)
                qrels[query_id][doc_id] = relevance
            except ValueError:
                continue
    
    return dict(qrels)


def prepare_chunk_slates(
    qrels: Dict[str, Dict[str, int]],
    retriever: HybridRetriever,
    top_k: int = SLATE_SIZE_TRAINING,
) -> List[Dict[str, Any]]:
    """
    Chunk-level slate kÃ©szÃ­tÃ©s meglÃ©vÅ‘ qrels-bÅ‘l.
    
    Agents.md szerint:
    - HasznÃ¡lja retriever.get_last_chunk_scores("fused") az RRF baseline orderhez
    - BetÃ¶lti teljes chunk texteket retriever._load_chunk_texts()-tel
    - Metaadatokat retriever.get_doc_metadata()-val
    - Slate order = chunk-level RRF ranking (konzisztens a doc-level RRF-fel)
    
    Steps:
    1. Minden query-hez: retriever.retrieve(query, top_k=top_k)
    2. Chunk-szintÅ± fused scores lekÃ©rÃ©se: retriever.get_last_chunk_scores("fused")
    3. Top-K chunk kivÃ¡lasztÃ¡sa (ez lesz a baseline order)
    4. Teljes szÃ¶vegek betÃ¶ltÃ©se
    5. Doc-level relevance mapping chunk-ekre
    6. Slate Ã¶sszeÃ¡llÃ­tÃ¡sa metadata-val
    
    Returns:
        List of slate objects for GRPO training
    """
    slates = []
    
    print(f"\nğŸ“¦ Slate preparation ({len(qrels)} query)...")
    print(f"   Top-K: {top_k}\n")
    
    for query_id, doc_relevances in tqdm(qrels.items(), total=len(qrels), desc="Slate kÃ©szÃ­tÃ©s", unit="query"):
        
        # Retrieve candidates (this also stores chunk-level scores)
        _ = retriever.retrieve(query_id)
        
        # Get chunk-level fused scores (RRF baseline order)
        # KRITIKUS: Csak tiszta RRF fusion sorrendet hasznÃ¡lunk, nincs relevancia-bÃ³nusz vagy extra chunk
        fused_chunks = retriever.get_last_chunk_scores("fused", top_k=top_k)
        
        if not fused_chunks:
            # Edge case: nincs chunk talÃ¡lat
            slates.append({
                "query_id": query_id,
                "slate": [],  # Ãœres slate
            })
            continue
        
        # Get individual scores for each chunk
        bm25_scores = {chunk_id: score for chunk_id, score in retriever.get_last_chunk_scores("bm25")}
        faiss_scores = {chunk_id: score for chunk_id, score in retriever.get_last_chunk_scores("dense")}
        
        # Load full chunk texts (agents.md: full text, not preview)
        chunk_ids = [chunk_id for chunk_id, _ in fused_chunks]
        chunk_texts = retriever._load_chunk_texts(chunk_ids)
        
        # Build slate with full metadata
        slate_candidates = []
        for chunk_id, rrf_score in fused_chunks:
            # Get doc_id and metadata
            doc_id = retriever._chunk_id_to_doc_id(chunk_id)
            metadata = retriever.get_doc_metadata(doc_id)
            
            # Map doc-level relevance to chunk
            relevance = doc_relevances.get(doc_id, 0)
            
            # Full chunk text (not preview!)
            text = chunk_texts.get(chunk_id, "")
            
            slate_candidates.append({
                "chunk_id": chunk_id,
                "doc_id": doc_id,
                "bm25_score": bm25_scores.get(chunk_id, 0.0),
                "faiss_score": faiss_scores.get(chunk_id, 0.0),
                "rrf_score": rrf_score,
                "relevance": relevance,  # Csak metadata, nem mÃ³dosÃ­tja a sorrendet
                "court": metadata.get("court", ""),
                "domain": metadata.get("domain", ""),
                "year": metadata.get("year", ""),
                "text": text,  # FULL TEXT
            })
        
        # A slate sorrend = tiszta baseline RRF ranking (hibrid retriever eredmÃ©nye)
        slates.append({
            "query_id": query_id,
            "slate": slate_candidates,  # Order = tiszta baseline RRF ranking (nincs relevancia leakage!)
        })
    
    print(f"   âœ… Feldolgozva: {len(slates)}/{len(qrels)} query")
    print(f"   ğŸ“Š Slate mÃ©ret: {SLATE_SIZE_TRAINING} chunk/query (training), {SLATE_SIZE_CHUNKS} candidate pool")
    print(f"   ğŸ“Š Tiszta RRF sorrend: nincs relevancia-bÃ³nusz, nincs extra chunk, nincs qrels-alapÃº filtering")
    
    return slates


def save_slates(
    slates: List[Dict[str, Any]],
    output_path: Path,
) -> None:
    """Menti a slate-eket JSONL formÃ¡tumban."""
    with open(output_path, "w", encoding="utf-8") as f:
        for slate in slates:
            f.write(json.dumps(slate, ensure_ascii=False) + "\n")
    
    print(f"\nğŸ’¾ Slate-ek mentve: {output_path}")
    print(f"   {len(slates)} query slate")


def print_statistics(
    slates: List[Dict[str, Any]],
    original_count: int,
) -> None:
    """StatisztikÃ¡k kiÃ­rÃ¡sa."""
    print("\n" + "=" * 60)
    print("ğŸ“Š Ã–SSZEFOGLALÃ“ STATISZTIKÃK")
    print("=" * 60)
    
    print(f"\nğŸ“‹ Query-k:")
    print(f"   â€¢ Eredeti:         {original_count}")
    print(f"   â€¢ Feldolgozott:    {len(slates)} ({100 * len(slates) / original_count:.1f}%)")
    
    if slates:
        slate_sizes = [len(slate["slate"]) for slate in slates]
        non_empty_slates = [s for s in slates if s["slate"]]  # Non-empty slates
        
        print(f"\nğŸ“¦ Slate-ek:")
        print(f"   â€¢ Nem Ã¼res slate-ek:     {len(non_empty_slates)}")
        print(f"   â€¢ Ãœres slate-ek:         {len(slates) - len(non_empty_slates)}")
        
        if non_empty_slates:
            non_empty_sizes = [len(slate["slate"]) for slate in non_empty_slates]
            print(f"   â€¢ Ãtlag candidates/query: {sum(non_empty_sizes) / len(non_empty_sizes):.1f}")
            print(f"   â€¢ Min candidates:         {min(non_empty_sizes)}")
            print(f"   â€¢ Max candidates:         {max(non_empty_sizes)}")
            
            # Relevance distribution
            relevance_counts = defaultdict(int)
            for slate in non_empty_slates:
                for candidate in slate["slate"]:
                    relevance_counts[candidate["relevance"]] += 1
            
            total_candidates = sum(relevance_counts.values())
            print(f"\nğŸ¯ Relevancia eloszlÃ¡s a slate-ekben:")
            for rel in sorted(relevance_counts.keys()):
                count = relevance_counts[rel]
                pct = 100 * count / total_candidates if total_candidates > 0 else 0
                print(f"   â€¢ Relevance {rel}: {count:4d} ({pct:5.1f}%)")
    
    print("\n" + "=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="GRPO training slate preparation - chunk-level candidates from existing qrels"
    )
    parser.add_argument(
        "--qrels",
        type=Path,
        default=config.QRELS_FILE,
        help="Qrels fÃ¡jl elÃ©rÃ©si Ãºtja",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=config.DATA_DIR / "models" / "grpo_policy" / "training_slates.jsonl",
        help="Output slate fÃ¡jl",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=SLATE_SIZE_TRAINING,
        help="HÃ¡ny chunk kerÃ¼ljÃ¶n a training slate-be (default: 30)",
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ GRPO SLATE PREPARATION")
    print("=" * 60)
    
    # 1. Load qrels
    print(f"\nğŸ“– Qrels betÃ¶ltÃ©se: {args.qrels}")
    qrels = load_qrels(args.qrels)
    print(f"   âœ… {len(qrels)} query betÃ¶ltve")
    
    # 2. Initialize retriever
    print(f"\nğŸ”§ Retrieval modell inicializÃ¡lÃ¡sa...")
    retriever = HybridRetriever()
    retriever.initialize()
    print(f"   âœ… Modell kÃ©sz")
    
    # 2.1. Build unified cache (kÃ¼lÃ¶n fÃ¼ggvÃ©ny a prepare_grpo_slates.py-ban)
    chunks_path = retriever.base_path / "data" / "processed" / "chunks.jsonl"
    chunk_cache = build_chunk_cache(chunks_path)
    
    # 2.2. Set cache in retriever
    retriever.set_chunk_cache(chunk_cache)
    
    # 3. Prepare slates (tiszta RRF sorrend, nincs relevancia-bÃ³nusz vagy extra chunk)
    slates = prepare_chunk_slates(
        qrels,
        retriever,
        top_k=args.top_k,
    )
    
    # 4. Save outputs
    args.output.parent.mkdir(parents=True, exist_ok=True)
    save_slates(slates, args.output)
    
    # 5. Statistics
    print_statistics(slates, len(qrels))
    
    print("\nâœ… KÃ‰SZ!")
    print(f"\nğŸ’¡ KÃ¶vetkezÅ‘ lÃ©pÃ©s:")
    print(f"   TÃ¶ltsd fel a {args.output} fÃ¡jlt a cloud GPU-ra")
    print(f"   Ã©s futtasd a grpo_train_runpod.ipynb notebookot")


if __name__ == "__main__":
    main()

