{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4801bb",
   "metadata": {},
   "source": [
    "# Qwen3 Embeddingek EDA - CourtRankRL projekt\n",
    "\n",
    "Ez a notebook a qwen_embedding_runpod.ipynb √°ltal gener√°lt FAISS indexben t√°rolt embeddingeket elemzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "\n",
    "# Plot st√≠lus be√°ll√≠t√°sa\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Projekt konfigur√°ci√≥ bet√∂lt√©se\n",
    "import sys\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "try:\n",
    "    from configs import config\n",
    "except ImportError:\n",
    "    print(\"Figyelem: configs modul nem tal√°lhat√≥. Haszn√°lja a default √©rt√©keket.\")\n",
    "    class Config:\n",
    "        FAISS_INDEX_PATH = project_root / \"data\" / \"index\" / \"faiss_index.bin\"\n",
    "        CHUNK_ID_MAP_PATH = project_root / \"data\" / \"index\" / \"chunk_id_map.json\"\n",
    "    config = Config()\n",
    "\n",
    "print(\"FAISS index √©s chunk adatok bet√∂lt√©se...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_2",
   "metadata": {},
   "source": [
    "## 1. FAISS Index bet√∂lt√©se √©s embeddingek kivon√°sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS index bet√∂lt√©se\n",
    "faiss_path = config.FAISS_INDEX_PATH\n",
    "chunk_map_path = config.CHUNK_ID_MAP_PATH\n",
    "\n",
    "if faiss_path and Path(faiss_path).exists():\n",
    "    try:\n",
    "        index = faiss.read_index(str(faiss_path))\n",
    "        print(f\"‚úÖ FAISS index bet√∂ltve: {index.ntotal} vektor, {index.d} dimenzi√≥\")\n",
    "\n",
    "        # Embeddingek kivon√°sa (minden vektor)\n",
    "        embeddings = []\n",
    "        for i in range(index.ntotal):\n",
    "            embedding = index.reconstruct(i)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        embeddings = np.array(embeddings)\n",
    "        print(f\"‚úÖ Embeddingek kivonva: {embeddings.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hiba a FAISS index bet√∂lt√©se sor√°n: {e}\")\n",
    "        index = None\n",
    "        embeddings = None\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è FAISS index nem tal√°lhat√≥: {faiss_path}\")\n",
    "    print(\"Futtassa a qwen_embedding_runpod.ipynb-t el≈ësz√∂r!\")\n",
    "\n",
    "# Chunk ID mapping bet√∂lt√©se\n",
    "if chunk_map_path and Path(chunk_map_path).exists():\n",
    "    try:\n",
    "        with open(chunk_map_path, 'r', encoding='utf-8') as f:\n",
    "            chunk_id_map = json.load(f)\n",
    "        print(f\"‚úÖ Chunk ID map bet√∂ltve: {len(chunk_id_map)} mapping\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hiba a chunk ID map bet√∂lt√©se sor√°n: {e}\")\n",
    "        chunk_id_map = None\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Chunk ID map nem tal√°lhat√≥: {chunk_map_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_4",
   "metadata": {},
   "source": [
    "## 2. Chunk adatok bet√∂lt√©se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunkok bet√∂lt√©se mintav√©telez√©ssel\n",
    "df = None\n",
    "chunks_file = getattr(config, 'CHUNKS_JSONL', None)\n",
    "\n",
    "if chunks_file and Path(chunks_file).exists() and index is not None and chunk_id_map is not None:\n",
    "    try:\n",
    "        sample_size = min(5000, index.ntotal)  # Maximum 5000 chunk elemz√©sre\n",
    "        print(f\"üìä Mintav√©tel: {sample_size} chunk bet√∂lt√©se...\")\n",
    "\n",
    "        chunks_list = []\n",
    "        chunk_ids = list(chunk_id_map.values())[:sample_size]\n",
    "\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    chunk = json.loads(line.strip())\n",
    "                    if chunk.get('chunk_id') in chunk_ids:\n",
    "                        chunks_list.append(chunk)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                if len(chunks_list) >= sample_size:\n",
    "                    break\n",
    "\n",
    "        if chunks_list:\n",
    "            df = pd.DataFrame(chunks_list)\n",
    "            print(f\"‚úÖ Bet√∂lt√∂tt chunkok sz√°ma: {len(df)}\")\n",
    "\n",
    "            # Embeddingek hozz√°rendel√©se a chunkokhoz\n",
    "            if embeddings is not None:\n",
    "                try:\n",
    "                    embedding_dict = {chunk_id_map[str(i)]: embeddings[i] for i in range(len(embeddings))}\n",
    "                    df = df.assign(embedding=df['chunk_id'].map(embedding_dict))\n",
    "                    valid_embeddings = df['embedding'].notna().sum()\n",
    "                    print(f\"‚úÖ Embeddingek hozz√°rendelve: {valid_embeddings} chunk\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Hiba az embeddingek hozz√°rendel√©se sor√°n: {e}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Nincsenek embeddingek a hozz√°rendel√©shez\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Nem tal√°lhat√≥ak megfelel≈ë chunkok\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hiba a chunkok bet√∂lt√©se sor√°n: {e}\")\n",
    "        df = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Hi√°nyz√≥ adatok a chunk bet√∂lt√©shez:\")\n",
    "    if not chunks_file:\n",
    "        print(\"  - CHUNKS_JSONL konfigur√°ci√≥\")\n",
    "    if not Path(chunks_file).exists() if chunks_file else None:\n",
    "        print(f\"  - F√°jl nem tal√°lhat√≥: {chunks_file}\")\n",
    "    if index is None:\n",
    "        print(\"  - FAISS index\")\n",
    "    if chunk_id_map is None:\n",
    "        print(\"  - Chunk ID map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_6",
   "metadata": {},
   "source": [
    "## 3. Embeddingek alapellen≈ërz√©se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'embedding' in df.columns:\n",
    "    print(\"Embeddingek alapvet≈ë statisztik√°i:\")\n",
    "    \n",
    "    # Els≈ë embedding t√≠pusa √©s dimenzi√≥ja\n",
    "    first_embedding = df['embedding'].iloc[0]\n",
    "    print(f\"Els≈ë embedding t√≠pusa: {type(first_embedding)}\")\n",
    "    print(f\"Els≈ë embedding dimenzi√≥ja: {len(first_embedding)}\")\n",
    "    \n",
    "    # Embeddingek norm√°i (L2 normaliz√°l√°s ellen≈ërz√©se)\n",
    "    norms = df['embedding'].apply(lambda x: np.linalg.norm(x))\n",
    "    print(f\"Embedding norm√°k statisztik√°i:\")\n",
    "    print(norms.describe())\n",
    "    \n",
    "    # Normaliz√°l√°s ellen≈ërz√©se\n",
    "    normalized_count = norms.apply(lambda x: abs(x - 1.0) < 0.01).sum()\n",
    "    print(f\"L2-normaliz√°lt embeddingek: {normalized_count}/{len(df)} ({100*normalized_count/len(df):.1f}%)\")\n",
    "    \n",
    "    # Hi√°nyz√≥ embeddingek\n",
    "    missing_embeddings = df['embedding'].isna().sum()\n",
    "    print(f\"Hi√°nyz√≥ embeddingek: {missing_embeddings}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Nincs embedding adat az elemz√©shez.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_8",
   "metadata": {},
   "source": [
    "## 4. PCA dimenzi√≥cs√∂kkent√©s √©s vizualiz√°ci√≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'embedding' in df.columns:\n",
    "    # Hi√°nyz√≥ embeddingek elt√°vol√≠t√°sa\n",
    "    valid_df = df.dropna(subset=['embedding']).copy()\n",
    "    \n",
    "    if len(valid_df) > 100:  # Minimum 100 embedding PCA-hoz\n",
    "        # Embeddingek NumPy t√∂mbb√© alak√≠t√°sa\n",
    "        X = np.vstack(valid_df['embedding'].values)\n",
    "        print(f\"PCA bemenet: {X.shape}\")\n",
    "        \n",
    "        # PCA futtat√°sa\n",
    "        pca = PCA(n_components=2)\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "        \n",
    "        print(f\"PCA els≈ë k√©t komponens varianci√°ja: {pca.explained_variance_ratio_}\")\n",
    "        print(f\"√ñsszes magyar√°zott variancia: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "        \n",
    "        # PCA eredm√©ny vizualiz√°ci√≥\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.scatter(X_reduced[:, 0], X_reduced[:, 1], s=2, alpha=0.6)\n",
    "        plt.title(\"Qwen3 Embeddingek PCA 2D lek√©pez√©se\")\n",
    "        plt.xlabel(\"F≈ëkomponens 1\")\n",
    "        plt.ylabel(\"F≈ëkomponens 2\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Jogter√ºlet szerinti sz√≠nez√©s\n",
    "        if 'domain' in valid_df.columns:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            domains = valid_df['domain'].fillna('ismeretlen').values\n",
    "            scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=valid_df['domain'].astype('category').cat.codes, s=8, alpha=0.6, cmap='tab10')\n",
    "            plt.title(\"Qwen3 Embeddingek jogter√ºlet szerint sz√≠nezve\")\n",
    "            plt.xlabel(\"F≈ëkomponens 1\")\n",
    "            plt.ylabel(\"F≈ëkomponens 2\")\n",
    "            plt.legend(handles=scatter.legend_elements()[0], labels=valid_df['domain'].astype('category').cat.categories.tolist(), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "        \n",
    "        # B√≠r√≥s√°g szerinti sz√≠nez√©s\n",
    "        if 'court' in valid_df.columns:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            courts = valid_df['court'].fillna('ismeretlen').values\n",
    "            scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=valid_df['court'].astype('category').cat.codes, s=8, alpha=0.6, cmap='Set3')\n",
    "            plt.title(\"Qwen3 Embeddingek b√≠r√≥s√°g szerint sz√≠nezve\")\n",
    "            plt.xlabel(\"F≈ëkomponens 1\")\n",
    "            plt.ylabel(\"F≈ëkomponens 2\")\n",
    "            plt.legend(handles=scatter.legend_elements()[0], labels=valid_df['court'].astype('category').cat.categories.tolist(), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"T√∫l kev√©s embedding PCA-hoz: {len(valid_df)}\")\n",
    "else:\n",
    "    print(\"Nincs embedding adat a PCA-hoz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_10",
   "metadata": {},
   "source": [
    "## 5. Embedding min≈ës√©gi metrik√°k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'embedding' in df.columns:\n",
    "    valid_df = df.dropna(subset=['embedding']).copy()\n",
    "    \n",
    "    if len(valid_df) > 10:\n",
    "        X = np.vstack(valid_df['embedding'].values)\n",
    "        \n",
    "        print(\"Embedding min≈ës√©gi metrik√°k:\")\n",
    "        \n",
    "        # √Åtlagos norma (m√°r ellen≈ërizt√ºk)\n",
    "        norms = np.linalg.norm(X, axis=1)\n",
    "        print(f\"Norm√°k √°tlaga: {norms.mean():.4f}\")\n",
    "        print(f\"Norm√°k sz√≥r√°sa: {norms.std():.4f}\")\n",
    "        \n",
    "        # Embeddingek k√∂z√∂tti t√°vols√°gok\n",
    "        # V√©letlenszer≈±en kiv√°lasztott 1000 p√°r t√°vols√°ga\n",
    "        n_pairs = min(1000, len(X) * (len(X) - 1) // 2)\n",
    "        indices = np.random.choice(len(X), size=min(len(X), 100), replace=False)\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i + 1, len(indices)):\n",
    "                dist = np.linalg.norm(X[indices[i]] - X[indices[j]])\n",
    "                distances.append(dist)\n",
    "        \n",
    "        distances = np.array(distances)\n",
    "        print(f\"Embeddingek k√∂z√∂tti √°tlagos t√°vols√°g: {distances.mean():.4f}\")\n",
    "        print(f\"Embeddingek k√∂z√∂tti t√°vols√°g sz√≥r√°sa: {distances.std():.4f}\")\n",
    "        \n",
    "        # Legk√∂zelebbi √©s legt√°volabbi pontok\n",
    "        print(f\"Legkisebb t√°vols√°g: {distances.min():.4f}\")\n",
    "        print(f\"Legnagyobb t√°vols√°g: {distances.max():.4f}\")\n",
    "        \n",
    "        # Embeddingek s≈±r≈±s√©ge\n",
    "        print(f\"\\nEmbedding dimenzi√≥: {X.shape[1]}\")\n",
    "        print(f\"Adatpontok sz√°ma: {len(X)}\")\n",
    "        print(f\"Adatpontok s≈±r≈±s√©ge: {len(X) / X.shape[1]:.2f}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"T√∫l kev√©s embedding a min≈ës√©gi metrik√°khoz: {len(valid_df)}\")\n",
    "else:\n",
    "    print(\"Nincs embedding adat a min≈ës√©gi metrik√°khoz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_12",
   "metadata": {},
   "source": [
    "## 6. K√∂vetkeztet√©sek"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
