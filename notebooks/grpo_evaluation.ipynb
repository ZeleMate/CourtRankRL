{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grpo-evaluation-header",
   "metadata": {},
   "source": [
    "# GRPO Reranking Ki√©rt√©kel√©se - CourtRankRL Projekt\n",
    "\n",
    "Ez a notebook a CourtRankRL GRPO alap√∫ reranking komponens√©t √©rt√©keli ki. Az agents.md specifik√°ci√≥ alapj√°n a reinforcement learning reranking teljes√≠tm√©ny√©t √©s hat√©konys√°g√°t teszteli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "import time\n",
    "\n",
    "# Plot st√≠lus be√°ll√≠t√°sa\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Projekt konfigur√°ci√≥ bet√∂lt√©se\n",
    "import sys\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "from configs import config\n",
    "from src.search.hybrid_search import HybridRetriever\n",
    "from src.search.grpo_reranker import GRPOReranker\n",
    "\n",
    "print(\"CourtRankRL - GRPO Reranking Evaluation\")\n",
    "print(f\"RL Policy: {config.RL_POLICY_PATH}\")\n",
    "print(f\"BM25 index: {config.BM25_INDEX_PATH}\")\n",
    "print(f\"FAISS index: {config.FAISS_INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-components",
   "metadata": {},
   "source": [
    "## 1. Komponensek Bet√∂lt√©se\n",
    "\n",
    "A GRPO reranker √©s retrieval komponensek bet√∂lt√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-components-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komponensek inicializ√°l√°sa\n",
    "retriever = None\n",
    "reranker = None\n",
    "policy_loaded = False\n",
    "\n",
    "print(\"Komponensek bet√∂lt√©se...\")\n",
    "\n",
    "# Hybrid retriever\n",
    "try:\n",
    "    retriever = HybridRetriever()\n",
    "    print(\"‚úÖ Hybrid retriever bet√∂ltve\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Hybrid retriever hiba: {e}\")\n",
    "    retriever = None\n",
    "\n",
    "# GRPO reranker\n",
    "try:\n",
    "    reranker = GRPOReranker()\n",
    "    print(\"‚úÖ GRPO reranker inicializ√°lva\")\n",
    "    \n",
    "    # Policy bet√∂lt√©se\n",
    "    if config.RL_POLICY_PATH.exists():\n",
    "        reranker.load_policy(config.RL_POLICY_PATH)\n",
    "        policy_loaded = True\n",
    "        print(\"‚úÖ RL policy bet√∂ltve\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è RL policy nem tal√°lhat√≥: {config.RL_POLICY_PATH}\")\n",
    "        print(\"Policy n√©lk√ºli reranker m≈±k√∂d√©s (baseline)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GRPO reranker hiba: {e}\")\n",
    "    reranker = None\n",
    "\n",
    "if reranker is not None:\n",
    "    print(f\"\\nReranker konfigur√°ci√≥:\")\n",
    "    print(f\"  Learning rate: {config.RL_LEARNING_RATE}\")\n",
    "    print(f\"  Batch size: {config.RL_BATCH_SIZE}\")\n",
    "    print(f\"  Hidden dim: {config.RL_HIDDEN_DIM}\")\n",
    "    print(f\"  Policy loaded: {policy_loaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reranking-performance",
   "metadata": {},
   "source": [
    "## 2. Reranking Teljes√≠tm√©ny Tesztel√©se\n",
    "\n",
    "A GRPO reranking teljes√≠tm√©ny√©nek √©s hat√©konys√°g√°nak tesztel√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-reranking-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retriever is not None and reranker is not None:\n",
    "    print(\"üéØ Reranking teljes√≠tm√©ny tesztel√©se:\")\n",
    "    \n",
    "    # Teszt lek√©rdez√©sek\n",
    "    test_queries = [\n",
    "        \"szerz≈ëd√©s felmond√°sa\",\n",
    "        \"k√°rt√©r√≠t√©s\",\n",
    "        \"csal√°di jog\",\n",
    "        \"munkajog\",\n",
    "        \"ingatlan tulajdonjog\"\n",
    "    ]\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nüîç Teszt lek√©rdez√©s: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Baseline retrieval\n",
    "            start_time = time.time()\n",
    "            bm25_results, dense_results = retriever.retrieve_candidates(query, top_k=config.TOP_K_BASELINE)\n",
    "            baseline_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"  Baseline retrieval: {len(bm25_results)} BM25 + {len(dense_results)} dense = {len(bm25_results) + len(dense_results)} candidate\")\n",
    "            print(f\"  Baseline id≈ë: {baseline_time*1000:.1f}ms\")\n",
    "            \n",
    "            # Baseline fusion (RRF)\n",
    "            start_time = time.time()\n",
    "            baseline_fusion = retriever.retrieve(query, top_k=config.TOP_K_RERANKED, fusion_method=\"rrf\")\n",
    "            fusion_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"  Baseline fusion (RRF): {len(baseline_fusion)} eredm√©ny, {fusion_time*1000:.1f}ms\")\n",
    "            \n",
    "            # GRPO reranking\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                reranked_results = reranker.rerank(bm25_results, dense_results)\n",
    "                reranking_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"  GRPO reranking: {len(reranked_results)} eredm√©ny, {reranking_time*1000:.1f}ms\")\n",
    "                \n",
    "                # Eredm√©nyek √∂sszehasonl√≠t√°sa\n",
    "                if reranked_results and baseline_fusion:\n",
    "                    # √Åtfed√©s sz√°m√≠t√°sa\n",
    "                    reranked_set = set([doc_id for doc_id, _ in reranked_results[:5]])\n",
    "                    baseline_set = set(baseline_fusion[:5])\n",
    "                    \n",
    "                    overlap = len(reranked_set & baseline_set) / len(baseline_set) if baseline_set else 0\n",
    "                    print(f\"  √Åtfed√©s baseline vs reranked (top 5): {overlap:.2f}\")\n",
    "                    \n",
    "                    # Elt√©r√©sek\n",
    "                    only_reranked = reranked_set - baseline_set\n",
    "                    only_baseline = baseline_set - reranked_set\n",
    "                    \n",
    "                    if only_reranked:\n",
    "                        print(f\"  Csak reranked-ben: {list(only_reranked)}\")\n",
    "                    if only_baseline:\n",
    "                        print(f\"  Csak baseline-ben: {list(only_baseline)}\")\n",
    "                    \n",
    "                    # Reranked eredm√©nyek megjelen√≠t√©se\n",
    "                    print(f\"\\nTop 5 reranked eredm√©ny:\")\n",
    "                    for i, (doc_id, score) in enumerate(reranked_results[:5], 1):\n",
    "                        print(f\"  {i}. {doc_id} (score: {score:.4f})\")\n",
    "                \n",
    "                # √ñsszefoglal√≥ adatok\n",
    "                results_summary.append({\n",
    "                    'query': query,\n",
    "                    'baseline_results': len(baseline_fusion),\n",
    "                    'baseline_time': baseline_time * 1000,\n",
    "                    'fusion_time': fusion_time * 1000,\n",
    "                    'reranking_time': reranking_time * 1000,\n",
    "                    'total_reranking_time': (baseline_time + reranking_time) * 1000,\n",
    "                    'overlap_ratio': overlap if 'overlap' in locals() else 0\n",
    "                })\n",
    "                \n",
    "            except Exception as rerank_e:\n",
    "                print(f\"‚ùå Reranking hiba: {rerank_e}\")\n",
    "                results_summary.append({\n",
    "                    'query': query,\n",
    "                    'baseline_results': len(baseline_fusion),\n",
    "                    'baseline_time': baseline_time * 1000,\n",
    "                    'fusion_time': fusion_time * 1000,\n",
    "                    'reranking_time': 0,\n",
    "                    'total_reranking_time': baseline_time * 1000,\n",
    "                    'overlap_ratio': 0\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Teszt hiba: {e}\")\n",
    "    \n",
    "    # √ñsszefoglal√≥ t√°bl√°zat\n",
    "    if results_summary:\n",
    "        results_df = pd.DataFrame(results_summary)\n",
    "        print(\"\\nüìä Reranking teljes√≠tm√©ny √∂sszefoglal√≥:\")\n",
    "        display(results_df.round(2))\n",
    "        \n",
    "        # √Åtlagos teljes√≠tm√©ny\n",
    "        print(\"\\nüìà √Åtlagos teljes√≠tm√©ny:\")\n",
    "        print(f\"  Baseline retrieval: {results_df['baseline_time'].mean():.1f}ms √°tlag\")\n",
    "        print(f\"  Fusion: {results_df['fusion_time'].mean():.1f}ms √°tlag\")\n",
    "        print(f\"  Reranking: {results_df['reranking_time'].mean():.1f}ms √°tlag\")\n",
    "        print(f\"  √ñsszes reranking: {results_df['total_reranking_time'].mean():.1f}ms √°tlag\")\n",
    "        print(f\"  √Åtfed√©s ar√°ny: {results_df['overlap_ratio'].mean():.2f} √°tlag\")\n",
    "        \n",
    "        # Teljes√≠tm√©ny vizualiz√°ci√≥\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        x = np.arange(len(test_queries))\n",
    "        width = 0.2\n",
    "        \n",
    "        plt.bar(x - width, results_df['baseline_time'], width, label='Baseline', alpha=0.7)\n",
    "        plt.bar(x, results_df['fusion_time'], width, label='Fusion', alpha=0.7)\n",
    "        plt.bar(x + width, results_df['reranking_time'], width, label='Reranking', alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Teszt lek√©rdez√©s')\n",
    "        plt.ylabel('Id≈ë (ms)')\n",
    "        plt.title('Reranking teljes√≠tm√©ny √∂sszehasonl√≠t√°sa')\n",
    "        plt.xticks(x, [f'Q{i+1}' for i in range(len(test_queries))], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Reranking tesztel√©s nem el√©rhet≈ë - hi√°nyz√≥ komponensek\")\n",
    "    if retriever is None:\n",
    "        print(\"üí° Sz√ºks√©ges: Hybrid retriever - futtassa: uv run courtrankrl build\")\n",
    "    if reranker is None:\n",
    "        print(\"üí° Sz√ºks√©ges: GRPO reranker - ellen≈ërizze a GRPOReranker implement√°ci√≥t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "policy-analysis",
   "metadata": {},
   "source": [
    "## 3. Policy Elemz√©se\n",
    "\n",
    "A betan√≠tott RL policy tulajdons√°gainak √©s teljes√≠tm√©ny√©nek elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reranker is not None and policy_loaded:\n",
    "    print(\"üß† Policy elemz√©se:\")\n",
    "    \n",
    "    try:\n",
    "        # Policy tulajdons√°gok\n",
    "        policy_info = reranker.get_policy_info()\n",
    "        \n",
    "        print(f\"Policy inform√°ci√≥k:\")\n",
    "        print(f\"  Policy t√≠pusa: {policy_info.get('type', 'N/A')}\")\n",
    "        print(f\"  Input dimenzi√≥: {policy_info.get('input_dim', 'N/A')}\")\n",
    "        print(f\"  Hidden dimenzi√≥: {policy_info.get('hidden_dim', config.RL_HIDDEN_DIM)}\")\n",
    "        print(f\"  Param√©terek sz√°ma: {policy_info.get('param_count', 'N/A'):,}\")\n",
    "        print(f\"  Tan√≠t√°si id≈ë: {policy_info.get('training_time', 'N/A')}\")\n",
    "        \n",
    "        # Policy teljes√≠tm√©ny metrik√°k\n",
    "        if 'metrics' in policy_info:\n",
    "            print(f\"\\nTan√≠t√°si metrik√°k:\")\n",
    "            metrics = policy_info['metrics']\n",
    "            for epoch, epoch_metrics in metrics.items():\n",
    "                print(f\"  Epoch {epoch}:\")\n",
    "                for metric, value in epoch_metrics.items():\n",
    "                    print(f\"    {metric}: {value:.4f}\")\n",
    "        \n",
    "        # Feature s√∫lyok elemz√©se (ha line√°ris policy)\n",
    "        if 'feature_weights' in policy_info:\n",
    "            print(f\"\\nFeature s√∫lyok:\")\n",
    "            weights = policy_info['feature_weights']\n",
    "            \n",
    "            # S√∫lyok vizualiz√°ci√≥ja\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            features = list(weights.keys())\n",
    "            weight_values = list(weights.values())\n",
    "            \n",
    "            plt.barh(features, weight_values)\n",
    "            plt.title('Feature s√∫lyok a policy-ben')\n",
    "            plt.xlabel('S√∫ly')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Legfontosabb feature-√∂k\n",
    "            sorted_weights = sorted(weights.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            print(f\"\\nLegfontosabb feature-√∂k:\")\n",
    "            for i, (feature, weight) in enumerate(sorted_weights[:5]):\n",
    "                print(f\"  {i+1}. {feature}: {weight:.4f}\")\n",
    "        \n",
    "        # Policy stabilit√°s\n",
    "        if 'stability_metrics' in policy_info:\n",
    "            print(f\"\\nPolicy stabilit√°s:\")\n",
    "            stability = policy_info['stability_metrics']\n",
    "            for metric, value in stability.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Policy elemz√©s hiba: {e}\")\n",
    "        print(\"Policy metrik√°k nem el√©rhet≈ëek\")\n",
    "else:\n",
    "    print(\"‚ùå Policy elemz√©s nem el√©rhet≈ë - hi√°nyz√≥ policy\")\n",
    "    if not policy_loaded:\n",
    "        print(\"üí° Policy bet√∂lt√©se sz√ºks√©ges: futtassa a train parancsot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-analysis",
   "metadata": {},
   "source": [
    "## 4. Feature Elemz√©se\n",
    "\n",
    "A GRPO reranking √°ltal haszn√°lt feature-√∂k elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retriever is not None and reranker is not None:\n",
    "    print(\"üìä Feature elemz√©se:\")\n",
    "    \n",
    "    # Teszt lek√©rdez√©s feature elemz√©shez\n",
    "    test_query = \"szerz≈ëd√©s felmond√°sa\"\n",
    "    \n",
    "    try:\n",
    "        # Kandid√°tusok lek√©r√©se\n",
    "        bm25_results, dense_results = retriever.retrieve_candidates(test_query, top_k=10)\n",
    "        \n",
    "        if bm25_results and dense_results:\n",
    "            # Features sz√°m√≠t√°sa\n",
    "            features = reranker.extract_features(bm25_results, dense_results, test_query)\n",
    "            \n",
    "            print(f\"\\nTeszt lek√©rdez√©s: '{test_query}'\")\n",
    "            print(f\"Feature m√°trix: {features.shape}\")\n",
    "            \n",
    "            # Feature statisztik√°k\n",
    "            feature_stats = pd.DataFrame({\n",
    "                'mean': features.mean(axis=0),\n",
    "                'std': features.std(axis=0),\n",
    "                'min': features.min(axis=0),\n",
    "                'max': features.max(axis=0)\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nFeature statisztik√°k:\")\n",
    "            display(feature_stats.round(4))\n",
    "            \n",
    "            # Feature korrel√°ci√≥\n",
    "            if features.shape[1] > 1:\n",
    "                correlation_matrix = np.corrcoef(features.T)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                          xticklabels=[f'F{i}' for i in range(features.shape[1])],\n",
    "                          yticklabels=[f'F{i}' for i in range(features.shape[1])])\n",
    "                plt.title('Feature korrel√°ci√≥ m√°trix')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Feature eloszl√°sok\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            for i in range(min(6, features.shape[1])):  # Maximum 6 feature\n",
    "                plt.subplot(2, 3, i+1)\n",
    "                plt.hist(features[:, i], bins=20, alpha=0.7)\n",
    "                plt.title(f'Feature {i} eloszl√°sa')\n",
    "                plt.xlabel(f'Feature {i} √©rt√©k')\n",
    "                plt.ylabel('Kandid√°tusok sz√°ma')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Policy score-ok\n",
    "            if policy_loaded:\n",
    "                try:\n",
    "                    scores = reranker.policy.predict(features)\n",
    "                    \n",
    "                    print(f\"\\nPolicy score statisztik√°k:\")\n",
    "                    print(f\"  √Åtlag score: {scores.mean():.4f}\")\n",
    "                    print(f\"  Score tartom√°ny: [{scores.min():.4f}, {scores.max():.4f}]\")\n",
    "                    \n",
    "                    # Score √©s feature kapcsolat\n",
    "                    print(f\"\\nTop 5 scoring kandid√°tus:\")\n",
    "                    top_indices = np.argsort(scores)[::-1][:5]\n",
    "                    for i, idx in enumerate(top_indices):\n",
    "                        doc_id = bm25_results[idx] if idx < len(bm25_results) else dense_results[idx - len(bm25_results)]\n",
    "                        print(f\"  {i+1}. {doc_id}: {scores[idx]:.4f}\")\n",
    "                        print(f\"      Features: {features[idx]}\")\n",
    "                        \n",
    "                except Exception as score_e:\n",
    "                    print(f\"‚ùå Score sz√°m√≠t√°s hiba: {score_e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Feature elemz√©s hiba: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Feature elemz√©s nem el√©rhet≈ë - hi√°nyz√≥ komponensek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-comparison",
   "metadata": {},
   "source": [
    "## 5. Ranking √ñsszehasonl√≠t√°s\n",
    "\n",
    "A baseline √©s GRPO reranked eredm√©nyek √∂sszehasonl√≠t√°sa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-rankings",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retriever is not None and reranker is not None and policy_loaded:\n",
    "    print(\"üìä Ranking √∂sszehasonl√≠t√°s:\")\n",
    "    \n",
    "    # Teszt lek√©rdez√©sek r√©szletes elemz√©shez\n",
    "    test_queries_detailed = [\n",
    "        \"szerz≈ëd√©s felmond√°sa\",\n",
    "        \"k√°rt√©r√≠t√©s m√©rt√©ke\",\n",
    "        \"csal√°djogi √ºgyek\"\n",
    "    ]\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for query in test_queries_detailed:\n",
    "        try:\n",
    "            # Baseline retrieval\n",
    "            baseline_results = retriever.retrieve(query, top_k=config.TOP_K_RERANKED, fusion_method=\"rrf\")\n",
    "            \n",
    "            # Reranked retrieval\n",
    "            bm25_results, dense_results = retriever.retrieve_candidates(query, top_k=config.TOP_K_BASELINE)\n",
    "            reranked_results = reranker.rerank(bm25_results, dense_results)\n",
    "            \n",
    "            if baseline_results and reranked_results:\n",
    "                # √ñsszehasonl√≠t√°s\n",
    "                baseline_set = set(baseline_results[:5])\n",
    "                reranked_set = set([doc_id for doc_id, _ in reranked_results[:5]])\n",
    "                \n",
    "                overlap = len(baseline_set & reranked_set) / len(baseline_set) if baseline_set else 0\n",
    "                \n",
    "                print(f\"\\nüîç Lek√©rdez√©s: '{query}'\")\n",
    "                print(f\"\\nBaseline ranking:\")\n",
    "                for i, doc_id in enumerate(baseline_results[:5], 1):\n",
    "                    print(f\"  {i}. {doc_id}\")\n",
    "                \n",
    "                print(f\"\\nReranked ranking:\")\n",
    "                for i, (doc_id, score) in enumerate(reranked_results[:5], 1):\n",
    "                    print(f\"  {i}. {doc_id} (score: {score:.4f})\")\n",
    "                \n",
    "                print(f\"\\n√Åtfed√©s (top 5): {overlap:.2f}\")\n",
    "                \n",
    "                # Kendall tau korrel√°ci√≥ (ha lehets√©ges)\n",
    "                try:\n",
    "                    from scipy.stats import kendalltau\n",
    "                    \n",
    "                    # K√∂z√∂s dokumentumok rangsorol√°sa\n",
    "                    common_docs = list(baseline_set & reranked_set)\n",
    "                    if len(common_docs) >= 3:  # Minimum 3 dokumentum a korrel√°ci√≥hoz\n",
    "                        baseline_ranks = [baseline_results.index(doc) for doc in common_docs]\n",
    "                        reranked_ranks = [reranked_results.index((doc, 0)) for doc in common_docs]\n",
    "                        \n",
    "                        tau, p_value = kendalltau(baseline_ranks, reranked_ranks)\n",
    "                        print(f\"  Kendall tau korrel√°ci√≥: {tau:.3f} (p-value: {p_value:.3f})\")\n",
    "                    \n",
    "                except ImportError:\n",
    "                    print(\"  (scipy nem el√©rhet≈ë a Kendall tau korrel√°ci√≥hoz)\")\n",
    "                except Exception as kendall_e:\n",
    "                    print(f\"  Kendall tau sz√°m√≠t√°s hiba: {kendall_e}\")\n",
    "                \n",
    "                comparison_results.append({\n",
    "                    'query': query,\n",
    "                    'overlap': overlap,\n",
    "                    'baseline_top1': baseline_results[0] if len(baseline_results) > 0 else None,\n",
    "                    'reranked_top1': reranked_results[0][0] if len(reranked_results) > 0 else None\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå √ñsszehasonl√≠t√°s hiba: {e}\")\n",
    "    \n",
    "    # √ñsszefoglal√≥\n",
    "    if comparison_results:\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        print(f\"\\nüìã √ñsszehasonl√≠t√°s √∂sszefoglal√≥:\")\n",
    "        display(comparison_df)\n",
    "        \n",
    "        print(f\"\\n√Åtlagos √°tfed√©s: {comparison_df['overlap'].mean():.2f}\")\n",
    "        \n",
    "        # Baseline vs reranked top-1 v√°ltoz√°s\n",
    "        top1_changes = comparison_df['baseline_top1'] != comparison_df['reranked_top1']\n",
    "        print(f\"Top-1 v√°ltoz√°s: {top1_changes.sum()}/{len(comparison_df)} lek√©rdez√©s\")\n",
    "else:\n",
    "    print(\"‚ùå Ranking √∂sszehasonl√≠t√°s nem el√©rhet≈ë - hi√°nyz√≥ komponensek vagy policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-data-analysis",
   "metadata": {},
   "source": [
    "## 6. Training Data Elemz√©se\n",
    "\n",
    "A GRPO training sor√°n haszn√°lt adatok √©s folyamat elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-training-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reranker is not None:\n",
    "    print(\"üéì Training data elemz√©se:\")\n",
    "    \n",
    "    try:\n",
    "        # Training data inform√°ci√≥k\n",
    "        training_info = reranker.get_training_info()\n",
    "        \n",
    "        if training_info:\n",
    "            print(f\"Training inform√°ci√≥k:\")\n",
    "            print(f\"  Qrels f√°jl: {training_info.get('qrels_file', 'N/A')}\")\n",
    "            print(f\"  Training mint√°k: {training_info.get('num_samples', 'N/A')}\")\n",
    "            print(f\"  Epoch-ok: {training_info.get('epochs', 'N/A')}\")\n",
    "            print(f\"  Batch size: {training_info.get('batch_size', config.RL_BATCH_SIZE)}\")\n",
    "            \n",
    "            # Training metrik√°k\n",
    "            if 'training_metrics' in training_info:\n",
    "                print(f\"\\nTraining metrik√°k:\")\n",
    "                metrics = training_info['training_metrics']\n",
    "                \n",
    "                # Metrik√°k vizualiz√°ci√≥ja\n",
    "                if isinstance(metrics, dict) and 'epochs' in metrics:\n",
    "                    epochs = metrics['epochs']\n",
    "                    \n",
    "                    plt.figure(figsize=(14, 8))\n",
    "                    \n",
    "                    # Loss g√∂rbe\n",
    "                    if 'loss' in metrics:\n",
    "                        plt.subplot(2, 2, 1)\n",
    "                        plt.plot(epochs, metrics['loss'])\n",
    "                        plt.title('Training Loss')\n",
    "                        plt.xlabel('Epoch')\n",
    "                        plt.ylabel('Loss')\n",
    "                        plt.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Reward g√∂rbe\n",
    "                    if 'reward' in metrics:\n",
    "                        plt.subplot(2, 2, 2)\n",
    "                        plt.plot(epochs, metrics['reward'])\n",
    "                        plt.title('Average Reward')\n",
    "                        plt.xlabel('Epoch')\n",
    "                        plt.ylabel('Reward')\n",
    "                        plt.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # nDCG g√∂rbe\n",
    "                    if 'ndcg' in metrics:\n",
    "                        plt.subplot(2, 2, 3)\n",
    "                        plt.plot(epochs, metrics['ndcg'])\n",
    "                        plt.title('nDCG Score')\n",
    "                        plt.xlabel('Epoch')\n",
    "                        plt.ylabel('nDCG')\n",
    "                        plt.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Policy entropy\n",
    "                    if 'entropy' in metrics:\n",
    "                        plt.subplot(2, 2, 4)\n",
    "                        plt.plot(epochs, metrics['entropy'])\n",
    "                        plt.title('Policy Entropy')\n",
    "                        plt.xlabel('Epoch')\n",
    "                        plt.ylabel('Entropy')\n",
    "                        plt.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                \n",
    "                # Legutols√≥ metrik√°k\n",
    "                print(f\"\\nLegutols√≥ epoch metrik√°k:\")\n",
    "                for metric, values in metrics.items():\n",
    "                    if isinstance(values, list) and values:\n",
    "                        print(f\"  {metric}: {values[-1]:.4f}\")\n",
    "                    elif not isinstance(values, list):\n",
    "                        print(f\"  {metric}: {values:.4f}\")\n",
    "            \n",
    "            # Qrels statisztik√°k\n",
    "            if 'qrels_stats' in training_info:\n",
    "                qrels_stats = training_info['qrels_stats']\n",
    "                print(f\"\\nQrels statisztik√°k:\")\n",
    "                for stat, value in qrels_stats.items():\n",
    "                    print(f\"  {stat}: {value}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Training inform√°ci√≥k nem el√©rhet≈ëek\")\n",
    "            print(\"A policy betan√≠t√°sa ut√°n lesznek el√©rhet≈ëek a r√©szletes metrik√°k\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training data elemz√©s hiba: {e}\")\n",
    "        print(\"Training metrik√°k nem el√©rhet≈ëek\")\n",
    "else:\n",
    "    print(\"‚ùå Training data elemz√©s nem el√©rhet≈ë - hi√°nyz√≥ reranker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ablation-study",
   "metadata": {},
   "source": [
    "## 7. Ablation Study\n",
    "\n",
    "A GRPO komponensek hat√°s√°nak vizsg√°lata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perform-ablation-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retriever is not None and reranker is not None:\n",
    "    print(\"üî¨ Ablation study:\")\n",
    "    \n",
    "    # Teszt lek√©rdez√©s\n",
    "    test_query = \"szerz≈ëd√©s felmond√°sa\"\n",
    "    \n",
    "    try:\n",
    "        # √ñsszes komponens bekapcsolva\n",
    "        bm25_results, dense_results = retriever.retrieve_candidates(test_query, top_k=config.TOP_K_BASELINE)\n",
    "        \n",
    "        # Baseline (csak RRF fusion)\n",
    "        baseline_results = retriever.retrieve(test_query, top_k=config.TOP_K_RERANKED, fusion_method=\"rrf\")\n",
    "        \n",
    "        # Reranking (BM25 + dense + policy)\n",
    "        if policy_loaded:\n",
    "            reranked_results = reranker.rerank(bm25_results, dense_results)\n",
    "            \n",
    "            # Csak BM25 (ablation)\n",
    "            bm25_only_results = reranker.rerank(bm25_results[:10], [])  # √úres dense lista\n",
    "            \n",
    "            # Csak dense (ablation)\n",
    "            dense_only_results = reranker.rerank([], dense_results[:10])  # √úres BM25 lista\n",
    "            \n",
    "            print(f\"\\nAblation study - '{test_query}':\")\n",
    "            \n",
    "            # Eredm√©nyek √∂sszehasonl√≠t√°sa\n",
    "            ablation_results = {\n",
    "                'Baseline (RRF)': baseline_results[:5],\n",
    "                'Reranked (full)': [doc_id for doc_id, _ in reranked_results[:5]],\n",
    "                'BM25 only': [doc_id for doc_id, _ in bm25_only_results[:5]],\n",
    "                'Dense only': [doc_id for doc_id, _ in dense_only_results[:5]]\n",
    "            }\n",
    "            \n",
    "            # √Åtfed√©sek sz√°m√≠t√°sa\n",
    "            baseline_set = set(ablation_results['Baseline (RRF)'])\n",
    "            reranked_set = set(ablation_results['Reranked (full)'])\n",
    "            bm25_set = set(ablation_results['BM25 only'])\n",
    "            dense_set = set(ablation_results['Dense only'])\n",
    "            \n",
    "            print(f\"\\n√Åtfed√©sek (top 5):\")\n",
    "            print(f\"  Baseline vs Reranked: {len(baseline_set & reranked_set)}/5\")\n",
    "            print(f\"  Baseline vs BM25: {len(baseline_set & bm25_set)}/5\")\n",
    "            print(f\"  Baseline vs Dense: {len(baseline_set & dense_set)}/5\")\n",
    "            print(f\"  BM25 vs Dense: {len(bm25_set & dense_set)}/5\")\n",
    "            \n",
    "            # Eredm√©nyek megjelen√≠t√©se\n",
    "            print(f\"\\nEredm√©nyek:\")\n",
    "            for method, results in ablation_results.items():\n",
    "                print(f\"\\n{method}:\")\n",
    "                for i, doc_id in enumerate(results, 1):\n",
    "                    print(f\"  {i}. {doc_id}\")\n",
    "            \n",
    "            # Ablation metrik√°k\n",
    "            print(f\"\\nAblation metrik√°k:\")\n",
    "            print(f\"  Baseline diverzit√°s: {len(baseline_set)} egyedi dokumentum\")\n",
    "            print(f\"  Reranked diverzit√°s: {len(reranked_set)} egyedi dokumentum\")\n",
    "            print(f\"  BM25 diverzit√°s: {len(bm25_set)} egyedi dokumentum\")\n",
    "            print(f\"  Dense diverzit√°s: {len(dense_set)} egyedi dokumentum\")\n",
    "            \n",
    "            # Kombin√°ci√≥ el≈ënye\n",
    "            combo_advantage = len(baseline_set & reranked_set) / len(baseline_set) if baseline_set else 0\n",
    "            print(f\"  Kombin√°ci√≥ el≈ënye: {combo_advantage:.2f} (baseline vs reranked √°tfed√©s)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Ablation study policy n√©lk√ºl nem teljes - csak baseline vs fusion √∂sszehasonl√≠t√°s\")\n",
    "            \n",
    "            print(f\"\\nBaseline vs Fusion √∂sszehasonl√≠t√°s:\")\n",
    "            baseline_results = retriever.retrieve(test_query, top_k=config.TOP_K_RERANKED, fusion_method=\"rrf\")\n",
    "            \n",
    "            print(f\"\\nBaseline (RRF) eredm√©nyek:\")\n",
    "            for i, doc_id in enumerate(baseline_results[:5], 1):\n",
    "                print(f\"  {i}. {doc_id}\")\n",
    "            \n",
    "            # Egyszer≈± fusion vs baseline √∂sszehasonl√≠t√°s\n",
    "            bm25_results, dense_results = retriever.retrieve_candidates(test_query, top_k=20)\n",
    "            \n",
    "            bm25_set = set(bm25_results[:5])\n",
    "            dense_set = set(dense_results[:5])\n",
    "            baseline_set = set(baseline_results[:5])\n",
    "            \n",
    "            print(f\"\\nBM25 top 5: {list(bm25_set)}\")\n",
    "            print(f\"Dense top 5: {list(dense_set)}\")\n",
    "            \n",
    "            print(f\"\\n√Åtfed√©sek:\")\n",
    "            print(f\"  BM25 vs Baseline: {len(bm25_set & baseline_set)}/5\")\n",
    "            print(f\"  Dense vs Baseline: {len(dense_set & baseline_set)}/5\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ablation study hiba: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Ablation study nem el√©rhet≈ë - hi√°nyz√≥ komponensek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 8. K√∂vetkeztet√©sek\n",
    "\n",
    "A GRPO reranking ki√©rt√©kel√©s√©nek √∂sszefoglal√°sa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-conclusions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== GRPO RERANKING ELEMZ√âS √ñSSZEFOGLAL√ì ===\")\n",
    "print(\"\\n‚úÖ Komponensek √°llapota:\")\n",
    "\n",
    "if retriever is not None:\n",
    "    print(f\"   üéØ Hybrid retriever: m≈±k√∂d≈ëk√©pes\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Hybrid retriever: nem el√©rhet≈ë\")\n",
    "\n",
    "if reranker is not None:\n",
    "    print(f\"   üß† GRPO reranker: m≈±k√∂d≈ëk√©pes\")\n",
    "else:\n",
    "    print(f\"   ‚ùå GRPO reranker: nem el√©rhet≈ë\")\n",
    "\n",
    "if policy_loaded:\n",
    "    print(f\"   üìà RL policy: bet√∂ltve\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è RL policy: nincs bet√∂ltve\")\n",
    "\n",
    "print(\"\\nüìã Agents.md specifik√°ci√≥ ellen≈ërz√©s:\")\n",
    "if reranker is not None and policy_loaded:\n",
    "    print(\"   ‚úÖ GRPO reranking komponens m≈±k√∂d≈ëk√©pes\")\n",
    "    print(\"   üéØ Features: dense similarity, BM25 score, rank difference\")\n",
    "    print(\"   üèÜ Reward: nDCG@10 group level\")\n",
    "    print(\"   ü§ñ Policy: linear/shallow MLP\")\n",
    "    print(\"   üìä Groupwise softmax\")\n",
    "    \n",
    "    # Teljes√≠tm√©ny ellen≈ërz√©s\n",
    "    try:\n",
    "        test_query = \"szerz≈ëd√©s felmond√°sa\"\n",
    "        bm25_results, dense_results = retriever.retrieve_candidates(test_query, top_k=10)\n",
    "        reranked_results = reranker.rerank(bm25_results, dense_results)\n",
    "        \n",
    "        if reranked_results:\n",
    "            print(\"   ‚úÖ Reranking m≈±k√∂dik - eredm√©nyek gener√°lva\")\n",
    "            \n",
    "            # Min≈ës√©gi metrik√°k\n",
    "            scores = [score for _, score in reranked_results[:10]]\n",
    "            if len(set(scores)) > 1:  # V√°ltozatos score-ok\n",
    "                print(\"   ‚úÖ Policy differenci√°l - v√°ltozatos score-ok\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è Policy egys√©ges - score-ok nem differenci√°lnak\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Reranking nem gener√°l eredm√©nyeket\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Reranking m≈±k√∂d√©si hiba: {e}\")\n",
    "else:\n",
    "    missing_components = []\n",
    "    if retriever is None:\n",
    "        missing_components.append(\"Hybrid retriever\")\n",
    "    if reranker is None:\n",
    "        missing_components.append(\"GRPO reranker\")\n",
    "    if not policy_loaded:\n",
    "        missing_components.append(\"RL policy\")\n",
    "    \n",
    "    print(f\"   ‚ùå Hi√°nyz√≥ komponensek: {', '.join(missing_components)}\")\n",
    "    print(\"   üí° Sz√ºks√©ges: uv run courtrankrl build + uv run courtrankrl train\")\n",
    "\n",
    "print(\"\\nüí° Aj√°nl√°sok:\")\n",
    "if reranker is not None and policy_loaded:\n",
    "    print(\"   ‚úÖ GRPO reranking haszn√°latra k√©sz\")\n",
    "    print(\"   üöÄ Haszn√°lat: uv run courtrankrl query \\\"lek√©rdez√©s\\\" --rerank\")\n",
    "    print(\"   üìà Tov√°bbi jav√≠t√°s: t√∂bb training data, finomhangolt hyperparameters\")\n",
    "else:\n",
    "    if not policy_loaded:\n",
    "        print(\"   üéì Policy betan√≠t√°sa sz√ºks√©ges: uv run courtrankrl train\")\n",
    "    print(\"   üîß Komponensek inicializ√°l√°sa sz√ºks√©ges\")\n",
    "\n",
    "print(\"\\nüéØ GRPO reranking elemz√©se k√©sz!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
