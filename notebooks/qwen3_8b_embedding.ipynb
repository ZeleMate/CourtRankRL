{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunPod A100 GPU - Könyvtárak telepítése és importálása\n",
        "!pip install -U torch sentence-transformers accelerate pyarrow pandas tqdm transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import psutil\n",
        "import time\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# A100 GPU optimalizáció\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(\"RunPod A100 környezet inicializálva!\")\n",
        "print(f\"CUDA elérhető: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memória: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunPod A100 konfiguráció\n",
        "print(\"RunPod A100 konfiguráció beállítása...\")\n",
        "\n",
        "# Fájl elérési utak RunPod-on\n",
        "INPUT_CSV = \"/workspace/cleaned_data_for_embedding.csv\"\n",
        "OUTPUT_PARQUET = \"/workspace/processed_documents_with_embeddings.parquet\"\n",
        "\n",
        "# Qwen3-8B A100 optimalizált paraméterek\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-8B\"\n",
        "EMBEDDING_DIMENSION = 8192\n",
        "BATCH_SIZE = 32            # A100 optimális\n",
        "CHUNK_SIZE = 5000          # Chunk méret\n",
        "MAX_TOKEN_LENGTH = 8192\n",
        "USE_MIXED_PRECISION = True\n",
        "MEMORY_LIMIT_GB = 75       # A100 80GB-ból 75GB használata\n",
        "\n",
        "print(f\"Bemeneti CSV: {INPUT_CSV}\")\n",
        "print(f\"Kimeneti Parquet: {OUTPUT_PARQUET}\")\n",
        "print(f\"Modell: {MODEL_NAME}\")\n",
        "print(f\"Dimenzió: {EMBEDDING_DIMENSION}\")\n",
        "print(f\"Batch méret: {BATCH_SIZE}\")\n",
        "print(f\"Chunk méret: {CHUNK_SIZE:,}\")\n",
        "print(f\"Mixed Precision: {USE_MIXED_PRECISION}\")\n",
        "print(f\"Memória limit: {MEMORY_LIMIT_GB}GB\")\n",
        "\n",
        "# Logging konfiguráció\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('/workspace/embedding_generation.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV adatok betöltése és validálása\n",
        "logger.info(\"CSV adatok validálása...\")\n",
        "\n",
        "# Fájl létezés ellenőrzése\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    raise FileNotFoundError(f\"CSV fájl nem található: {INPUT_CSV}\")\n",
        "\n",
        "# Mintaadatok betöltése struktúra ellenőrzéshez\n",
        "df_sample = pd.read_csv(INPUT_CSV, nrows=1000)\n",
        "logger.info(f\"Minta betöltve: {len(df_sample)} sor\")\n",
        "\n",
        "# Teljes fájl méret becslése\n",
        "total_rows = sum(1 for _ in open(INPUT_CSV, 'r', encoding='utf-8')) - 1\n",
        "logger.info(f\"Becsült teljes sorok: {total_rows:,}\")\n",
        "\n",
        "# Kötelező oszlopok ellenőrzése\n",
        "required_columns = ['text', 'doc_id']\n",
        "missing_columns = [col for col in required_columns if col not in df_sample.columns]\n",
        "if missing_columns:\n",
        "    raise ValueError(f\"Hiányzó kötelező oszlopok: {missing_columns}\")\n",
        "\n",
        "# Teljes metadata oszlop lista a preprocess_documents.py alapján\n",
        "expected_metadata_columns = [\n",
        "    'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',\n",
        "    'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',\n",
        "    'KapcsolodoHatarozatok', 'Jogszabalyhelyek'\n",
        "]\n",
        "\n",
        "# Jelenlegi oszlopok listázása\n",
        "available_columns = list(df_sample.columns)\n",
        "metadata_columns_present = [col for col in expected_metadata_columns if col in available_columns]\n",
        "metadata_columns_missing = [col for col in expected_metadata_columns if col not in available_columns]\n",
        "\n",
        "print(\"CSV validáció sikeres!\")\n",
        "print(f\"Teljes sorok: {total_rows:,}\")\n",
        "print(f\"Összes oszlop: {len(available_columns)}\")\n",
        "print(f\"Jelenlevő metadata oszlopok ({len(metadata_columns_present)}): {metadata_columns_present}\")\n",
        "if metadata_columns_missing:\n",
        "    print(f\"Hiányzó metadata oszlopok ({len(metadata_columns_missing)}): {metadata_columns_missing}\")\n",
        "\n",
        "# Szöveg hossz statisztikák\n",
        "text_lengths = df_sample['text'].str.len()\n",
        "print(f\"\\nSzöveg hossz statisztikák (minta):\")\n",
        "print(f\"  Átlag: {text_lengths.mean():.0f} karakter\")\n",
        "print(f\"  Medián: {text_lengths.median():.0f} karakter\")\n",
        "print(f\"  Min: {text_lengths.min():.0f} karakter\")\n",
        "print(f\"  Max: {text_lengths.max():.0f} karakter\")\n",
        "\n",
        "# Becsült feldolgozási idő\n",
        "estimated_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "estimated_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "print(f\"\\nBecsült feldolgozás:\")\n",
        "print(f\"  Chunk-ok száma: {estimated_chunks:,}\")\n",
        "print(f\"  Batch-ek száma: {estimated_batches:,}\")\n",
        "print(f\"  Becsült idő: 2-3 óra A100 GPU-n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qwen3-Embedding-8B modell osztály A100-ra optimalizálva\n",
        "logger.info(\"Qwen3-Embedding-8B modell osztály létrehozása...\")\n",
        "\n",
        "class OptimizedQwen3EmbeddingGenerator:\n",
        "    def __init__(self):\n",
        "        self.model_name = MODEL_NAME\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.dimension = EMBEDDING_DIMENSION\n",
        "        self.max_tokens = MAX_TOKEN_LENGTH\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        \n",
        "        # Teljesítmény követés\n",
        "        self.processed_count = 0\n",
        "        self.failed_count = 0\n",
        "        self.batch_times = []\n",
        "        self.peak_memory_usage = 0\n",
        "        \n",
        "        logger.info(f\"Device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Modell betöltése\n",
        "            logger.info(\"Qwen3-8B modell betöltése...\")\n",
        "            self.model = SentenceTransformer(\n",
        "                self.model_name,\n",
        "                device=self.device,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            # Mixed precision optimalizáció\n",
        "            if self.device == 'cuda' and USE_MIXED_PRECISION:\n",
        "                self.model.half()\n",
        "                logger.info(\"Mixed precision bekapcsolva\")\n",
        "            \n",
        "            # Modell warmup\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen betöltve és optimalizálva\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell betöltési hiba: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _warmup_model(self):\n",
        "        \"\"\"Modell warmup konzisztens teljesítményért\"\"\"\n",
        "        logger.info(\"Modell warmup...\")\n",
        "        dummy_texts = [\"Ez egy teszt szöveg a modell bemelegítéséhez.\"] * 4\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=USE_MIXED_PRECISION):\n",
        "            _ = self.model.encode(dummy_texts, show_progress_bar=False)\n",
        "        \n",
        "        self._cleanup_memory()\n",
        "        logger.info(\"Warmup befejezve\")\n",
        "    \n",
        "    def _cleanup_memory(self):\n",
        "        \"\"\"Memória tisztítás\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "    \n",
        "    def _monitor_memory(self):\n",
        "        \"\"\"GPU memória monitoring\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return {}\n",
        "        \n",
        "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "        \n",
        "        self.peak_memory_usage = max(self.peak_memory_usage, allocated)\n",
        "        \n",
        "        return {\n",
        "            'allocated_gb': allocated,\n",
        "            'reserved_gb': reserved,\n",
        "            'peak_usage_gb': self.peak_memory_usage\n",
        "        }\n",
        "\n",
        "# Embedding generátor inicializálása\n",
        "embedding_generator = OptimizedQwen3EmbeddingGenerator()\n",
        "print(\"Qwen3-8B modell sikeresen inicializálva!\")\n",
        "print(f\"Dimenzió: {embedding_generator.dimension}\")\n",
        "print(f\"Device: {embedding_generator.device}\")\n",
        "print(f\"Mixed Precision: {USE_MIXED_PRECISION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding generálás metódus hozzáadása\n",
        "def generate_embeddings_batch(self, texts):\n",
        "    \"\"\"A100 optimalizált batch embedding generálás\"\"\"\n",
        "    batch_start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Szöveg előfeldolgozás\n",
        "        processed_texts = []\n",
        "        for text in texts:\n",
        "            if len(text) > self.max_tokens * 3:  # ~3 char/token becslés\n",
        "                text = text[:self.max_tokens * 3]\n",
        "            processed_texts.append(text)\n",
        "        \n",
        "        # Mixed precision embedding generálás\n",
        "        with torch.cuda.amp.autocast(enabled=USE_MIXED_PRECISION):\n",
        "            embeddings = self.model.encode(\n",
        "                processed_texts,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True,\n",
        "                batch_size=len(processed_texts)\n",
        "            )\n",
        "        \n",
        "        # Dimenzió ellenőrzés és korrekció\n",
        "        if embeddings.shape[1] != self.dimension:\n",
        "            if embeddings.shape[1] > self.dimension:\n",
        "                embeddings = embeddings[:, :self.dimension]\n",
        "            else:\n",
        "                padding = np.zeros((embeddings.shape[0], self.dimension - embeddings.shape[1]))\n",
        "                embeddings = np.hstack([embeddings, padding])\n",
        "        \n",
        "        # Teljesítmény követés\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        self.batch_times.append(batch_time)\n",
        "        self.processed_count += len(texts)\n",
        "        \n",
        "        return embeddings.astype(np.float32)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Batch feldolgozási hiba: {e}\")\n",
        "        self.failed_count += len(texts)\n",
        "        # Fallback: NaN vektorok\n",
        "        return np.full((len(texts), self.dimension), np.nan, dtype=np.float32)\n",
        "    \n",
        "    finally:\n",
        "        # Rendszeres memória cleanup\n",
        "        if self.processed_count % 1000 == 0:\n",
        "            self._cleanup_memory()\n",
        "\n",
        "# Metódus hozzáadása az osztályhoz\n",
        "OptimizedQwen3EmbeddingGenerator.generate_embeddings_batch = generate_embeddings_batch\n",
        "\n",
        "print(\"Embedding generálás metódus hozzáadva!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segédfüggvények\n",
        "def create_metadata_json(row):\n",
        "    \"\"\"Teljes metadata JSON készítése az összes elérhető oszloppal\"\"\"\n",
        "    metadata = {\n",
        "        'doc_id': str(row.get('doc_id', '')),\n",
        "        'birosag': str(row.get('birosag', '')),\n",
        "        'JogTerulet': str(row.get('JogTerulet', '')),\n",
        "        'Azonosito': str(row.get('Azonosito', '')),\n",
        "        'MeghozoBirosag': str(row.get('MeghozoBirosag', '')),\n",
        "        'EgyediAzonosito': str(row.get('EgyediAzonosito', '')),\n",
        "        'HatarozatEve': str(row.get('HatarozatEve', '')),\n",
        "        'AllKapcsolodoUgyszam': str(row.get('AllKapcsolodoUgyszam', '')),\n",
        "        'AllKapcsolodoBirosag': str(row.get('AllKapcsolodoBirosag', '')),\n",
        "        'KapcsolodoHatarozatok': str(row.get('KapcsolodoHatarozatok', '')),\n",
        "        'Jogszabalyhelyek': str(row.get('Jogszabalyhelyek', '')),\n",
        "        'text_length': len(str(row.get('text', ''))),\n",
        "        'processed_timestamp': time.time()\n",
        "    }\n",
        "    return json.dumps(metadata, ensure_ascii=False)\n",
        "\n",
        "def adaptive_batch_size(text_lengths, base_batch_size=BATCH_SIZE):\n",
        "    \"\"\"Adaptív batch méret szöveg hossz alapján\"\"\"\n",
        "    avg_length = np.mean(text_lengths)\n",
        "    \n",
        "    if avg_length > 6000:\n",
        "        return max(8, base_batch_size // 4)\n",
        "    elif avg_length > 4000:\n",
        "        return max(16, base_batch_size // 2)\n",
        "    elif avg_length > 2000:\n",
        "        return base_batch_size\n",
        "    else:\n",
        "        return min(64, base_batch_size * 2)\n",
        "\n",
        "def prepare_final_columns(chunk_df):\n",
        "    \"\"\"Végső oszlopok előkészítése - összes metadata megőrzése\"\"\"\n",
        "    # Alapvető oszlopok (kötelező)\n",
        "    final_columns = ['doc_id', 'text', 'embedding', 'metadata_json']\n",
        "    \n",
        "    # Összes metadata oszlop hozzáadása, ha létezik\n",
        "    metadata_columns = [\n",
        "        'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',\n",
        "        'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', \n",
        "        'AllKapcsolodoBirosag', 'KapcsolodoHatarozatok', 'Jogszabalyhelyek'\n",
        "    ]\n",
        "    \n",
        "    # Csak a létező oszlopokat adjuk hozzá\n",
        "    for col in metadata_columns:\n",
        "        if col in chunk_df.columns:\n",
        "            final_columns.append(col)\n",
        "    \n",
        "    # Visszaadjuk a létező oszlopokat\n",
        "    available_columns = [col for col in final_columns if col in chunk_df.columns]\n",
        "    return available_columns\n",
        "\n",
        "print(\"Segédfüggvények betöltve!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A100 főfolyamat - Robosztus embedding generálás\n",
        "def process_embeddings_a100():\n",
        "    \"\"\"\n",
        "    A100 GPU-ra optimalizált robosztus embedding generálás\n",
        "    \"\"\"\n",
        "    \n",
        "    start_time = time.time()\n",
        "    logger.info(\"A100 embedding feldolgozás kezdése...\")\n",
        "    \n",
        "    total_rows = sum(1 for _ in open(INPUT_CSV, 'r', encoding='utf-8')) - 1\n",
        "    processed_rows = 0\n",
        "    all_results = []\n",
        "    \n",
        "    logger.info(f\"Feldolgozandó sorok: {total_rows:,}\")\n",
        "    logger.info(f\"Chunk méret: {CHUNK_SIZE:,}\")\n",
        "    logger.info(f\"Batch méret: {BATCH_SIZE}\")\n",
        "    \n",
        "    # Chunk-alapú feldolgozás\n",
        "    chunk_count = 0\n",
        "    total_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "    \n",
        "    with tqdm(total=total_chunks, desc=\"Chunk feldolgozás\", unit=\"chunk\") as chunk_pbar:\n",
        "        \n",
        "        for chunk_df in pd.read_csv(INPUT_CSV, chunksize=CHUNK_SIZE, encoding='utf-8'):\n",
        "            chunk_count += 1\n",
        "            chunk_start_time = time.time()\n",
        "            \n",
        "            # Adatok tisztítása\n",
        "            original_len = len(chunk_df)\n",
        "            chunk_df = chunk_df.dropna(subset=['text', 'doc_id'])\n",
        "            chunk_df['text'] = chunk_df['text'].astype(str)\n",
        "            chunk_df = chunk_df[chunk_df['text'].str.len() > 10]  # Min szöveghossz\n",
        "            \n",
        "            if len(chunk_df) == 0:\n",
        "                logger.warning(f\"Chunk {chunk_count}: nincs érvényes adat\")\n",
        "                chunk_pbar.update(1)\n",
        "                continue\n",
        "            \n",
        "            logger.info(f\"Chunk {chunk_count}/{total_chunks}: {len(chunk_df):,} érvényes sor\")\n",
        "            \n",
        "            # Szövegek és adaptív batch méret\n",
        "            texts = chunk_df['text'].tolist()\n",
        "            text_lengths = [len(text) for text in texts]\n",
        "            dynamic_batch_size = adaptive_batch_size(text_lengths, BATCH_SIZE)\n",
        "            \n",
        "            # Batch-es embedding generálás\n",
        "            all_embeddings = []\n",
        "            total_batches_in_chunk = (len(texts) + dynamic_batch_size - 1) // dynamic_batch_size\n",
        "            \n",
        "            with tqdm(total=total_batches_in_chunk, desc=f\"Chunk {chunk_count} batch-ek\", \n",
        "                     unit=\"batch\", leave=False) as batch_pbar:\n",
        "                \n",
        "                for batch_idx in range(0, len(texts), dynamic_batch_size):\n",
        "                    batch_texts = texts[batch_idx:batch_idx + dynamic_batch_size]\n",
        "                    \n",
        "                    # Embedding generálás hibakezeléssel\n",
        "                    try:\n",
        "                        batch_embeddings = embedding_generator.generate_embeddings_batch(batch_texts)\n",
        "                        all_embeddings.extend(batch_embeddings.tolist())\n",
        "                        \n",
        "                        # Memória monitoring\n",
        "                        memory_info = embedding_generator._monitor_memory()\n",
        "                        if memory_info.get('allocated_gb', 0) > MEMORY_LIMIT_GB * 0.9:\n",
        "                            logger.warning(f\"Magas memória: {memory_info.get('allocated_gb', 0):.1f}GB\")\n",
        "                            embedding_generator._cleanup_memory()\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Batch hiba: {e}\")\n",
        "                        # Fallback NaN vektorok\n",
        "                        nan_embeddings = np.full((len(batch_texts), EMBEDDING_DIMENSION), np.nan)\n",
        "                        all_embeddings.extend(nan_embeddings.tolist())\n",
        "                    \n",
        "                    batch_pbar.update(1)\n",
        "            \n",
        "            # Embedding számossági ellenőrzés\n",
        "            if len(all_embeddings) != len(chunk_df):\n",
        "                logger.error(f\"Embedding számossági hiba: {len(all_embeddings)} != {len(chunk_df)}\")\n",
        "                # Kiegészítés NaN-okkal\n",
        "                while len(all_embeddings) < len(chunk_df):\n",
        "                    all_embeddings.append(np.full(EMBEDDING_DIMENSION, np.nan).tolist())\n",
        "            \n",
        "            # Eredmények hozzáadása\n",
        "            chunk_df['embedding'] = all_embeddings\n",
        "            chunk_df['metadata_json'] = chunk_df.apply(create_metadata_json, axis=1)\n",
        "            \n",
        "            # Végső oszlopok - összes metadata megőrzése\n",
        "            available_columns = prepare_final_columns(chunk_df)\n",
        "            chunk_result = chunk_df[available_columns].copy()\n",
        "            \n",
        "            all_results.append(chunk_result)\n",
        "            processed_rows += len(chunk_df)\n",
        "            \n",
        "            # Progress update\n",
        "            chunk_time = time.time() - chunk_start_time\n",
        "            rows_per_sec = len(chunk_df) / chunk_time\n",
        "            \n",
        "            chunk_pbar.set_postfix({\n",
        "                'Sorok/sec': f'{rows_per_sec:.1f}',\n",
        "                'Memória': f'{embedding_generator._monitor_memory().get(\"allocated_gb\", 0):.1f}GB',\n",
        "                'Sikeres': embedding_generator.processed_count,\n",
        "                'Hibás': embedding_generator.failed_count\n",
        "            })\n",
        "            chunk_pbar.update(1)\n",
        "            \n",
        "            # Rendszeres cleanup\n",
        "            if chunk_count % 3 == 0:\n",
        "                embedding_generator._cleanup_memory()\n",
        "    \n",
        "    # Eredmények egyesítése\n",
        "    logger.info(\"DataFrame-ek egyesítése...\")\n",
        "    if not all_results:\n",
        "        raise ValueError(\"Nincs feldolgozott adat!\")\n",
        "    \n",
        "    final_df = pd.concat(all_results, ignore_index=True)\n",
        "    logger.info(f\"Egyesített DataFrame: {len(final_df):,} sor\")\n",
        "    \n",
        "    return final_df, processed_rows, time.time() - start_time\n",
        "\n",
        "# A100 főfolyamat indítása\n",
        "logger.info(\"A100 embedding feldolgozás indítása...\")\n",
        "final_df, processed_rows, total_time = process_embeddings_a100()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parquet mentés és végső validáció\n",
        "logger.info(\"Parquet mentés és validáció...\")\n",
        "\n",
        "# Embedding validáció\n",
        "valid_embeddings = 0\n",
        "nan_embeddings = 0\n",
        "dimension_errors = 0\n",
        "\n",
        "for idx, emb in enumerate(final_df['embedding']):\n",
        "    if isinstance(emb, list):\n",
        "        if len(emb) == EMBEDDING_DIMENSION:\n",
        "            if not np.any(np.isnan(emb)):\n",
        "                valid_embeddings += 1\n",
        "            else:\n",
        "                nan_embeddings += 1\n",
        "        else:\n",
        "            dimension_errors += 1\n",
        "    else:\n",
        "        dimension_errors += 1\n",
        "\n",
        "logger.info(f\"Embedding validáció:\")\n",
        "logger.info(f\"  Érvényes: {valid_embeddings:,}\")\n",
        "logger.info(f\"  NaN: {nan_embeddings:,}\")\n",
        "logger.info(f\"  Dimenzió hiba: {dimension_errors:,}\")\n",
        "\n",
        "# Parquet mentés\n",
        "logger.info(f\"Végső Parquet mentés: {OUTPUT_PARQUET}\")\n",
        "\n",
        "final_df.to_parquet(\n",
        "    OUTPUT_PARQUET,\n",
        "    engine='pyarrow',\n",
        "    index=False,\n",
        "    compression='snappy',\n",
        "    row_group_size=50000\n",
        ")\n",
        "\n",
        "# Fájl validáció\n",
        "file_size = os.path.getsize(OUTPUT_PARQUET) / (1024**3)\n",
        "\n",
        "# Gyors visszaolvasási teszt\n",
        "test_df = pd.read_parquet(OUTPUT_PARQUET, nrows=100)\n",
        "logger.info(f\"Visszaolvasási teszt sikeres: {len(test_df)} sor\")\n",
        "\n",
        "# Végső statisztikák\n",
        "logger.info(\"A100 QWEN3-8B EMBEDDING GENERÁLÁS BEFEJEZVE!\")\n",
        "logger.info(f\"Feldolgozott sorok: {processed_rows:,}\")\n",
        "logger.info(f\"Végső sorok: {len(final_df):,}\")\n",
        "logger.info(f\"Végső oszlopok ({len(final_df.columns)}): {list(final_df.columns)}\")\n",
        "logger.info(f\"Érvényes embeddings: {valid_embeddings:,}\")\n",
        "logger.info(f\"Fájl méret: {file_size:.2f}GB\")\n",
        "logger.info(f\"Teljes futási idő: {total_time/3600:.2f} óra\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"QWEN3-8B EMBEDDING FELDOLGOZÁS BEFEJEZVE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"📊 Feldolgozott dokumentumok: {processed_rows:,}\")\n",
        "print(f\"📁 Végső Parquet fájl: {OUTPUT_PARQUET}\")\n",
        "print(f\"📈 Oszlopok száma: {len(final_df.columns)}\")\n",
        "print(f\"🎯 Érvényes embeddings: {valid_embeddings:,}\")\n",
        "print(f\"💾 Fájl méret: {file_size:.2f}GB\")\n",
        "print(f\"⏱️  Futási idő: {total_time/3600:.2f} óra\")\n",
        "print(\"=\"*80)\n",
        "logger.info(f\"Teljes futási idő: {total_time/3600:.2f} óra\")\n",
        "logger.info(f\"Átlag sebesség: {processed_rows/total_time:.1f} sor/sec\")\n",
        "logger.info(f\"Fájl méret: {file_size:.2f} GB\")\n",
        "logger.info(f\"Csúcs memória: {embedding_generator.peak_memory_usage:.1f}GB\")\n",
        "\n",
        "print(\"\\nA100 QWEN3-8B EMBEDDING GENERÁLÁS SIKERESEN BEFEJEZVE!\")\n",
        "print(f\"Feldolgozott sorok: {processed_rows:,}\")\n",
        "print(f\"Érvényes embeddings: {valid_embeddings:,}\")\n",
        "print(f\"Fájl méret: {file_size:.2f} GB\")\n",
        "print(f\"Teljes idő: {total_time/3600:.2f} óra\")\n",
        "print(f\"Sebesség: {processed_rows/total_time:.1f} sor/sec\")\n",
        "print(f\"Csúcs memória: {embedding_generator.peak_memory_usage:.1f}GB\")\n",
        "print(f\"Sikerességi arány: {(valid_embeddings/len(final_df)*100):.1f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
