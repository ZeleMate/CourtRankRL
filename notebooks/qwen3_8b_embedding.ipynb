{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1. KÖRNYEZET BEÁLLÍTÁSA ===\n",
        "# Könyvtárak telepítése és importálása\n",
        "!pip install -U torch sentence-transformers accelerate pyarrow pandas tqdm transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# !!! KRITIKUS JAVÍTÁS: PyTorch memória töredezettségének kezelése !!!\n",
        "# A hibaüzenet javaslata alapján beállítjuk ezt a környezeti változót,\n",
        "# hogy a PyTorch rugalmasabban kezelje a GPU memóriát.\n",
        "# Ezt minden más PyTorch művelet előtt kell megtenni.\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# GPU optimalizáció A100-hoz\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "print(f\"CUDA elérhető: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2. KONFIGURÁCIÓ ===\n",
        "# RunPod felhő környezethez igazított konfiguráció.\n",
        "\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "# --- CSV OLVASÁSI LIMIT NÖVELÉSE ---\n",
        "try:\n",
        "    max_int = sys.maxsize\n",
        "    while True:\n",
        "        try:\n",
        "            csv.field_size_limit(max_int)\n",
        "            break\n",
        "        except OverflowError:\n",
        "            max_int = int(max_int / 10)\n",
        "except (ValueError, TypeError):\n",
        "    csv.field_size_limit(1_000_000_000)\n",
        "\n",
        "INPUT_CSV_PATH = Path(\"/workspace/cleaned_data_for_embedding.csv\")\n",
        "OUTPUT_PARQUET_PATH = Path(\"/workspace/documents_with_embeddings.parquet\")\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "EMBEDDING_DIMENSION = 1024\n",
        "# !!! VÉGSŐ BIZTONSÁGI INTÉZKEDÉS: BATCH MÉRET CSÖKKENTÉSE !!!\n",
        "BATCH_SIZE = 128  # Tovább csökkentjük 256-ról 128-ra\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "current_limit = csv.field_size_limit()\n",
        "logger.info(f\"CSV field size limit beállítva: {current_limit:,}\")\n",
        "\n",
        "logger.info(f\"Input: {INPUT_CSV_PATH}\")\n",
        "logger.info(f\"Output: {OUTPUT_PARQUET_PATH}\")\n",
        "logger.info(f\"Modell: {MODEL_NAME}\")\n",
        "logger.info(f\"Batch méret: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3. EMBEDDING GENERÁTOR OSZTÁLY ===\n",
        "# Ez az osztály tiszta és önálló, csak az embedding generálásra fókuszál.\n",
        "class EmbeddingGenerator:\n",
        "    def __init__(self, model_name: str, batch_size: int, dimension: int, device: str = 'cuda'):\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "        self.dimension = dimension\n",
        "        self.device = device if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = None\n",
        "        logger.info(f\"Generátor inicializálva a(z) '{self.device}' eszközön.\")\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model is not None:\n",
        "            logger.info(\"Modell már be van töltve.\")\n",
        "            return\n",
        "        try:\n",
        "            logger.info(f\"'{self.model_name}' modell betöltése...\")\n",
        "            self.model = SentenceTransformer(self.model_name, device=self.device, trust_remote_code=True)\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen betöltve és bemelegítve.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell betöltési hiba: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _warmup_model(self):\n",
        "        logger.info(\"Modell bemelegítése...\")\n",
        "        self.generate_embeddings([\"melegítés\"])\n",
        "        self._cleanup_memory()\n",
        "        logger.info(\"Bemelegítés kész.\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"A modell nincs betöltve. Hívd meg a load_model() metódust.\")\n",
        "        try:\n",
        "            embeddings = self.model.encode(\n",
        "                texts, \n",
        "                batch_size=self.batch_size, \n",
        "                normalize_embeddings=True, \n",
        "                show_progress_bar=True, # Legyen progress bar a konzolon\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "            if embeddings.shape[1] != self.dimension: # Biztonsági ellenőrzés\n",
        "                logger.warning(f\"Váratlan embedding dimenzió: {embeddings.shape[1]}. Korrekció {self.dimension}-ra.\")\n",
        "                embeddings = embeddings[:, :self.dimension]\n",
        "            return embeddings.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            # Részletesebb logolás a hiba jobb megértéséhez\n",
        "            problematic_text_snippet = texts[0][:200] if texts else \"Üres a szöveg lista\"\n",
        "            logger.error(f\"!!! KRITIKUS HIBA AZ EMBEDDING GENERÁLÁSKOR !!!\")\n",
        "            logger.error(f\"Hibaüzenet: {e}\")\n",
        "            logger.error(f\"A hibát okozó batch első szövegének részlete (első 200 karakter): '{problematic_text_snippet}'\")\n",
        "            \n",
        "            # Újra feldobjuk a hibát a teljes hiba-visszakövetésért (traceback)\n",
        "            raise\n",
        "\n",
        "    def _cleanup_memory(self):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4. FŐ FELDOLGOZÁSI FOLYAMAT ===\n",
        "def create_metadata_json(row: pd.Series) -> str:\n",
        "    metadata_cols = [col for col in row.index if col not in ['text', 'embedding']]\n",
        "    metadata_dict = row[metadata_cols].dropna().to_dict()\n",
        "    return json.dumps({k: str(v) for k, v in metadata_dict.items()}, ensure_ascii=False)\n",
        "\n",
        "def main():\n",
        "    logger.info(\"Feldolgozás indítása...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Bemeneti adatok beolvasása\n",
        "    if not INPUT_CSV_PATH.exists():\n",
        "        error_msg = f\"Hiba: A bemeneti fájl nem található: {INPUT_CSV_PATH}\"\n",
        "        logger.error(error_msg)\n",
        "        raise FileNotFoundError(error_msg)\n",
        "    \n",
        "    logger.info(f\"Bemeneti CSV beolvasása: {INPUT_CSV_PATH}\")\n",
        "    df = pd.read_csv(INPUT_CSV_PATH, engine='python', quoting=csv.QUOTE_ALL, on_bad_lines='warn')\n",
        "    logger.info(f\"{len(df):,} sor sikeresen beolvasva.\")\n",
        "\n",
        "    # Szövegek kinyerése és tisztítása\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    texts_to_process = df['text'].astype(str).tolist()\n",
        "    \n",
        "    if not texts_to_process:\n",
        "        logger.warning(\"Nincs feldolgozandó szöveg a bemeneti fájlban.\")\n",
        "        return\n",
        "\n",
        "    # Embedding generátor inicializálása\n",
        "    generator = EmbeddingGenerator(MODEL_NAME, BATCH_SIZE, EMBEDDING_DIMENSION)\n",
        "    generator.load_model()\n",
        "\n",
        "    # --- MEMÓRIAHATÉKONY FELDOLGOZÁS DARABOKBAN (CHUNK-OKBAN) ---\n",
        "    logger.info(\"Embedding generálás megkezdése memóriahatékony, darabolt módszerrel.\")\n",
        "    all_embeddings = []\n",
        "    # Biztonsági okokból csökkentett darabméret\n",
        "    processing_chunk_size = 4096 \n",
        "\n",
        "    for i in tqdm(range(0, len(texts_to_process), processing_chunk_size), desc=\"Adatdarabok feldolgozása\"):\n",
        "        batch_texts = texts_to_process[i:i + processing_chunk_size]\n",
        "        batch_embeddings = generator.generate_embeddings(batch_texts)\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "        \n",
        "        # !!! KRITIKUS JAVÍTÁS: GPU memória felszabadítása minden darab után !!!\n",
        "        generator._cleanup_memory()\n",
        "            \n",
        "    # Az összes darab embeddingjeinek összefűzése\n",
        "    embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "    \n",
        "    # Eredmények hozzáadása a DataFrame-hez\n",
        "    if len(embeddings) == len(df):\n",
        "        df['embedding'] = list(embeddings)\n",
        "    else:\n",
        "        logger.error(f\"KRITIKUS HIBA: Az embeddingek száma ({len(embeddings)}) nem egyezik a DataFrame sorainak számával ({len(df)}). A program leáll.\")\n",
        "        return\n",
        "\n",
        "    # Metaadatok generálása\n",
        "    tqdm.pandas(desc=\"Metaadat JSON generálása\")\n",
        "    df['metadata_json'] = df.progress_apply(create_metadata_json, axis=1)\n",
        "\n",
        "    # Kimeneti DataFrame és mentés Parquet formátumba\n",
        "    final_df = df[['doc_id', 'text', 'embedding', 'metadata_json']]\n",
        "    OUTPUT_PARQUET_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    logger.info(f\"Eredmények mentése a Parquet fájlba: {OUTPUT_PARQUET_PATH}\")\n",
        "    final_df.to_parquet(OUTPUT_PARQUET_PATH, index=False, compression='snappy')\n",
        "    \n",
        "    total_rows_processed = len(final_df)\n",
        "    total_time_seconds = time.time() - start_time\n",
        "    rows_per_second = total_rows_processed / total_time_seconds if total_time_seconds > 0 else 0\n",
        "    \n",
        "    # Összegzés\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✅ FELDOLGOZÁS BEFEJEZVE\")\n",
        "    print(f\"📄 Kimeneti fájl: {OUTPUT_PARQUET_PATH}\")\n",
        "    print(f\"⏱️ Teljes idő: {total_time_seconds:.2f} másodperc ({total_time_seconds / 60:.2f} perc)\")\n",
        "    print(f\"📊 Feldolgozott sorok: {total_rows_processed:,}\")\n",
        "    print(f\"⚡ Átlagos sebesség: {rows_per_second:.2f} sor/mp\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Fő folyamat futtatása\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5. VALIDÁCIÓ ===\n",
        "logger.info(\"Kimeneti Parquet fájl validálása...\")\n",
        "\n",
        "if OUTPUT_PARQUET_PATH.exists():\n",
        "    try:\n",
        "        parquet_file = pq.ParquetFile(OUTPUT_PARQUET_PATH)\n",
        "        file_num_rows = parquet_file.metadata.num_rows\n",
        "        file_size_mb = OUTPUT_PARQUET_PATH.stat().st_size / (1024 * 1024)\n",
        "        \n",
        "        df_sample = pd.read_parquet(OUTPUT_PARQUET_PATH, engine='pyarrow', use_threads=True).head(5)\n",
        "        sample_embedding = df_sample['embedding'].iloc[0]\n",
        "        \n",
        "        print(\"\\n✅ VALIDÁCIÓ SIKERES!\")\n",
        "        print(f\"  Fájl méret: {file_size_mb:.2f} MB\")\n",
        "        print(f\"  Sorok száma: {file_num_rows:,}\")\n",
        "        print(f\"  Oszlopok: {df_sample.columns.tolist()}\")\n",
        "        print(f\"  Első embedding dimenziója: {len(sample_embedding)}\")\n",
        "        print(\"\\n--- Minta Adatsor ---\")\n",
        "        display(df_sample)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Hiba a Parquet fájl validálása közben: {e}\")\n",
        "        print(f\"\\n❌ HIBA a validáció során: {e}\")\n",
        "else:\n",
        "    logger.error(\"A kimeneti Parquet fájl nem jött létre.\")\n",
        "    print(\"\\n❌ HIBA: A kimeneti fájl nem található!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
