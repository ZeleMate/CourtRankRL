{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1. KÖRNYEZET BEÁLLÍTÁSA ===\n",
        "# Könyvtárak telepítése és importálása\n",
        "%pip install -U torch sentence-transformers accelerate pyarrow pandas tqdm transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import time\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# GPU optimalizáció A100-hoz\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "print(f\"CUDA elérhető: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2. KONFIGURÁCIÓ ===\n",
        "# Feltételezzük, hogy a notebook a 'notebooks' mappában van.\n",
        "# A projekt gyökere a szülő mappa.\n",
        "project_root = Path.cwd().parent\n",
        "\n",
        "# Bemeneti útvonalak\n",
        "# Elsődleges: egyetlen, nagy CSV fájl\n",
        "UNIFIED_CSV_PATH = project_root / \"processed_data\" / \"cleaned_data_for_embedding.csv\"\n",
        "# Fallback: a chunk-olt CSV-ket tartalmazó mappa\n",
        "CHUNKED_INPUT_DIR = project_root / \"processed_data\" / \"chunked_cleaned\"\n",
        "\n",
        "# Kimenet: Egyetlen Parquet fájl\n",
        "OUTPUT_PARQUET = project_root / \"processed_data\" / \"documents_with_embeddings.parquet\"\n",
        "\n",
        "# Modell és feldolgozási paraméterek A100-ra optimalizálva\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "EMBEDDING_DIMENSION = 1024\n",
        "BATCH_SIZE = 256  # A100-on magasabb batch méretet használhatunk\n",
        "UNIFIED_CSV_CHUNK_SIZE = 10000 # Mekkora darabokban olvassuk a nagy CSV-t\n",
        "\n",
        "# Logging beállítása\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(f\"Elsődleges input: {UNIFIED_CSV_PATH}\")\n",
        "logger.info(f\"Fallback input: {CHUNKED_INPUT_DIR}\")\n",
        "logger.info(f\"Output fájl: {OUTPUT_PARQUET}\")\n",
        "logger.info(f\"Modell: {MODEL_NAME}\")\n",
        "logger.info(f\"Batch méret: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3. BEMENETI MÓD MEGHATÁROZÁSA ===\n",
        "INPUT_MODE = None\n",
        "if UNIFIED_CSV_PATH.exists():\n",
        "    INPUT_MODE = 'UNIFIED'\n",
        "    logger.info(f\"✅ Elsődleges mód kiválasztva: Egyetlen CSV fájl feldolgozása ({UNIFIED_CSV_PATH}).\")\n",
        "elif CHUNKED_INPUT_DIR.exists() and any(CHUNKED_INPUT_DIR.glob('*.csv')):\n",
        "    INPUT_MODE = 'CHUNKED'\n",
        "    logger.info(f\"⚠️  Fallback mód kiválasztva: Chunk-olt CSV-k feldolgozása ({CHUNKED_INPUT_DIR}).\")\n",
        "else:\n",
        "    error_msg = f\"Hiba: Sem az elsődleges input ({UNIFIED_CSV_PATH}), sem a fallback input ({CHUNKED_INPUT_DIR}) nem található.\"\n",
        "    logger.error(error_msg)\n",
        "    raise FileNotFoundError(error_msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4. EMBEDDING GENERÁTOR OSZTÁLY ===\n",
        "# Ez az osztály tiszta és önálló, csak az embedding generálásra fókuszál.\n",
        "class EmbeddingGenerator:\n",
        "    def __init__(self, model_name: str, batch_size: int, dimension: int, device: str = 'cuda'):\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "        self.dimension = dimension\n",
        "        self.device = device if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = None\n",
        "        logger.info(f\"Generátor inicializálva a(z) '{self.device}' eszközön.\")\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model is not None:\n",
        "            logger.info(\"Modell már be van töltve.\")\n",
        "            return\n",
        "        try:\n",
        "            logger.info(f\"'{self.model_name}' modell betöltése...\")\n",
        "            self.model = SentenceTransformer(self.model_name, device=self.device, trust_remote_code=True)\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen betöltve és bemelegítve.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell betöltési hiba: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _warmup_model(self):\n",
        "        logger.info(\"Modell bemelegítése...\")\n",
        "        self.generate_embeddings([\"melegítés\"])\n",
        "        self._cleanup_memory()\n",
        "        logger.info(\"Bemelegítés kész.\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"A modell nincs betöltve. Hívd meg a load_model() metódust.\")\n",
        "        try:\n",
        "            embeddings = self.model.encode(texts, batch_size=self.batch_size, normalize_embeddings=True, show_progress_bar=False, convert_to_numpy=True)\n",
        "            if embeddings.shape[1] != self.dimension: # Biztonsági ellenőrzés\n",
        "                logger.warning(f\"Váratlan embedding dimenzió: {embeddings.shape[1]}. Korrekció {self.dimension}-ra.\")\n",
        "                embeddings = embeddings[:, :self.dimension]\n",
        "            return embeddings.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Hiba az embedding generálás közben: {e}\")\n",
        "            return np.full((len(texts), self.dimension), np.nan, dtype=np.float32)\n",
        "\n",
        "    def _cleanup_memory(self):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5. FŐ FELDOLGOZÁSI FOLYAMAT ===\n",
        "def create_metadata_json(row: pd.Series) -> str:\n",
        "    metadata_cols = [col for col in row.index if col not in ['text', 'embedding']]\n",
        "    metadata_dict = row[metadata_cols].dropna().to_dict()\n",
        "    return json.dumps({k: str(v) for k, v in metadata_dict.items()}, ensure_ascii=False)\n",
        "\n",
        "def process_and_write_chunk(df_chunk, generator, parquet_writer):\n",
        "    \"\"\"Feldolgoz egy dataframe chunk-ot és kiírja a Parquet fájlba.\"\"\"\n",
        "    texts_to_process = df_chunk['text'].dropna().astype(str).tolist()\n",
        "    if not texts_to_process:\n",
        "        return 0\n",
        "\n",
        "    embeddings = generator.generate_embeddings(texts_to_process)\n",
        "    df_chunk['embedding'] = list(embeddings)\n",
        "    df_chunk['metadata_json'] = df_chunk.apply(create_metadata_json, axis=1)\n",
        "\n",
        "    final_df = df_chunk[['doc_id', 'text', 'embedding', 'metadata_json']]\n",
        "    pa_table = pa.Table.from_pandas(final_df, preserve_index=False)\n",
        "    \n",
        "    parquet_writer.write_table(pa_table)\n",
        "    return len(final_df)\n",
        "\n",
        "def main():\n",
        "    logger.info(f\"Feldolgozás indítása '{INPUT_MODE}' módban...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    generator = EmbeddingGenerator(MODEL_NAME, BATCH_SIZE, EMBEDDING_DIMENSION)\n",
        "    generator.load_model()\n",
        "    \n",
        "    total_rows_processed = 0\n",
        "    parquet_writer = None\n",
        "\n",
        "    try:\n",
        "        # A séma meghatározásához beolvasunk egyetlen sort\n",
        "        if INPUT_MODE == 'UNIFIED':\n",
        "            schema_df = pd.read_csv(UNIFIED_CSV_PATH, nrows=1, engine='python')\n",
        "        else: # CHUNKED\n",
        "            chunk_files_list = sorted(CHUNKED_INPUT_DIR.glob(\"cleaned_chunk_*.csv\"))\n",
        "            schema_df = pd.read_csv(chunk_files_list[0], nrows=1, engine='python')\n",
        "\n",
        "        schema_df['embedding'] = [np.zeros(EMBEDDING_DIMENSION, dtype=np.float32)]\n",
        "        schema_df['metadata_json'] = \"\"\n",
        "        output_schema = pa.Table.from_pandas(schema_df[['doc_id', 'text', 'embedding', 'metadata_json']], preserve_index=False).schema\n",
        "\n",
        "        OUTPUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
        "        parquet_writer = pq.ParquetWriter(OUTPUT_PARQUET, output_schema, compression='snappy')\n",
        "\n",
        "        # Feldolgozás a kiválasztott mód szerint\n",
        "        if INPUT_MODE == 'UNIFIED':\n",
        "            # A nagy CSV darabonkénti olvasása\n",
        "            row_count = sum(1 for row in open(UNIFIED_CSV_PATH)) -1\n",
        "            with tqdm(total=row_count, desc=\"Sorok feldolgozása\", unit=\"sor\") as pbar:\n",
        "                for df_chunk in pd.read_csv(UNIFIED_CSV_PATH, chunksize=UNIFIED_CSV_CHUNK_SIZE, engine='python'):\n",
        "                    rows_done = process_and_write_chunk(df_chunk, generator, parquet_writer)\n",
        "                    total_rows_processed += rows_done\n",
        "                    pbar.update(rows_done)\n",
        "        \n",
        "        elif INPUT_MODE == 'CHUNKED':\n",
        "            chunk_files = sorted(CHUNKED_INPUT_DIR.glob(\"cleaned_chunk_*.csv\"))\n",
        "            for chunk_file in tqdm(chunk_files, desc=\"Chunk fájlok feldolgozása\", unit=\"fájl\"):\n",
        "                df_chunk = pd.read_csv(chunk_file, engine='python')\n",
        "                total_rows_processed += process_and_write_chunk(df_chunk, generator, parquet_writer)\n",
        "\n",
        "    finally:\n",
        "        if parquet_writer:\n",
        "            parquet_writer.close()\n",
        "            logger.info(\"Parquet író sikeresen lezárva.\")\n",
        "\n",
        "    # Összegzés\n",
        "    total_time_seconds = time.time() - start_time\n",
        "    rows_per_second = total_rows_processed / total_time_seconds if total_time_seconds > 0 else 0\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✅ FELDOLGOZÁS BEFEJEZVE\")\n",
        "    print(f\"📄 Kimeneti fájl: {OUTPUT_PARQUET}\")\n",
        "    print(f\"⏱️ Teljes idő: {total_time_seconds / 60:.2f} perc\")\n",
        "    print(f\"📊 Feldolgozott sorok: {total_rows_processed:,}\")\n",
        "    print(f\"⚡ Átlagos sebesség: {rows_per_second:.2f} sor/mp\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Fő folyamat futtatása\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6. VALIDÁCIÓ ===\n",
        "logger.info(\"Kimeneti Parquet fájl validálása...\")\n",
        "\n",
        "if OUTPUT_PARQUET.exists():\n",
        "    try:\n",
        "        parquet_file = pq.ParquetFile(OUTPUT_PARQUET)\n",
        "        file_num_rows = parquet_file.metadata.num_rows\n",
        "        file_size_mb = OUTPUT_PARQUET.stat().st_size / (1024 * 1024)\n",
        "        \n",
        "        df_sample = pd.read_parquet(OUTPUT_PARQUET, nrows=5)\n",
        "        sample_embedding = df_sample['embedding'].iloc[0]\n",
        "        \n",
        "        print(\"\\n✅ VALIDÁCIÓ SIKERES!\")\n",
        "        print(f\"  Fájl méret: {file_size_mb:.2f} MB\")\n",
        "        print(f\"  Sorok száma: {file_num_rows:,}\")\n",
        "        print(f\"  Oszlopok: {df_sample.columns.tolist()}\")\n",
        "        print(f\"  Első embedding dimenziója: {len(sample_embedding)}\")\n",
        "        print(\"\\n--- Minta Adatsor ---\")\n",
        "        display(df_sample)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Hiba a Parquet fájl validálása közben: {e}\")\n",
        "        print(f\"\\n❌ HIBA a validáció során: {e}\")\n",
        "else:\n",
        "    logger.error(\"A kimeneti Parquet fájl nem jött létre.\")\n",
        "    print(\"\\n❌ HIBA: A kimeneti fájl nem található!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
