{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunPod A100 GPU - K√∂nyvt√°rak telep√≠t√©se √©s import√°l√°sa\n",
        "%pip install --upgrade pip\n",
        "%pip install -U torch sentence-transformers accelerate pyarrow pandas tqdm transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import psutil\n",
        "import time\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# A100 GPU optimaliz√°ci√≥ + mem√≥ria fragment√°ci√≥ jav√≠t√°s\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(\"RunPod A100 k√∂rnyezet inicializ√°lva!\")\n",
        "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunPod A100 konfigur√°ci√≥\n",
        "print(\"RunPod A100 konfigur√°ci√≥ be√°ll√≠t√°sa...\")\n",
        "\n",
        "# F√°jl el√©r√©si utak RunPod-on\n",
        "INPUT_CSV = \"/workspace/cleaned_data_for_embedding.csv\"\n",
        "OUTPUT_PARQUET = \"/workspace/processed_documents_with_embeddings.parquet\"\n",
        "\n",
        "# üö® CRISIS MODE: Ha lass√∫, kapcsold √°t!\n",
        "# MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # üöÄ VILL√ÅMGYORS! \n",
        "# EMBEDDING_DIMENSION = 384\n",
        "# BATCH_SIZE = 512\n",
        "\n",
        "# K√∂lts√©g-optimaliz√°lt konfigur√°ci√≥: 10 √≥ra alatt, $25 limit\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"  # Kisebb modell a sebess√©g√©rt\n",
        "EMBEDDING_DIMENSION = 1024  # Qwen3-0.6B val√≥di dimenzi√≥ja\n",
        "BATCH_SIZE = 256           # NAGY batch (0.6B-hez ak√°r 512 is megy)\n",
        "CHUNK_SIZE = 5000          # Standard chunk m√©ret\n",
        "USE_MIXED_PRECISION = False # Stabilit√°s √©rdek√©ben\n",
        "MEMORY_LIMIT_GB = 70       # Standard mem√≥ria limit\n",
        "\n",
        "print(f\"Bemeneti CSV: {INPUT_CSV}\")\n",
        "print(f\"Kimeneti Parquet: {OUTPUT_PARQUET}\")\n",
        "print(f\"Modell: {MODEL_NAME}\")\n",
        "print(f\"Dimenzi√≥: {EMBEDDING_DIMENSION}\")\n",
        "print(f\"Batch m√©ret: {BATCH_SIZE}\")\n",
        "print(f\"Chunk m√©ret: {CHUNK_SIZE:,}\")\n",
        "print(f\"Mixed Precision: {USE_MIXED_PRECISION}\")\n",
        "print(f\"Mem√≥ria limit: {MEMORY_LIMIT_GB}GB\")\n",
        "print(\"Alap√©rtelmezett konfigur√°ci√≥ - tesztel√©si f√°zis\")\n",
        "\n",
        "# Logging konfigur√°ci√≥\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('/workspace/embedding_generation.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHUNKED INPUT T√ÅMOGAT√ÅS - adatok bet√∂lt√©se √©s valid√°l√°sa\n",
        "logger.info(\"Chunked input-kompatibilis adatvalid√°l√°s...\")\n",
        "\n",
        "# ===== 1. CHUNKED CLEANED INPUT ELLEN≈êRZ√âSE (PRIORIT√ÅS) =====\n",
        "CHUNKED_CLEANED_DIR = \"/workspace/processed_data/chunked_cleaned\"\n",
        "CHUNKED_INPUT_MODE = False\n",
        "cleaned_chunk_files = []\n",
        "\n",
        "if os.path.exists(CHUNKED_CLEANED_DIR):\n",
        "    cleaned_chunk_files = sorted([\n",
        "        os.path.join(CHUNKED_CLEANED_DIR, f) \n",
        "        for f in os.listdir(CHUNKED_CLEANED_DIR) \n",
        "        if f.startswith(\"cleaned_chunk_\") and f.endswith(\".csv\")\n",
        "    ])\n",
        "    \n",
        "    if cleaned_chunk_files:\n",
        "        CHUNKED_INPUT_MODE = True\n",
        "        logger.info(f\"üéØ CHUNKED INPUT M√ìD: {len(cleaned_chunk_files)} cleaned chunk tal√°lhat√≥\")\n",
        "\n",
        "# ===== 2. UNIFIED CSV FALLBACK =====\n",
        "if not CHUNKED_INPUT_MODE:\n",
        "    if not os.path.exists(INPUT_CSV):\n",
        "        raise FileNotFoundError(f\"Nincs el√©rhet≈ë input! Sem chunked ({CHUNKED_CLEANED_DIR}), sem unified ({INPUT_CSV})\")\n",
        "    logger.info(\"üìÑ UNIFIED CSV M√ìD: Fallback unified CSV-re\")\n",
        "\n",
        "# ===== 3. MINTAADATOK BET√ñLT√âSE VALID√ÅL√ÅSHOZ =====\n",
        "if CHUNKED_INPUT_MODE:\n",
        "    # Els≈ë chunk-b√≥l minta\n",
        "    df_sample = pd.read_csv(cleaned_chunk_files[0], nrows=1000)\n",
        "    logger.info(f\"Minta bet√∂ltve els≈ë chunk-b√≥l: {len(df_sample)} sor\")\n",
        "    \n",
        "    # Teljes sorok becsl√©se chunk-okb√≥l\n",
        "    total_rows = 0\n",
        "    for chunk_file in cleaned_chunk_files:\n",
        "        chunk_rows = sum(1 for _ in open(chunk_file, 'r', encoding='utf-8')) - 1\n",
        "        total_rows += chunk_rows\n",
        "    logger.info(f\"Becs√ºlt teljes sorok (chunked): {total_rows:,}\")\n",
        "else:\n",
        "    # Unified CSV minta\n",
        "    df_sample = pd.read_csv(INPUT_CSV, nrows=1000)\n",
        "    logger.info(f\"Minta bet√∂ltve unified CSV-b≈ël: {len(df_sample)} sor\")\n",
        "    \n",
        "    # Teljes f√°jl m√©ret becsl√©se\n",
        "    total_rows = sum(1 for _ in open(INPUT_CSV, 'r', encoding='utf-8')) - 1\n",
        "    logger.info(f\"Becs√ºlt teljes sorok (unified): {total_rows:,}\")\n",
        "\n",
        "# ===== 4. OSZLOP VALID√ÅL√ÅS (K√ñZ√ñS LOGIKA) =====\n",
        "# K√∂telez≈ë oszlopok ellen≈ërz√©se\n",
        "required_columns = ['text', 'doc_id']\n",
        "missing_columns = [col for col in required_columns if col not in df_sample.columns]\n",
        "if missing_columns:\n",
        "    raise ValueError(f\"Hi√°nyz√≥ k√∂telez≈ë oszlopok: {missing_columns}\")\n",
        "\n",
        "# Teljes metadata oszlop lista\n",
        "expected_metadata_columns = [\n",
        "    'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',\n",
        "    'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',\n",
        "    'KapcsolodoHatarozatok', 'Jogszabalyhelyek'\n",
        "]\n",
        "\n",
        "# Jelenlegi oszlopok list√°z√°sa\n",
        "available_columns = list(df_sample.columns)\n",
        "metadata_columns_present = [col for col in expected_metadata_columns if col in available_columns]\n",
        "metadata_columns_missing = [col for col in expected_metadata_columns if col not in available_columns]\n",
        "\n",
        "# ===== 5. EREDM√âNYEK =====\n",
        "input_mode = \"CHUNKED\" if CHUNKED_INPUT_MODE else \"UNIFIED\"\n",
        "print(f\"\\n‚úÖ {input_mode} INPUT VALID√ÅCI√ì SIKERES!\")\n",
        "print(f\"üìä Teljes sorok: {total_rows:,}\")\n",
        "if CHUNKED_INPUT_MODE:\n",
        "    print(f\"üìÅ Chunk f√°jlok: {len(cleaned_chunk_files)}\")\n",
        "print(f\"üìã √ñsszes oszlop: {len(available_columns)}\")\n",
        "print(f\"‚úÖ Jelenlev≈ë metadata oszlopok ({len(metadata_columns_present)}): {metadata_columns_present}\")\n",
        "if metadata_columns_missing:\n",
        "    print(f\"‚ö†Ô∏è  Hi√°nyz√≥ metadata oszlopok ({len(metadata_columns_missing)}): {metadata_columns_missing}\")\n",
        "\n",
        "# ===== 6. SZ√ñVEG STATISZTIK√ÅK =====\n",
        "text_lengths = df_sample['text'].str.len()\n",
        "print(f\"\\nüìù Sz√∂veg hossz statisztik√°k (minta):\")\n",
        "print(f\"  √Åtlag: {text_lengths.mean():.0f} karakter\")\n",
        "print(f\"  Medi√°n: {text_lengths.median():.0f} karakter\")\n",
        "print(f\"  Min: {text_lengths.min():.0f} karakter\")\n",
        "print(f\"  Max: {text_lengths.max():.0f} karakter\")\n",
        "\n",
        "# ===== 7. FELDOLGOZ√ÅSI BECSL√âS =====\n",
        "estimated_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "estimated_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "print(f\"\\n‚ö° Becs√ºlt feldolgoz√°s:\")\n",
        "print(f\"  Chunk-ok sz√°ma: {estimated_chunks:,}\")\n",
        "print(f\"  Batch-ek sz√°ma: {estimated_batches:,}\")\n",
        "print(f\"  Input m√≥d: {input_mode}\")\n",
        "if CHUNKED_INPUT_MODE:\n",
        "    print(f\"  üöÄ Memory-optimaliz√°lt chunked feldolgoz√°s!\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Memory-intenz√≠v unified feldolgoz√°s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimaliz√°lt Qwen3-Embedding-0.6B modell oszt√°ly (STABIL VERZI√ì)\n",
        "logger.info(\"Optimaliz√°lt Qwen3-Embedding-0.6B modell oszt√°ly l√©trehoz√°sa...\")\n",
        "\n",
        "class OptimizedQwen3EmbeddingGenerator:\n",
        "    def __init__(self):\n",
        "        self.model_name = MODEL_NAME\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.dimension = EMBEDDING_DIMENSION\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        \n",
        "        # Teljes√≠tm√©ny k√∂vet√©s\n",
        "        self.processed_count = 0\n",
        "        self.failed_count = 0\n",
        "        self.batch_times = []\n",
        "        self.peak_memory_usage = 0\n",
        "        \n",
        "        logger.info(f\"Device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Alap√©rtelmezett modell bet√∂lt√©s - STABIL konfigur√°ci√≥\n",
        "            logger.info(\"Qwen3-0.6B modell bet√∂lt√©se (STABIL)...\")\n",
        "            self.model = SentenceTransformer(\n",
        "                self.model_name,\n",
        "                device=self.device,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            # GPU mem√≥ria kezel√©s\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "            # Modell warmup\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen inicializ√°lva!\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell bet√∂lt√©s hiba: {e}\")\n",
        "            raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qwen3-Embedding-0.6B modell oszt√°ly alap√©rtelmezett konfigur√°ci√≥val\n",
        "logger.info(\"Qwen3-Embedding-0.6B modell oszt√°ly l√©trehoz√°sa...\")\n",
        "\n",
        "class OptimizedQwen3EmbeddingGenerator:\n",
        "    def __init__(self):\n",
        "        self.model_name = MODEL_NAME\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.dimension = EMBEDDING_DIMENSION\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        \n",
        "        # Teljes√≠tm√©ny k√∂vet√©s\n",
        "        self.processed_count = 0\n",
        "        self.failed_count = 0\n",
        "        self.batch_times = []\n",
        "        self.peak_memory_usage = 0\n",
        "        \n",
        "        logger.info(f\"Device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Alap√©rtelmezett modell bet√∂lt√©s\n",
        "            logger.info(\"Qwen3-0.6B modell bet√∂lt√©se...\")\n",
        "            self.model = SentenceTransformer(\n",
        "                self.model_name,\n",
        "                device=self.device,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            # Alapvet≈ë GPU mem√≥ria kezel√©s\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "                logger.info(\"GPU mem√≥ria tiszt√≠tva\")\n",
        "                \n",
        "            # Modell warmup\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen bet√∂ltve √©s inicializ√°lva\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell bet√∂lt√©si hiba: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _warmup_model(self):\n",
        "        \"\"\"Modell warmup konzisztens teljes√≠tm√©ny√©rt\"\"\"\n",
        "        logger.info(\"Modell warmup...\")\n",
        "        dummy_texts = [\"Ez egy teszt sz√∂veg a modell bemeleg√≠t√©s√©hez.\"] * 8\n",
        "        \n",
        "        try:\n",
        "            _ = self.model.encode(dummy_texts, show_progress_bar=False)\n",
        "            logger.info(\"Warmup sikeresen befejezve\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Warmup hiba: {e}\")\n",
        "        \n",
        "        self._cleanup_memory()\n",
        "    \n",
        "    def _cleanup_memory(self):\n",
        "        \"\"\"Alapvet≈ë mem√≥ria tiszt√≠t√°s\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    def _monitor_memory(self):\n",
        "        \"\"\"GPU mem√≥ria monitoring\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return {}\n",
        "        \n",
        "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "        \n",
        "        self.peak_memory_usage = max(self.peak_memory_usage, allocated)\n",
        "        \n",
        "        return {\n",
        "            'allocated_gb': allocated,\n",
        "            'reserved_gb': reserved,\n",
        "            'peak_usage_gb': self.peak_memory_usage\n",
        "        }\n",
        "\n",
        "# Embedding gener√°tor inicializ√°l√°sa\n",
        "embedding_generator = OptimizedQwen3EmbeddingGenerator()\n",
        "print(\"Qwen3-0.6B modell sikeresen inicializ√°lva!\")\n",
        "print(f\"Dimenzi√≥: {embedding_generator.dimension}\")\n",
        "print(f\"Device: {embedding_generator.device}\")\n",
        "print(\"Teljes√≠tm√©ny tesztel√©s - baseline m√©r√©s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIAGNOSZTIKA - Futtasd a modell inicializ√°l√°sa ut√°n!\n",
        "print(\"=== KRITIKUS DIAGNOSZTIKA ===\")\n",
        "print(f\"Device: {embedding_generator.device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Model on device: {next(embedding_generator.model.parameters()).device}\")\n",
        "print(f\"Actual embedding dim: {embedding_generator.dimension}\")\n",
        "\n",
        "# üöÄ SEBESS√âG TESZT - 50 sz√∂veggel (re√°lis m√©ret)\n",
        "test_texts = [f\"Ez egy teszt sz√∂veg a b√≠r√≥s√°gi hat√°rozat feldolgoz√°s√°hoz. Sz√°m: {i}. Lorem ipsum dolor sit amet, consectetur adipiscing elit.\" for i in range(50)]\n",
        "\n",
        "print(f\"Test sz√∂vegek hossza: {len(test_texts[0])} karakter\")\n",
        "\n",
        "start_time = time.time()\n",
        "test_embeddings = embedding_generator.model.encode(test_texts, batch_size=BATCH_SIZE, show_progress_bar=False)\n",
        "test_time = time.time() - start_time\n",
        "\n",
        "print(f\"50 sz√∂veg: {test_time:.2f} sec\")\n",
        "print(f\"‚ö° SEBESS√âG: {50/test_time:.1f} sor/sec\")\n",
        "print(f\"Test embedding shape: {test_embeddings.shape}\")\n",
        "\n",
        "# üìä BECSL√âS 213,000 sorra\n",
        "total_rows = 213000\n",
        "estimated_hours = (total_rows / (50/test_time)) / 3600\n",
        "estimated_cost = estimated_hours * 2.10  # $2.10/hour RunPod A100\n",
        "\n",
        "print(f\"\\nüìä BECSL√âS:\")\n",
        "print(f\"213,000 sor: {estimated_hours:.1f} √≥ra\")\n",
        "print(f\"Becs√ºlt k√∂lts√©g: ${estimated_cost:.1f}\")\n",
        "\n",
        "# üö® KRITIKUS D√ñNT√âS\n",
        "if 50/test_time < 3:\n",
        "    print(\"üö® T√öLLASS√ö! Modellv√°lt√°s sz√ºks√©ges!\")\n",
        "elif 50/test_time < 6:\n",
        "    print(\"‚ö†Ô∏è Lass√∫, de elfogadhat√≥\")\n",
        "else:\n",
        "    print(\"‚úÖ J√≥ sebess√©g!\")\n",
        "\n",
        "# GPU mem√≥ria info\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU memory: {torch.cuda.memory_allocated()/1024**3:.1f}GB allocated\")\n",
        "    print(f\"GPU memory: {torch.cuda.memory_reserved()/1024**3:.1f}GB reserved\")\n",
        "\n",
        "print(\"=== DIAGNOSZTIKA V√âGE ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding gener√°l√°s met√≥dus hozz√°ad√°sa\n",
        "def generate_embeddings_batch(self, texts):\n",
        "    \"\"\"Robosztus batch embedding gener√°l√°s\"\"\"\n",
        "    batch_start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Sz√∂vegek haszn√°lata k√∂zvetlen√ºl - az eda_clean_for_embedding.py m√°r feldolgozta\n",
        "        processed_texts = [str(text) for text in texts]\n",
        "        \n",
        "        # Alap√©rtelmezett embedding gener√°l√°s\n",
        "        embeddings = self.model.encode(\n",
        "            processed_texts,\n",
        "            normalize_embeddings=True,\n",
        "            show_progress_bar=False,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "        \n",
        "        # Gyors dimenzi√≥ ellen≈ërz√©s\n",
        "        if embeddings.shape[1] != self.dimension:\n",
        "            logger.warning(f\"Dimenzi√≥ hiba: {embeddings.shape[1]} != {self.dimension}\")\n",
        "            if embeddings.shape[1] > self.dimension:\n",
        "                embeddings = embeddings[:, :self.dimension]\n",
        "            else:\n",
        "                padding = np.zeros((embeddings.shape[0], self.dimension - embeddings.shape[1]))\n",
        "                embeddings = np.hstack([embeddings, padding])\n",
        "        \n",
        "        # Teljes√≠tm√©ny k√∂vet√©s\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        self.batch_times.append(batch_time)\n",
        "        self.processed_count += len(texts)\n",
        "        \n",
        "        # Sebess√©g sz√°m√≠t√°s\n",
        "        speed = len(texts) / batch_time\n",
        "        if speed < 5.0:  # Ha 5 sor/sec alatt\n",
        "            logger.warning(f\"Lass√∫ batch: {speed:.1f} sor/sec\")\n",
        "        \n",
        "        return embeddings.astype(np.float32)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Batch feldolgoz√°si hiba: {e}\")\n",
        "        self.failed_count += len(texts)\n",
        "        # Fallback: NaN vektorok\n",
        "        return np.full((len(texts), self.dimension), np.nan, dtype=np.float32)\n",
        "    \n",
        "    finally:\n",
        "        # Alapvet≈ë mem√≥ria cleanup\n",
        "        if self.processed_count % 500 == 0:\n",
        "            self._cleanup_memory()\n",
        "\n",
        "# Met√≥dus hozz√°ad√°sa az oszt√°lyhoz (ellen≈ërizz√ºk, hogy l√©tezik-e az oszt√°ly)\n",
        "if 'embedding_generator' in globals():\n",
        "    OptimizedQwen3EmbeddingGenerator.generate_embeddings_batch = generate_embeddings_batch\n",
        "    print(\"Embedding gener√°l√°s met√≥dus hozz√°adva!\")\n",
        "else:\n",
        "    print(\"HIBA: El≈ësz√∂r futtasd a modell inicializ√°l√≥ cell√°t!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seg√©df√ºggv√©nyek\n",
        "def create_metadata_json(row):\n",
        "    \"\"\"Teljes metadata JSON k√©sz√≠t√©se az √∂sszes el√©rhet≈ë oszloppal\"\"\"\n",
        "    metadata = {\n",
        "        'doc_id': str(row.get('doc_id', '')),\n",
        "        'birosag': str(row.get('birosag', '')),\n",
        "        'JogTerulet': str(row.get('JogTerulet', '')),\n",
        "        'Azonosito': str(row.get('Azonosito', '')),\n",
        "        'MeghozoBirosag': str(row.get('MeghozoBirosag', '')),\n",
        "        'EgyediAzonosito': str(row.get('EgyediAzonosito', '')),\n",
        "        'HatarozatEve': str(row.get('HatarozatEve', '')),\n",
        "        'AllKapcsolodoUgyszam': str(row.get('AllKapcsolodoUgyszam', '')),\n",
        "        'AllKapcsolodoBirosag': str(row.get('AllKapcsolodoBirosag', '')),\n",
        "        'KapcsolodoHatarozatok': str(row.get('KapcsolodoHatarozatok', '')),\n",
        "        'Jogszabalyhelyek': str(row.get('Jogszabalyhelyek', '')),\n",
        "        'text_length': len(str(row.get('text', ''))),\n",
        "        'processed_timestamp': time.time()\n",
        "    }\n",
        "    return json.dumps(metadata, ensure_ascii=False)\n",
        "\n",
        "def adaptive_batch_size(text_lengths, base_batch_size=BATCH_SIZE):\n",
        "    \"\"\"Adapt√≠v batch m√©ret sz√∂veg hossz alapj√°n\"\"\"\n",
        "    avg_length = np.mean(text_lengths)\n",
        "    \n",
        "    if avg_length > 6000:\n",
        "        return max(8, base_batch_size // 4)\n",
        "    elif avg_length > 4000:\n",
        "        return max(16, base_batch_size // 2)\n",
        "    elif avg_length > 2000:\n",
        "        return base_batch_size\n",
        "    else:\n",
        "        return min(64, base_batch_size * 2)\n",
        "\n",
        "def prepare_final_columns(chunk_df):\n",
        "    \"\"\"V√©gs≈ë oszlopok el≈ëk√©sz√≠t√©se - √∂sszes metadata meg≈ërz√©se\"\"\"\n",
        "    # Alapvet≈ë oszlopok (k√∂telez≈ë)\n",
        "    final_columns = ['doc_id', 'text', 'embedding', 'metadata_json']\n",
        "    \n",
        "    # √ñsszes metadata oszlop hozz√°ad√°sa, ha l√©tezik\n",
        "    metadata_columns = [\n",
        "        'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',\n",
        "        'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', \n",
        "        'AllKapcsolodoBirosag', 'KapcsolodoHatarozatok', 'Jogszabalyhelyek'\n",
        "    ]\n",
        "    \n",
        "    # Csak a l√©tez≈ë oszlopokat adjuk hozz√°\n",
        "    for col in metadata_columns:\n",
        "        if col in chunk_df.columns:\n",
        "            final_columns.append(col)\n",
        "    \n",
        "    # Visszaadjuk a l√©tez≈ë oszlopokat\n",
        "    available_columns = [col for col in final_columns if col in chunk_df.columns]\n",
        "    return available_columns\n",
        "\n",
        "print(\"Seg√©df√ºggv√©nyek bet√∂ltve!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A100 f≈ëfolyamat - Robosztus embedding gener√°l√°s\n",
        "def process_embeddings_a100():\n",
        "    \"\"\"\n",
        "    A100 GPU-ra optimaliz√°lt robosztus embedding gener√°l√°s\n",
        "    √öJDONS√ÅG: Chunked input t√°mogat√°s memory-safe feldolgoz√°shoz\n",
        "    \"\"\"\n",
        "    \n",
        "    start_time = time.time()\n",
        "    logger.info(\"A100 chunked-kompatibilis embedding feldolgoz√°s kezd√©se...\")\n",
        "    \n",
        "    processed_rows = 0\n",
        "    all_results = []\n",
        "    \n",
        "    # ===== CHUNKED INPUT M√ìD =====\n",
        "    if CHUNKED_INPUT_MODE:\n",
        "        logger.info(f\"üéØ CHUNKED INPUT feldolgoz√°s: {len(cleaned_chunk_files)} chunk f√°jl\")\n",
        "        \n",
        "        with tqdm(total=len(cleaned_chunk_files), desc=\"Cleaned chunk feldolgoz√°s\", unit=\"file\") as file_pbar:\n",
        "            \n",
        "            for file_idx, chunk_file in enumerate(cleaned_chunk_files):\n",
        "                chunk_start_time = time.time()\n",
        "                file_name = os.path.basename(chunk_file)\n",
        "                \n",
        "                try:\n",
        "                    # Cleaned chunk bet√∂lt√©se\n",
        "                    chunk_df = pd.read_csv(chunk_file, encoding='utf-8')\n",
        "                    logger.info(f\"Chunk f√°jl bet√∂ltve: {file_name} ({len(chunk_df):,} sor)\")\n",
        "                    \n",
        "                    # Alapvet≈ë adatellen≈ërz√©s\n",
        "                    original_len = len(chunk_df)\n",
        "                    chunk_df = chunk_df.dropna(subset=['text', 'doc_id'])\n",
        "                    chunk_df['text'] = chunk_df['text'].astype(str)\n",
        "                    \n",
        "                    if len(chunk_df) == 0:\n",
        "                        logger.warning(f\"Chunk f√°jl √ºres: {file_name}\")\n",
        "                        file_pbar.update(1)\n",
        "                        continue\n",
        "                    \n",
        "                    logger.info(f\"Chunk feldolgoz√°s: {file_name} - {len(chunk_df):,} √©rv√©nyes sor\")\n",
        "                    \n",
        "                    # Embedding gener√°l√°s a chunk-hoz\n",
        "                    chunk_with_embeddings = process_single_chunk_embeddings(\n",
        "                        chunk_df, f\"File-{file_idx+1}/{len(cleaned_chunk_files)}\"\n",
        "                    )\n",
        "                    \n",
        "                    all_results.append(chunk_with_embeddings)\n",
        "                    processed_rows += len(chunk_df)\n",
        "                    \n",
        "                    # Progress update\n",
        "                    chunk_time = time.time() - chunk_start_time\n",
        "                    rows_per_sec = len(chunk_df) / chunk_time\n",
        "                    \n",
        "                    file_pbar.set_postfix({\n",
        "                        'F√°jl': file_name[:20],\n",
        "                        'Sorok/sec': f'{rows_per_sec:.1f}',\n",
        "                        'Mem√≥ria': f'{embedding_generator._monitor_memory().get(\"allocated_gb\", 0):.1f}GB',\n",
        "                        '√ñsszes': f'{processed_rows:,}'\n",
        "                    })\n",
        "                    file_pbar.update(1)\n",
        "                    \n",
        "                    # Rendszeres cleanup\n",
        "                    if file_idx % 3 == 0:\n",
        "                        embedding_generator._cleanup_memory()\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Hiba a chunk f√°jl feldolgoz√°s√°ban ({file_name}): {e}\")\n",
        "                    file_pbar.update(1)\n",
        "                    continue\n",
        "    \n",
        "    # ===== UNIFIED CSV FALLBACK M√ìD =====\n",
        "    else:\n",
        "        logger.info(\"üìÑ UNIFIED CSV feldolgoz√°s (fallback mode)\")\n",
        "        \n",
        "        # Teljes f√°jl m√©ret becsl√©se\n",
        "        total_rows = sum(1 for _ in open(INPUT_CSV, 'r', encoding='utf-8')) - 1\n",
        "        total_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "        \n",
        "        logger.info(f\"Feldolgozand√≥ sorok: {total_rows:,}\")\n",
        "        logger.info(f\"Chunk m√©ret: {CHUNK_SIZE:,}\")\n",
        "        logger.info(f\"Batch m√©ret: {BATCH_SIZE}\")\n",
        "        \n",
        "        chunk_count = 0\n",
        "        \n",
        "        with tqdm(total=total_chunks, desc=\"Unified CSV chunk feldolgoz√°s\", unit=\"chunk\") as chunk_pbar:\n",
        "            \n",
        "            for chunk_df in pd.read_csv(INPUT_CSV, chunksize=CHUNK_SIZE, encoding='utf-8'):\n",
        "                chunk_count += 1\n",
        "                chunk_start_time = time.time()\n",
        "                \n",
        "                # Alapvet≈ë adatellen≈ërz√©s\n",
        "                original_len = len(chunk_df)\n",
        "                chunk_df = chunk_df.dropna(subset=['text', 'doc_id'])\n",
        "                chunk_df['text'] = chunk_df['text'].astype(str)\n",
        "                \n",
        "                if len(chunk_df) == 0:\n",
        "                    logger.warning(f\"Chunk {chunk_count}: nincs √©rv√©nyes adat\")\n",
        "                    chunk_pbar.update(1)\n",
        "                    continue\n",
        "                \n",
        "                logger.info(f\"Chunk {chunk_count}/{total_chunks}: {len(chunk_df):,} √©rv√©nyes sor\")\n",
        "                \n",
        "                # Embedding gener√°l√°s a chunk-hoz\n",
        "                chunk_with_embeddings = process_single_chunk_embeddings(\n",
        "                    chunk_df, f\"Chunk-{chunk_count}/{total_chunks}\"\n",
        "                )\n",
        "                \n",
        "                all_results.append(chunk_with_embeddings)\n",
        "                processed_rows += len(chunk_df)\n",
        "                \n",
        "                # Progress update\n",
        "                chunk_time = time.time() - chunk_start_time\n",
        "                rows_per_sec = len(chunk_df) / chunk_time\n",
        "                \n",
        "                chunk_pbar.set_postfix({\n",
        "                    'Sorok/sec': f'{rows_per_sec:.1f}',\n",
        "                    'Mem√≥ria': f'{embedding_generator._monitor_memory().get(\"allocated_gb\", 0):.1f}GB',\n",
        "                    'Sikeres': embedding_generator.processed_count,\n",
        "                    'Hib√°s': embedding_generator.failed_count\n",
        "                })\n",
        "                chunk_pbar.update(1)\n",
        "                \n",
        "                # Rendszeres cleanup minden 5. chunk ut√°n\n",
        "                if chunk_count % 5 == 0:\n",
        "                    embedding_generator._cleanup_memory()\n",
        "    \n",
        "    # ===== EREDM√âNYEK EGYES√çT√âSE =====\n",
        "    logger.info(\"DataFrame-ek egyes√≠t√©se...\")\n",
        "    if not all_results:\n",
        "        raise ValueError(\"Nincs feldolgozott adat!\")\n",
        "    \n",
        "    final_df = pd.concat(all_results, ignore_index=True)\n",
        "    logger.info(f\"Egyes√≠tett DataFrame: {len(final_df):,} sor\")\n",
        "    \n",
        "    return final_df, processed_rows, time.time() - start_time\n",
        "\n",
        "def process_single_chunk_embeddings(chunk_df, chunk_label):\n",
        "    \"\"\"\n",
        "    Egyetlen chunk embedding feldolgoz√°sa (k√∂z√∂s logika chunked √©s unified m√≥dhoz).\n",
        "    \"\"\"\n",
        "    # Sz√∂vegek √©s adapt√≠v batch m√©ret\n",
        "    texts = chunk_df['text'].tolist()\n",
        "    text_lengths = [len(text) for text in texts]\n",
        "    dynamic_batch_size = adaptive_batch_size(text_lengths, BATCH_SIZE)\n",
        "    \n",
        "    # Batch-es embedding gener√°l√°s\n",
        "    all_embeddings = []\n",
        "    total_batches_in_chunk = (len(texts) + dynamic_batch_size - 1) // dynamic_batch_size\n",
        "    \n",
        "    with tqdm(total=total_batches_in_chunk, desc=f\"{chunk_label} batch-ek\", \n",
        "             unit=\"batch\", leave=False) as batch_pbar:\n",
        "        \n",
        "        for batch_idx in range(0, len(texts), dynamic_batch_size):\n",
        "            batch_texts = texts[batch_idx:batch_idx + dynamic_batch_size]\n",
        "            \n",
        "            # Embedding gener√°l√°s hibakezel√©ssel\n",
        "            try:\n",
        "                batch_embeddings = embedding_generator.generate_embeddings_batch(batch_texts)\n",
        "                all_embeddings.extend(batch_embeddings.tolist())\n",
        "                \n",
        "                # Alapvet≈ë mem√≥ria monitoring\n",
        "                memory_info = embedding_generator._monitor_memory()\n",
        "                if memory_info.get('allocated_gb', 0) > MEMORY_LIMIT_GB * 0.85:\n",
        "                    logger.warning(f\"Magas mem√≥ria: {memory_info.get('allocated_gb', 0):.1f}GB\")\n",
        "                    embedding_generator._cleanup_memory()\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"Batch hiba: {e}\")\n",
        "                # Fallback NaN vektorok\n",
        "                nan_embeddings = np.full((len(batch_texts), EMBEDDING_DIMENSION), np.nan)\n",
        "                all_embeddings.extend(nan_embeddings.tolist())\n",
        "            \n",
        "            batch_pbar.update(1)\n",
        "    \n",
        "    # Embedding sz√°moss√°gi ellen≈ërz√©s\n",
        "    if len(all_embeddings) != len(chunk_df):\n",
        "        logger.error(f\"Embedding sz√°moss√°gi hiba: {len(all_embeddings)} != {len(chunk_df)}\")\n",
        "        # Kieg√©sz√≠t√©s NaN-okkal\n",
        "        while len(all_embeddings) < len(chunk_df):\n",
        "            all_embeddings.append(np.full(EMBEDDING_DIMENSION, np.nan).tolist())\n",
        "    \n",
        "    # Eredm√©nyek hozz√°ad√°sa\n",
        "    chunk_df['embedding'] = all_embeddings\n",
        "    chunk_df['metadata_json'] = chunk_df.apply(create_metadata_json, axis=1)\n",
        "    \n",
        "    # V√©gs≈ë oszlopok - √∂sszes metadata meg≈ërz√©se\n",
        "    available_columns = prepare_final_columns(chunk_df)\n",
        "    chunk_result = chunk_df[available_columns].copy()\n",
        "    \n",
        "    return chunk_result\n",
        "\n",
        "# A100 f≈ëfolyamat ind√≠t√°sa\n",
        "logger.info(\"A100 embedding feldolgoz√°s ind√≠t√°sa...\")\n",
        "final_df, processed_rows, total_time = process_embeddings_a100()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parquet ment√©s √©s v√©gs≈ë valid√°ci√≥\n",
        "logger.info(\"Parquet ment√©s √©s valid√°ci√≥...\")\n",
        "\n",
        "# Embedding valid√°ci√≥\n",
        "valid_embeddings = 0\n",
        "nan_embeddings = 0\n",
        "dimension_errors = 0\n",
        "\n",
        "for idx, emb in enumerate(final_df['embedding']):\n",
        "    if isinstance(emb, list):\n",
        "        if len(emb) == EMBEDDING_DIMENSION:\n",
        "            if not np.any(np.isnan(emb)):\n",
        "                valid_embeddings += 1\n",
        "            else:\n",
        "                nan_embeddings += 1\n",
        "        else:\n",
        "            dimension_errors += 1\n",
        "    else:\n",
        "        dimension_errors += 1\n",
        "\n",
        "logger.info(f\"Embedding valid√°ci√≥:\")\n",
        "logger.info(f\"  √ârv√©nyes: {valid_embeddings:,}\")\n",
        "logger.info(f\"  NaN: {nan_embeddings:,}\")\n",
        "logger.info(f\"  Dimenzi√≥ hiba: {dimension_errors:,}\")\n",
        "\n",
        "# Parquet ment√©s\n",
        "logger.info(f\"V√©gs≈ë Parquet ment√©s: {OUTPUT_PARQUET}\")\n",
        "\n",
        "final_df.to_parquet(\n",
        "    OUTPUT_PARQUET,\n",
        "    engine='pyarrow',\n",
        "    index=False,\n",
        "    compression='snappy',\n",
        "    row_group_size=50000\n",
        ")\n",
        "\n",
        "# F√°jl valid√°ci√≥\n",
        "file_size = os.path.getsize(OUTPUT_PARQUET) / (1024**3)\n",
        "\n",
        "# Gyors visszaolvas√°si teszt\n",
        "test_df = pd.read_parquet(OUTPUT_PARQUET, nrows=100)\n",
        "logger.info(f\"Visszaolvas√°si teszt sikeres: {len(test_df)} sor\")\n",
        "\n",
        "# V√©gs≈ë statisztik√°k\n",
        "logger.info(\"A100 QWEN3-4b EMBEDDING GENER√ÅL√ÅS BEFEJEZVE!\")\n",
        "logger.info(f\"Feldolgozott sorok: {processed_rows:,}\")\n",
        "logger.info(f\"V√©gs≈ë sorok: {len(final_df):,}\")\n",
        "logger.info(f\"V√©gs≈ë oszlopok ({len(final_df.columns)}): {list(final_df.columns)}\")\n",
        "logger.info(f\"√ârv√©nyes embeddings: {valid_embeddings:,}\")\n",
        "logger.info(f\"F√°jl m√©ret: {file_size:.2f}GB\")\n",
        "logger.info(f\"Teljes fut√°si id≈ë: {total_time/3600:.2f} √≥ra\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"QWEN3-0.6B EMBEDDING FELDOLGOZAS BEFEJEZVE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Feldolgozott dokumentumok: {processed_rows:,}\")\n",
        "print(f\"Vegso Parquet fajl: {OUTPUT_PARQUET}\")\n",
        "print(f\"Oszlopok szama: {len(final_df.columns)}\")\n",
        "print(f\"Ervenyes embeddings: {valid_embeddings:,}\")\n",
        "print(f\"Fajl meret: {file_size:.2f}GB\")\n",
        "print(f\"Futasi ido: {total_time/3600:.2f} ora\")\n",
        "print(\"=\"*80)\n",
        "logger.info(f\"Teljes fut√°si id≈ë: {total_time/3600:.2f} √≥ra\")\n",
        "logger.info(f\"√Åtlag sebess√©g: {processed_rows/total_time:.1f} sor/sec\")\n",
        "logger.info(f\"F√°jl m√©ret: {file_size:.2f} GB\")\n",
        "logger.info(f\"Cs√∫cs mem√≥ria: {embedding_generator.peak_memory_usage:.1f}GB\")\n",
        "\n",
        "print(\"\\nA100 QWEN3-0.6B EMBEDDING GENERALAS SIKERESEN BEFEJEZVE!\")\n",
        "print(f\"Feldolgozott sorok: {processed_rows:,}\")\n",
        "print(f\"√ârv√©nyes embeddings: {valid_embeddings:,}\")\n",
        "print(f\"F√°jl m√©ret: {file_size:.2f} GB\")\n",
        "print(f\"Teljes id≈ë: {total_time/3600:.2f} √≥ra\")\n",
        "print(f\"Sebess√©g: {processed_rows/total_time:.1f} sor/sec\")\n",
        "print(f\"Cs√∫cs mem√≥ria: {embedding_generator.peak_memory_usage:.1f}GB\")\n",
        "print(f\"Sikeress√©gi ar√°ny: {(valid_embeddings/len(final_df)*100):.1f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
