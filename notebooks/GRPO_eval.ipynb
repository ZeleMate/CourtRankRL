{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55acb53-1c00-473e-8abc-e46126962fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Csomagok telep√≠t√©se...\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Csomagok telep√≠tve\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Csomagok telep√≠tve\n"
     ]
    }
   ],
   "source": [
    "# Package telep√≠t√©s\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ Csomagok telep√≠t√©se...\")\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install -q transformers datasets huggingface_hub\n",
    "%pip install -q numpy scipy scikit-learn pandas\n",
    "%pip install -q accelerate peft bitsandbytes\n",
    "%pip install -q --upgrade trl unsloth\n",
    "%pip install -q sentence-transformers faiss-gpu bm25s ranx\n",
    "%pip install -q tqdm\n",
    "\n",
    "print(\"‚úÖ Csomagok telep√≠tve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be1a2a5-d6fe-455d-bc98-c73550d3720a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unsloth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Importok\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'unsloth'"
     ]
    }
   ],
   "source": [
    "# Importok\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "\n",
    "# Retrieval libraries\n",
    "import bm25s\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Evaluation\n",
    "from ranx import Qrels, Run, evaluate\n",
    "\n",
    "# Model libraries\n",
    "from huggingface_hub import login\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ Importok bet√∂ltve\")\n",
    "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
    "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bce5f4-8a9d-4345-9699-ec502987f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √ötvonalak\n",
    "BASE_PATH = Path(\"/workspace\")\n",
    "MODEL_PATH = BASE_PATH / \"grpo_policy\"\n",
    "EVAL_SPLIT_PATH = BASE_PATH / \"eval_split.json\"\n",
    "QRELS_PATH = BASE_PATH / \"baseline_qrels.tsv\"\n",
    "BASELINE_METRICS_PATH = BASE_PATH / \"baseline_query_metrics.csv\"\n",
    "CHUNKS_PATH = BASE_PATH / \"chunks.jsonl\"\n",
    "FAISS_INDEX_PATH = BASE_PATH / \"faiss_index.bin\"\n",
    "CHUNK_ID_MAP_PATH = BASE_PATH / \"chunk_id_map.npy\"\n",
    "BM25_INDEX_PATH = BASE_PATH / \"bm25_index\" / \"bm25s_model\"\n",
    "CHUNK_IDS_PATH = BASE_PATH / \"chunk_ids.json\"\n",
    "OUTPUT_DIR = BASE_PATH / \"evaluation_results\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Konfigur√°ci√≥\n",
    "SLATE_SIZE = 30\n",
    "EVAL_TOP_K = 20\n",
    "TOP_K_BASELINE = 300\n",
    "RRF_K = 60\n",
    "MAX_SEQ_LENGTH = 16384\n",
    "MAX_NEW_TOKENS = 768\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.9\n",
    "\n",
    "print(\"üìã Konfigur√°ci√≥:\")\n",
    "print(f\"  Base path: {BASE_PATH}\")\n",
    "print(f\"  Model path: {MODEL_PATH}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"  Slate size: {SLATE_SIZE}\")\n",
    "print(f\"  ‚ö†Ô∏è  Eval top-k: {EVAL_TOP_K} (truncated for fair baseline comparison)\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "# F√°jlellen≈ërz√©s\n",
    "print(\"\\nüìÇ F√°jlellen≈ërz√©s:\")\n",
    "required_files = [\n",
    "    (MODEL_PATH, \"Model directory\"),\n",
    "    (EVAL_SPLIT_PATH, \"Eval split\"),\n",
    "    (QRELS_PATH, \"Qrels\"),\n",
    "    (BASELINE_METRICS_PATH, \"Baseline metrics\"),\n",
    "    (CHUNKS_PATH, \"Chunks\"),\n",
    "    (FAISS_INDEX_PATH, \"FAISS index\"),\n",
    "    (CHUNK_ID_MAP_PATH, \"Chunk ID map\"),\n",
    "    (BM25_INDEX_PATH, \"BM25 index\"),\n",
    "    (CHUNK_IDS_PATH, \"Chunk IDs\"),\n",
    "]\n",
    "\n",
    "all_files_exist = True\n",
    "for path, name in required_files:\n",
    "    if path.exists():\n",
    "        print(f\"  ‚úÖ {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {name}: HI√ÅNYZIK - {path}\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if not all_files_exist:\n",
    "    raise FileNotFoundError(\"‚ùå Hi√°nyz√≥ f√°jlok! T√∂ltsd fel az √∂sszes sz√ºks√©ges f√°jlt.\")\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Minden sz√ºks√©ges f√°jl el√©rhet≈ë\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36192ec4-d964-49c4-823c-3ef3b9b6d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === METRIKA HELPER F√úGGV√âNYEK ===\n",
    "\n",
    "def calculate_ndcg(ranked_indices: List[int], true_relevance: List[float], k: int = 10) -> float:\n",
    "    \"\"\"nDCG sz√°m√≠t√°s sklearn-kel.\"\"\"\n",
    "    if not true_relevance or not ranked_indices or max(true_relevance) == 0:\n",
    "        return 0.0\n",
    "    y_true = np.array(true_relevance)\n",
    "    max_score = len(ranked_indices)\n",
    "    y_score = np.zeros_like(y_true, dtype=float)\n",
    "    for i, idx in enumerate(ranked_indices[:k]):\n",
    "        if idx < len(y_true):\n",
    "            y_score[idx] = max_score - i\n",
    "    if np.sum(y_score) == 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(ndcg_score(y_true.reshape(1, -1), y_score.reshape(1, -1), k=k))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_ndcg_metric(relevances: List[int], k: int) -> float:\n",
    "    \"\"\"nDCG@k sz√°m√≠t√°s ranking sorrendb≈ël.\"\"\"\n",
    "    if not relevances or k <= 0:\n",
    "        return 0.0\n",
    "    relevances_truncated = relevances[:k]\n",
    "    scores = list(range(len(relevances_truncated), 0, -1))\n",
    "    try:\n",
    "        ndcg = ndcg_score(\n",
    "            y_true=[relevances_truncated],\n",
    "            y_score=[scores],\n",
    "            k=k\n",
    "        )\n",
    "        return float(ndcg)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def compute_mrr(relevances: List[int], k: Optional[int] = None) -> float:\n",
    "    \"\"\"Mean Reciprocal Rank sz√°m√≠t√°s.\"\"\"\n",
    "    if k is not None:\n",
    "        relevances = relevances[:k]\n",
    "    for i, rel in enumerate(relevances, start=1):\n",
    "        if rel > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def compute_recall(relevances: List[int], k: int, total_relevant: Optional[int] = None) -> float:\n",
    "    \"\"\"Recall@k sz√°m√≠t√°s.\"\"\"\n",
    "    if not relevances:\n",
    "        return 0.0\n",
    "    relevant_in_topk = sum(1 for rel in relevances[:k] if rel > 0)\n",
    "    if total_relevant is None:\n",
    "        total_relevant = sum(1 for rel in relevances if rel > 0)\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return relevant_in_topk / total_relevant\n",
    "\n",
    "print(\"‚úÖ Metrika helper f√ºggv√©nyek defini√°lva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f911e4-106b-4f5c-8f87-cde180082e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARSING √âS PROMPT F√úGGV√âNYEK ===\n",
    "\n",
    "def parse_model_ranking(completion: str, slate_size: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Model completion parsing - kinyeri a ranking indexeket.\n",
    "    Ha nincs el√©g valid index, kieg√©sz√≠ti a baseline order szerint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        numbers = []\n",
    "        \n",
    "        # 1. Vessz≈ëvel elv√°lasztott sz√°mok\n",
    "        parts = completion.split(\",\")\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            for word in part.split():\n",
    "                word = word.strip().rstrip(\".,;\")\n",
    "                if word.isdigit():\n",
    "                    num = int(word)\n",
    "                    if 0 <= num < slate_size:\n",
    "                        numbers.append(num)\n",
    "        \n",
    "        # 2. Ha nincs el√©g, pr√≥b√°lunk sz√≥k√∂zzel elv√°lasztott form√°tumot\n",
    "        if len(numbers) < slate_size // 3:\n",
    "            for word in completion.split():\n",
    "                word = word.strip().rstrip(\".,;\")\n",
    "                if word.isdigit():\n",
    "                    num = int(word)\n",
    "                    if 0 <= num < slate_size and num not in numbers:\n",
    "                        numbers.append(num)\n",
    "        \n",
    "        # 3. Valid sz√°mok gy≈±jt√©se (deduplik√°ci√≥)\n",
    "        valid_numbers = []\n",
    "        seen = set()\n",
    "        for n in numbers:\n",
    "            if n not in seen:\n",
    "                valid_numbers.append(n)\n",
    "                seen.add(n)\n",
    "        \n",
    "        # 4. Ha van el√©g valid sz√°m, haszn√°ljuk\n",
    "        if len(valid_numbers) >= max(1, slate_size // 3):\n",
    "            missing_indices = [i for i in range(slate_size) if i not in valid_numbers]\n",
    "            result = valid_numbers + missing_indices\n",
    "            return result[:slate_size]\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: baseline order\n",
    "    return list(range(slate_size))\n",
    "\n",
    "def create_evaluation_prompt(query_id: str, slate: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    üîí BLIND EVALUATION prompt - NO relevance labels (data leakage fix!)\n",
    "    \n",
    "    CRITICAL DIFFERENCE FROM TRAINING:\n",
    "    - Training prompt (grpo_train_runpod.ipynb): Includes relevance labels for reward calculation\n",
    "    - Evaluation prompt (THIS): NO relevance labels - blind assessment only\n",
    "    \n",
    "    The model must rank purely based on:\n",
    "    - Document content\n",
    "    - Query-document semantic match\n",
    "    - Legal domain relevance\n",
    "    - BM25/FAISS signals (optional)\n",
    "    \n",
    "    Relevance scores are ONLY used post-hoc for metric calculation, NOT shown to model!\n",
    "    \"\"\"\n",
    "    prompt = f'''# Legal Document Relevance Assessment and Ranking\n",
    "\n",
    "## TASK\n",
    "You are helping to rank Hungarian court decision documents by their relevance to a search query.\n",
    "\n",
    "## SEARCH QUERY\n",
    "\"{query_id}\"\n",
    "\n",
    "## WHAT TO CONSIDER\n",
    "\n",
    "When ranking documents, think about:\n",
    "- How well the document topic matches the query\n",
    "- Whether the content directly addresses what the query is asking\n",
    "- The legal domain and court type (may or may not be relevant)\n",
    "- BM25 and FAISS scores show retrieval system confidence, but content is most important\n",
    "\n",
    "## DOCUMENT EVALUATION\n",
    "\n",
    "You need to evaluate the following {len(slate)} document excerpts:\n",
    "\n",
    "'''\n",
    "    \n",
    "    separator_line = \"\\u2500\" * 63\n",
    "    \n",
    "    for idx, doc in enumerate(slate):\n",
    "        chunk_text = doc.get('text', '')[:800]\n",
    "        bm25 = doc.get('bm25_score', 0)\n",
    "        faiss = doc.get('faiss_score', 0)\n",
    "        court = doc.get('court', 'N/A')\n",
    "        domain = doc.get('domain', 'N/A')\n",
    "        year = doc.get('year', 'N/A')\n",
    "        \n",
    "        # üîí CRITICAL: NO relevance label in evaluation prompt!\n",
    "        # relevance field exists in slate for metrics calculation only\n",
    "        \n",
    "        prompt += f'''{separator_line}\n",
    "DOCUMENT [{idx}]\n",
    "\n",
    "ID: {doc.get('doc_id', 'N/A')} | Court: {court} | Domain: {domain} | Year: {year}\n",
    "Scores: BM25={bm25:.2f}, FAISS={faiss:.3f}\n",
    "\n",
    "Content:\n",
    "{chunk_text}\n",
    "\n",
    "'''\n",
    "    \n",
    "    prompt += f'''{separator_line}\n",
    "\n",
    "## RANKING INSTRUCTIONS\n",
    "\n",
    "You need to rank the {len(slate)} documents by their relevance to the search query.\n",
    "\n",
    "### GUIDELINES (focus on relevance):\n",
    "- Consider the search query and how well each document matches it\n",
    "- Analyze document content, topic match, and legal domain relevance\n",
    "- BM25 and FAISS scores show retrieval system confidence, but focus primarily on actual content relevance to the query\n",
    "- Use your judgment to determine which documents best answer the legal query\n",
    "\n",
    "### OUTPUT FORMAT (STRICT):\n",
    "- Respond with EXACTLY one line in this format ‚Üí RANKING: <comma-separated indices> #END\n",
    "- Use document indices 0 to {len(slate)-1}, each exactly once, separated by commas and without extra spaces\n",
    "- Do NOT include headings, bullet points, Markdown, or explanations before or after the line\n",
    "- If you are uncertain, still provide your best full ranking covering every index\n",
    "- End the line with \"#END\" to confirm completion\n",
    "\n",
    "Example:\n",
    "RANKING: 3,0,5,1,4,2,8,6,7,9 #END\n",
    "\n",
    "RANKING:\n",
    "'''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"‚úÖ Parsing √©s prompt f√ºggv√©nyek defini√°lva\")\n",
    "print(\"   üîí BLIND EVALUATION MODE: Relevance labels REMOVED from prompt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e6401-61a9-46fd-ab93-4bf059e708cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVAL SPLIT BET√ñLT√âSE ===\n",
    "print(\"üìÇ Eval split bet√∂lt√©se...\")\n",
    "with open(EVAL_SPLIT_PATH, 'r', encoding='utf-8') as f:\n",
    "    eval_split_data = json.load(f)\n",
    "\n",
    "eval_query_ids = eval_split_data.get(\"query_ids\", [])\n",
    "split_params = eval_split_data.get(\"split_params\", {})\n",
    "\n",
    "print(f\"‚úÖ Eval split bet√∂ltve: {len(eval_query_ids)} query\")\n",
    "print(f\"   Split param√©terek: seed={split_params.get('seed')}, train_split={split_params.get('train_split')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d45feb-8ff5-469d-8928-45b754c1e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === QRELS BET√ñLT√âSE ===\n",
    "print(\"\\nüìÇ Qrels bet√∂lt√©se...\")\n",
    "qrels_dict = {}\n",
    "with open(QRELS_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:  # Skip header\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        query_id, doc_id, relevance = parts\n",
    "        if query_id not in qrels_dict:\n",
    "            qrels_dict[query_id] = {}\n",
    "        qrels_dict[query_id][doc_id] = int(relevance)\n",
    "\n",
    "print(f\"‚úÖ Qrels bet√∂ltve: {len(qrels_dict)} query, {sum(len(docs) for docs in qrels_dict.values())} relevanciaÂà§ÂÆö\")\n",
    "\n",
    "# Relevancia eloszl√°s\n",
    "relevance_counts = defaultdict(int)\n",
    "for query_rels in qrels_dict.values():\n",
    "    for rel_grade in query_rels.values():\n",
    "        relevance_counts[rel_grade] += 1\n",
    "\n",
    "print(\"\\nüìä Relevancia eloszl√°s:\")\n",
    "for grade in sorted(relevance_counts.keys()):\n",
    "    print(f\"  Grade {grade}: {relevance_counts[grade]} dokumentum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ca908-0170-4d59-ba86-bd13d6dc3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASELINE METRIK√ÅK BET√ñLT√âSE ===\n",
    "print(\"\\nüìÇ Baseline metrik√°k bet√∂lt√©se...\")\n",
    "baseline_df = pd.read_csv(BASELINE_METRICS_PATH, encoding='utf-8')\n",
    "baseline_metrics_dict = {\n",
    "    row[\"query_id\"]: row.to_dict()\n",
    "    for _, row in baseline_df.iterrows()\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Baseline metrik√°k bet√∂ltve: {len(baseline_metrics_dict)} query\")\n",
    "\n",
    "# Aggreg√°lt baseline teljes√≠tm√©ny\n",
    "avg_ndcg10 = np.mean([m.get(\"ndcg@10\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "avg_mrr = np.mean([m.get(\"mrr\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "avg_map = np.mean([m.get(\"map\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "\n",
    "print(f\"\\nüéØ Baseline teljes√≠tm√©ny (√°tlag):\")\n",
    "print(f\"  ‚Ä¢ nDCG@10: {avg_ndcg10:.4f}\")\n",
    "print(f\"  ‚Ä¢ MRR:     {avg_mrr:.4f}\")\n",
    "print(f\"  ‚Ä¢ MAP:     {avg_map:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd0626-4ae8-4ce0-9ae7-895989a47ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHUNKS BET√ñLT√âSE ===\n",
    "print(\"\\nüìÇ Chunks bet√∂lt√©se...\")\n",
    "chunks_df = pd.read_json(CHUNKS_PATH, lines=True, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ Chunks bet√∂ltve: {len(chunks_df):,} chunk\")\n",
    "print(f\"\\nüìä Chunks oszlopok: {list(chunks_df.columns)}\")\n",
    "print(f\"\\nüîç Minta chunk:\")\n",
    "sample_chunk = chunks_df.iloc[0]\n",
    "print(f\"  chunk_id: {sample_chunk.get('chunk_id', 'N/A')}\")\n",
    "print(f\"  doc_id: {sample_chunk.get('doc_id', 'N/A')}\")\n",
    "print(f\"  text length: {len(str(sample_chunk.get('text', '')))} karakter\")\n",
    "print(f\"  court: {sample_chunk.get('court', 'N/A')}\")\n",
    "print(f\"  domain: {sample_chunk.get('domain', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a233ed-f060-461b-9c5b-ec1aec308319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BM25S BET√ñLT√âSE ===\n",
    "print(\"üîß BM25S index bet√∂lt√©se...\")\n",
    "retriever_bm25 = bm25s.BM25.load(str(BM25_INDEX_PATH), mmap=True)\n",
    "\n",
    "# Chunk IDs bet√∂lt√©se - kezeli JSON √©s JSONL form√°tumot is\n",
    "print(f\"   Chunk IDs bet√∂lt√©se: {CHUNK_IDS_PATH}\")\n",
    "try:\n",
    "    # El≈ësz√∂r pr√≥b√°ljuk JSON-k√©nt (egyetlen objektum/t√∂mb)\n",
    "    with open(CHUNK_IDS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().strip()\n",
    "        if content.startswith(\"[\") or content.startswith(\"{\"):\n",
    "            # JSON form√°tum\n",
    "            bm25_chunk_ids = json.loads(content)\n",
    "        else:\n",
    "            # JSONL form√°tum - soronk√©nt bet√∂lt√©s\n",
    "            f.seek(0)\n",
    "            bm25_chunk_ids = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    bm25_chunk_ids.append(json.loads(line))\n",
    "except json.JSONDecodeError as e:\n",
    "    # Ha JSON dek√≥dol√°s sikertelen, pr√≥b√°ljuk JSONL-k√©nt\n",
    "    print(f\"   ‚ö†Ô∏è JSON dek√≥dol√°s sikertelen, JSONL-k√©nt pr√≥b√°ljuk...\")\n",
    "    bm25_chunk_ids = []\n",
    "    with open(CHUNK_IDS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    bm25_chunk_ids.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "# Ha lista, akkor rendben van; ha dict, akkor √©rt√©keket vessz√ºk\n",
    "if isinstance(bm25_chunk_ids, dict):\n",
    "    bm25_chunk_ids = list(bm25_chunk_ids.values())\n",
    "\n",
    "print(f\"‚úÖ BM25S bet√∂ltve: {len(bm25_chunk_ids):,} chunk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab195a-d1db-46c9-99aa-9ede8b80304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FAISS BET√ñLT√âSE ===\n",
    "print(\"\\nüîß FAISS index bet√∂lt√©se...\")\n",
    "index_faiss = faiss.read_index(str(FAISS_INDEX_PATH))\n",
    "\n",
    "# Chunk ID mapping\n",
    "chunk_id_map_raw = np.load(CHUNK_ID_MAP_PATH, allow_pickle=True)\n",
    "\n",
    "# Support both array and dict formats\n",
    "if isinstance(chunk_id_map_raw, np.ndarray) and chunk_id_map_raw.dtype == object:\n",
    "    if len(chunk_id_map_raw) > 0 and isinstance(chunk_id_map_raw[0], dict):\n",
    "        # Dict format: {0: 'chunk_id', 1: 'chunk_id', ...}\n",
    "        chunk_id_map = chunk_id_map_raw[0]\n",
    "    else:\n",
    "        # Array format: ['chunk_id', 'chunk_id', ...]\n",
    "        chunk_id_map = {i: cid for i, cid in enumerate(chunk_id_map_raw)}\n",
    "elif isinstance(chunk_id_map_raw, dict):\n",
    "    chunk_id_map = chunk_id_map_raw\n",
    "else:\n",
    "    chunk_id_map = {i: cid for i, cid in enumerate(chunk_id_map_raw)}\n",
    "\n",
    "print(f\"‚úÖ FAISS bet√∂ltve: {index_faiss.ntotal:,} vektor\")\n",
    "print(f\"   Index t√≠pus: {type(index_faiss).__name__}\")\n",
    "print(f\"   Chunk ID map: {len(chunk_id_map):,} mapping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a0b3f-fcc1-4871-ac1b-05c3fb79045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EMBEDDINGGEMMA BET√ñLT√âSE ===\n",
    "print(\"\\nüîß EmbeddingGemma modell bet√∂lt√©se...\")\n",
    "embedding_model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    embedding_model = embedding_model.to('cuda')\n",
    "    print(\"‚úÖ EmbeddingGemma GPU-n\")\n",
    "else:\n",
    "    print(\"‚úÖ EmbeddingGemma CPU-n\")\n",
    "\n",
    "print(f\"   Max seq length: {embedding_model.max_seq_length}\")\n",
    "print(f\"   Embedding dim: {embedding_model.get_sentence_embedding_dimension()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec00616-bd66-44a1-a15d-524f4daa49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RRF FUSION F√úGGV√âNY ===\n",
    "def rrf_fusion(bm25_results: List[Tuple[str, float]], \n",
    "               faiss_results: List[Tuple[str, float]], \n",
    "               k: int = 60) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion - robusztus, param√©ter-mentes fusion.\n",
    "    \n",
    "    RRF(d) = Œ£ 1/(k + rank(d))\n",
    "    \"\"\"\n",
    "    scores = defaultdict(float)\n",
    "    \n",
    "    # BM25 ranks\n",
    "    for rank, (doc_id, _) in enumerate(bm25_results, start=1):\n",
    "        scores[doc_id] += 1.0 / (k + rank)\n",
    "    \n",
    "    # FAISS ranks\n",
    "    for rank, (doc_id, _) in enumerate(faiss_results, start=1):\n",
    "        scores[doc_id] += 1.0 / (k + rank)\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results\n",
    "\n",
    "# === CHUNK ‚Üí DOC AGGREG√ÅCI√ì ===\n",
    "def aggregate_chunks_to_docs(chunk_scores: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Chunk-level scores ‚Üí doc-level aggregation (max score).\n",
    "    chunk_id: \"doc_0\" ‚Üí doc_id: \"doc\"\n",
    "    \"\"\"\n",
    "    doc_scores = {}\n",
    "    for chunk_id, score in chunk_scores:\n",
    "        # Strip _N suffix to get doc_id\n",
    "        doc_id = \"_\".join(chunk_id.split(\"_\")[:-1]) if \"_\" in chunk_id else chunk_id\n",
    "        if doc_id not in doc_scores or score > doc_scores[doc_id]:\n",
    "            doc_scores[doc_id] = score\n",
    "    \n",
    "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_docs\n",
    "\n",
    "print(\"‚úÖ RRF √©s aggreg√°ci√≥ f√ºggv√©nyek defini√°lva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed2c06-6c56-4673-94fe-518d5cb8abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SLATE GENERATION ===\n",
    "def generate_slate_for_query(query: str, \n",
    "                             retriever_bm25, \n",
    "                             index_faiss, \n",
    "                             embedding_model,\n",
    "                             chunks_df: pd.DataFrame,\n",
    "                             chunk_id_map: Dict,\n",
    "                             bm25_chunk_ids: List[str],\n",
    "                             qrels_dict: Dict,\n",
    "                             top_k: int = 30) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Slate gener√°l√°s hybrid retrieval-b≈ël (BM25 + FAISS + RRF fusion).\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with chunk_id, doc_id, text, metadata, scores, relevance\n",
    "    \"\"\"\n",
    "    # === BM25 Retrieval ===\n",
    "    query_normalized = query.lower().strip()\n",
    "    query_tokens = bm25s.tokenize(query_normalized, stopwords=None)\n",
    "    \n",
    "    bm25_results, bm25_scores = retriever_bm25.retrieve(\n",
    "        query_tokens, \n",
    "        k=TOP_K_BASELINE\n",
    "    )\n",
    "    \n",
    "    # Extract chunk_ids from BM25 corpus structure\n",
    "    bm25_chunk_scores = []\n",
    "    for idx in range(len(bm25_results[0])):\n",
    "        doc_dict = bm25_results[0][idx]\n",
    "        if isinstance(doc_dict, dict) and 'text' in doc_dict:\n",
    "            chunk_id = doc_dict['text']\n",
    "            score = float(bm25_scores[0][idx])\n",
    "            bm25_chunk_scores.append((chunk_id, score))\n",
    "    \n",
    "    # Aggregate to doc level\n",
    "    bm25_doc_scores = aggregate_chunks_to_docs(bm25_chunk_scores)\n",
    "    \n",
    "    # === FAISS Retrieval ===\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [query], \n",
    "        prompt_name=\"query\",\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # Search FAISS\n",
    "    distances, indices = index_faiss.search(query_embedding.astype('float32'), TOP_K_BASELINE)\n",
    "    \n",
    "    # Map indices to chunk_ids\n",
    "    faiss_chunk_scores = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        chunk_id = chunk_id_map.get(int(idx))\n",
    "        if chunk_id:\n",
    "            faiss_chunk_scores.append((chunk_id, float(dist)))\n",
    "    \n",
    "    # Aggregate to doc level\n",
    "    faiss_doc_scores = aggregate_chunks_to_docs(faiss_chunk_scores)\n",
    "    \n",
    "    # === RRF Fusion ===\n",
    "    fused_doc_scores = rrf_fusion(bm25_doc_scores, faiss_doc_scores, k=RRF_K)\n",
    "    top_doc_ids = [doc_id for doc_id, _ in fused_doc_scores[:top_k]]\n",
    "    \n",
    "    # === Slate Assembly ===\n",
    "    slate = []\n",
    "    for doc_id in top_doc_ids:\n",
    "        # Find best chunk for this doc (highest BM25 or FAISS score)\n",
    "        doc_chunks = chunks_df[chunks_df['doc_id'] == doc_id]\n",
    "        if doc_chunks.empty:\n",
    "            continue\n",
    "        \n",
    "        # Take first chunk (or could be best scoring chunk)\n",
    "        chunk = doc_chunks.iloc[0]\n",
    "        \n",
    "        # Get scores\n",
    "        bm25_score = next((s for cid, s in bm25_chunk_scores if cid == chunk['chunk_id']), 0.0)\n",
    "        faiss_score = next((s for cid, s in faiss_chunk_scores if cid == chunk['chunk_id']), 0.0)\n",
    "        \n",
    "        # üîí CRITICAL FIX: Relevance stored for POST-HOC metrics calculation ONLY!\n",
    "        # This value is NEVER shown to the model in the evaluation prompt.\n",
    "        # The model must rank blindly based on content, not ground truth.\n",
    "        relevance = qrels_dict.get(query, {}).get(doc_id, 0)\n",
    "        \n",
    "        slate.append({\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'doc_id': doc_id,\n",
    "            'text': str(chunk.get('text', ''))[:800],  # Truncate to 800 chars\n",
    "            'court': chunk.get('court', 'N/A'),\n",
    "            'domain': chunk.get('domain', 'N/A'),\n",
    "            'year': chunk.get('year', 'N/A'),\n",
    "            'bm25_score': bm25_score,\n",
    "            'faiss_score': faiss_score,\n",
    "            'relevance': relevance,\n",
    "        })\n",
    "    \n",
    "    return slate[:top_k]\n",
    "\n",
    "print(\"‚úÖ Slate generation f√ºggv√©ny defini√°lva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5d9ce-7765-4049-ab9d-65f6b8b38235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GPU MEM√ìRIA TISZT√çT√ÅSA ===\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ CUDA cache t√∂r√∂lve\")\n",
    "    \n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    free = total - allocated\n",
    "    \n",
    "    print(f\"\\nüìä GPU Mem√≥ria St√°tusz:\")\n",
    "    print(f\"  Total: {total:.2f} GB\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Free: {free:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d49ee5-7236-4c5e-82c9-b00fe4b6f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GRPO MODEL BET√ñLT√âSE ===\n",
    "# A checkpoint-562 mappa csak LoRA adapter f√°jlokat tartalmaz\n",
    "# El≈ësz√∂r bet√∂ltj√ºk a base modellt, majd r√°rakjuk az adaptert\n",
    "BASE_MODEL_NAME = \"unsloth/qwen3-4b-instruct-2507\"\n",
    "\n",
    "# Biztos√≠tjuk, hogy MODEL_PATH Path objektum legyen\n",
    "if isinstance(MODEL_PATH, str):\n",
    "    model_path_obj = Path(MODEL_PATH)\n",
    "else:\n",
    "    model_path_obj = MODEL_PATH\n",
    "\n",
    "ADAPTER_PATH = model_path_obj / \"checkpoint-562\"\n",
    "\n",
    "print(\"\\nüîÑ GRPO model bet√∂lt√©se Unsloth-tal...\")\n",
    "print(f\"  Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "# El≈ësz√∂r bet√∂ltj√ºk a base modellt\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# R√°rakjuk a LoRA adaptert\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, str(ADAPTER_PATH))\n",
    "\n",
    "model.eval()  # Inference mode\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"\\n‚úÖ GRPO model bet√∂ltve\")\n",
    "print(f\"  Padding side: {tokenizer.padding_side}\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    print(f\"  GPU mem√≥ria haszn√°lat: {allocated:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503294c7-8476-421d-aaef-9772dd2b89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN QUERY IDs MEGHAT√ÅROZ√ÅSA ===\n",
    "# Az √∂sszes qrels query-b≈ël kivonva az eval split query-ket\n",
    "all_query_ids = set(qrels_dict.keys())\n",
    "eval_query_ids_set = set(eval_query_ids)\n",
    "train_query_ids = sorted(list(all_query_ids - eval_query_ids_set))\n",
    "\n",
    "print(f\"\\nüìä Query split:\")\n",
    "print(f\"  Eval queries: {len(eval_query_ids)}\")\n",
    "print(f\"  Train queries: {len(train_query_ids)}\")\n",
    "print(f\"  Total queries: {len(all_query_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5683a0a-5f46-4d96-838f-b7c491719ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION F√úGGV√âNY ===\n",
    "def evaluate_query(query_id: str,\n",
    "                   model,\n",
    "                   tokenizer,\n",
    "                   retriever_bm25,\n",
    "                   index_faiss,\n",
    "                   embedding_model,\n",
    "                   chunks_df,\n",
    "                   chunk_id_map,\n",
    "                   bm25_chunk_ids,\n",
    "                   qrels_dict,\n",
    "                   baseline_metrics_dict) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Egyetlen query √©rt√©kel√©se:\n",
    "    1. Slate generation\n",
    "    2. Prompt creation\n",
    "    3. Model inference\n",
    "    4. Parsing\n",
    "    5. Metric calculation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Generate slate\n",
    "        # üîí Note: qrels_dict passed only to store relevance for metrics, NOT for prompt!\n",
    "        slate = generate_slate_for_query(\n",
    "            query_id,\n",
    "            retriever_bm25,\n",
    "            index_faiss,\n",
    "            embedding_model,\n",
    "            chunks_df,\n",
    "            chunk_id_map,\n",
    "            bm25_chunk_ids,\n",
    "            qrels_dict,\n",
    "            top_k=SLATE_SIZE\n",
    "        )\n",
    "        \n",
    "        if len(slate) == 0:\n",
    "            return None\n",
    "        \n",
    "        # 2. Create BLIND evaluation prompt (no relevance labels!)\n",
    "        prompt = create_evaluation_prompt(query_id, slate)\n",
    "        \n",
    "        # 3. Model inference\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=True,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        completion = tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # 4. Parse ranking\n",
    "        policy_indices = parse_model_ranking(completion, len(slate))\n",
    "        \n",
    "        # 5. Get doc IDs\n",
    "        policy_doc_ids = [slate[i][\"doc_id\"] for i in policy_indices]\n",
    "        baseline_doc_ids = [slate[i][\"doc_id\"] for i in range(len(slate))]\n",
    "        \n",
    "        # 6. Compute metrics with ranx\n",
    "        if query_id not in qrels_dict:\n",
    "            return None\n",
    "        \n",
    "        qrels_ranx = Qrels({query_id: qrels_dict[query_id]})\n",
    "        \n",
    "        # Policy run\n",
    "        policy_run_dict = {\n",
    "            query_id: {\n",
    "                doc_id: len(policy_doc_ids) - i\n",
    "                for i, doc_id in enumerate(policy_doc_ids)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ‚ö†Ô∏è TRUNCATE TO TOP-K FOR FAIR COMPARISON WITH BASELINE\n",
    "        # Baseline evaluates on top-20, so we truncate GRPO policy results to top-20\n",
    "        policy_run_dict_truncated = {\n",
    "            query_id: dict(\n",
    "                sorted(\n",
    "                    policy_run_dict[query_id].items(),\n",
    "                    key=lambda x: x[1],\n",
    "                    reverse=True\n",
    "                )[:EVAL_TOP_K]\n",
    "            )\n",
    "        }\n",
    "        policy_run = Run(policy_run_dict_truncated)\n",
    "        \n",
    "        # Compute policy metrics\n",
    "        metrics_list = [\n",
    "            \"map\", \"mrr\",\n",
    "            \"ndcg@5\", \"ndcg@10\", \"ndcg@20\",\n",
    "            \"precision@5\", \"precision@10\", \"precision@20\",\n",
    "            \"recall@5\", \"recall@10\", \"recall@20\"\n",
    "        ]\n",
    "        \n",
    "        policy_metrics = evaluate(qrels_ranx, policy_run, metrics_list)\n",
    "        \n",
    "        # Get baseline metrics\n",
    "        baseline_metrics = baseline_metrics_dict.get(query_id, {})\n",
    "        \n",
    "        # Assemble result\n",
    "        result = {\"query_id\": query_id}\n",
    "        \n",
    "        for metric in metrics_list:\n",
    "            baseline_key = metric.replace(\"@\", \"@\")\n",
    "            result[f\"baseline_{baseline_key}\"] = baseline_metrics.get(baseline_key, 0.0)\n",
    "            result[f\"policy_{baseline_key}\"] = policy_metrics.get(metric, 0.0)\n",
    "        \n",
    "        # Improvement\n",
    "\n",
    "        result[\"improvement_ndcg@10\"] = (\n",
    "\n",
    "            result[\"policy_ndcg@10\"] - result[\"baseline_ndcg@10\"]print(\"‚úÖ Evaluation f√ºggv√©ny defini√°lva\")\n",
    "\n",
    "        )\n",
    "\n",
    "                return None\n",
    "\n",
    "        return result        print(f\"\\n‚ö†Ô∏è Hiba query-n√©l ({query_id[:50]}...): {e}\")\n",
    "\n",
    "            except Exception as e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912ce5b-87f4-4773-8c43-5e35c0434358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION LOOP - EVAL SPLIT ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ EVAL SPLIT √âRT√âKEL√âSE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for query_id in tqdm(eval_query_ids, desc=\"Eval queries\"):\n",
    "    result = evaluate_query(\n",
    "        query_id,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        retriever_bm25,\n",
    "        index_faiss,\n",
    "        embedding_model,\n",
    "        chunks_df,\n",
    "        chunk_id_map,\n",
    "        bm25_chunk_ids,\n",
    "        qrels_dict,\n",
    "        baseline_metrics_dict\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        eval_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Eval √©rt√©kel√©s k√©sz: {len(eval_results)}/{len(eval_query_ids)} sikeres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9a179-4751-4e09-9f4d-c27738b2e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION LOOP - TRAIN SPLIT ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ TRAIN SPLIT √âRT√âKEL√âSE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_results = []\n",
    "\n",
    "for query_id in tqdm(train_query_ids, desc=\"Train queries\"):\n",
    "    result = evaluate_query(\n",
    "        query_id,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        retriever_bm25,\n",
    "        index_faiss,\n",
    "        embedding_model,\n",
    "        chunks_df,\n",
    "        chunk_id_map,\n",
    "        bm25_chunk_ids,\n",
    "        qrels_dict,\n",
    "        baseline_metrics_dict\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        train_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Train √©rt√©kel√©s k√©sz: {len(train_results)}/{len(train_query_ids)} sikeres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35d927-e8ca-4a2e-babd-381d47a4e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVAL SPLIT EXPORT ===\n",
    "eval_df = pd.DataFrame(eval_results)\n",
    "eval_output_path = OUTPUT_DIR / \"eval_per_query_results.csv\"\n",
    "eval_df.to_csv(eval_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n‚úÖ Eval results export√°lva: {eval_output_path}\")\n",
    "print(f\"   {len(eval_df)} query √ó {len(eval_df.columns)} oszlop\")\n",
    "\n",
    "# === TRAIN SPLIT EXPORT ===\n",
    "train_df = pd.DataFrame(train_results)\n",
    "train_output_path = OUTPUT_DIR / \"train_per_query_results.csv\"\n",
    "train_df.to_csv(train_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n‚úÖ Train results export√°lva: {train_output_path}\")\n",
    "print(f\"   {len(train_df)} query √ó {len(train_df.columns)} oszlop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db3d19-2e14-427b-a696-18bb4f4a957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY STATISTICS ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Eval split summary\n",
    "if len(eval_df) > 0:\n",
    "    print(\"\\nüéØ EVAL SPLIT:\")\n",
    "    print(f\"  Query count: {len(eval_df)}\")\n",
    "    \n",
    "    for metric in ['ndcg@10', 'ndcg@5', 'map', 'mrr']:\n",
    "        baseline_mean = eval_df[f'baseline_{metric}'].mean()\n",
    "        policy_mean = eval_df[f'policy_{metric}'].mean()\n",
    "        improvement = policy_mean - baseline_mean\n",
    "        relative = (improvement / baseline_mean * 100) if baseline_mean > 0 else 0\n",
    "        \n",
    "        print(f\"\\n  {metric.upper()}:\")\n",
    "        print(f\"    Baseline: {baseline_mean:.4f}\")\n",
    "        print(f\"    Policy:   {policy_mean:.4f}\")\n",
    "        print(f\"    Œî:        {improvement:+.4f} ({relative:+.1f}%)\")\n",
    "\n",
    "# Train split summary\n",
    "if len(train_df) > 0:\n",
    "    print(\"\\n\\nüéØ TRAIN SPLIT:\")\n",
    "    print(f\"  Query count: {len(train_df)}\")\n",
    "    \n",
    "    for metric in ['ndcg@10', 'ndcg@5', 'map', 'mrr']:\n",
    "        baseline_mean = train_df[f'baseline_{metric}'].mean()\n",
    "        policy_mean = train_df[f'policy_{metric}'].mean()\n",
    "        improvement = policy_mean - baseline_mean\n",
    "        relative = (improvement / baseline_mean * 100) if baseline_mean > 0 else 0\n",
    "        \n",
    "        print(f\"\\n  {metric.upper()}:\")\n",
    "        print(f\"    Baseline: {baseline_mean:.4f}\")\n",
    "        print(f\"    Policy:   {policy_mean:.4f}\")\n",
    "        print(f\"    Œî:        {improvement:+.4f} ({relative:+.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30891f93-f53d-4eca-bedb-8bdbe54cad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY JSON EXPORT ===\n",
    "summary = {\n",
    "    \"eval_split\": {\n",
    "        \"query_count\": len(eval_df),\n",
    "        \"metrics\": {}\n",
    "    },\n",
    "    \"train_split\": {\n",
    "        \"query_count\": len(train_df),\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Eval metrics\n",
    "if len(eval_df) > 0:\n",
    "    for metric in ['ndcg@10', 'ndcg@5', 'map', 'mrr', 'recall@10']:\n",
    "        summary[\"eval_split\"][\"metrics\"][metric] = {\n",
    "            \"baseline\": float(eval_df[f'baseline_{metric}'].mean()),\n",
    "            \"policy\": float(eval_df[f'policy_{metric}'].mean()),\n",
    "            \"improvement\": float(eval_df[f'policy_{metric}'].mean() - eval_df[f'baseline_{metric}'].mean())\n",
    "        }\n",
    "\n",
    "# Train metrics\n",
    "if len(train_df) > 0:\n",
    "    for metric in ['ndcg@10', 'ndcg@5', 'map', 'mrr', 'recall@10']:\n",
    "        summary[\"train_split\"][\"metrics\"][metric] = {\n",
    "            \"baseline\": float(train_df[f'baseline_{metric}'].mean()),\n",
    "            \"policy\": float(train_df[f'policy_{metric}'].mean()),\n",
    "            \"improvement\": float(train_df[f'policy_{metric}'].mean() - train_df[f'baseline_{metric}'].mean())\n",
    "        }\n",
    "\n",
    "summary_path = OUTPUT_DIR / \"evaluation_summary.json\"\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Summary JSON export√°lva: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4d2d7-e500-4124-a052-38eb9972d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAMPLE RESULTS ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MINTA EREDM√âNYEK (Eval Split)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(eval_df) > 0:\n",
    "    # Top 5 legnagyobb javul√°s\n",
    "    top_improvements = eval_df.nlargest(5, 'improvement_ndcg@10')\n",
    "    \n",
    "    print(\"\\nüèÜ Top 5 legnagyobb javul√°s (nDCG@10):\")\n",
    "    for idx, row in top_improvements.iterrows():\n",
    "        print(f\"\\n  Query: {row['query_id'][:60]}...\")\n",
    "        print(f\"    Baseline nDCG@10: {row['baseline_ndcg@10']:.4f}\")\n",
    "        print(f\"    Policy nDCG@10:   {row['policy_ndcg@10']:.4f}\")\n",
    "        print(f\"    Improvement:      {row['improvement_ndcg@10']:+.4f}\")\n",
    "    \n",
    "    # Top 5 legnagyobb roml√°s\n",
    "    print(\"\\n\\n‚ö†Ô∏è Top 5 legnagyobb roml√°s (nDCG@10):\")\n",
    "    worst_improvements = eval_df.nsmallest(5, 'improvement_ndcg@10')\n",
    "    \n",
    "    for idx, row in worst_improvements.iterrows():\n",
    "        print(f\"\\n  Query: {row['query_id'][:60]}...\")\n",
    "        print(f\"    Baseline nDCG@10: {row['baseline_ndcg@10']:.4f}\")\n",
    "        print(f\"    Policy nDCG@10:   {row['policy_ndcg@10']:.4f}\")\n",
    "        print(f\"    Improvement:      {row['improvement_ndcg@10']:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EVALUATION K√âSZ!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÅ Kimeneti f√°jlok:\")\n",
    "print(f\"  ‚Ä¢ {eval_output_path}\")\n",
    "print(f\"  ‚Ä¢ {train_output_path}\")\n",
    "print(f\"  ‚Ä¢ {summary_path}\")\n",
    "print(\"\\nüí° T√∂ltsd le ezeket a f√°jlokat lok√°lisan a data/models/grpo_policy/ mapp√°ba!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad621a0-6ddc-48c3-9787-14f2d5ad31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ADATSZIV√ÅRG√ÅS VALID√ÅCI√ì ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîí ADATSZIV√ÅRG√ÅS ELLEN≈êRZ√âS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Promptban nincs relevancia c√≠mke?\n",
    "print(\"\\n1Ô∏è‚É£ Prompt valid√°ci√≥:\")\n",
    "test_query = eval_query_ids[0] if len(eval_query_ids) > 0 else list(qrels_dict.keys())[0]\n",
    "test_slate = generate_slate_for_query(\n",
    "    test_query,\n",
    "    retriever_bm25,\n",
    "    index_faiss,\n",
    "    embedding_model,\n",
    "    chunks_df,\n",
    "    chunk_id_map,\n",
    "    bm25_chunk_ids,\n",
    "    qrels_dict,\n",
    "    top_k=5\n",
    ")\n",
    "test_prompt = create_evaluation_prompt(test_query, test_slate)\n",
    "\n",
    "# Ellen≈ërz√©s: nincs-e \"Relevance:\" vagy \"Grade\" a promptban\n",
    "has_relevance_leak = \"Relevance:\" in test_prompt or \"Grade\" in test_prompt\n",
    "if has_relevance_leak:\n",
    "    print(\"   ‚ùå HIBA: Relevancia c√≠mke BENNE VAN a promptban!\")\n",
    "    print(\"   ‚ö†Ô∏è  Ez adatsziv√°rg√°st okoz!\")\n",
    "else:\n",
    "    print(\"   ‚úÖ PASS: Nincs relevancia c√≠mke a promptban\")\n",
    "\n",
    "# 2. Slate tartalmaz relevancia mez≈ët? (csak metrik√°khoz)\n",
    "if len(test_slate) > 0:\n",
    "    has_relevance_field = 'relevance' in test_slate[0]\n",
    "    if has_relevance_field:\n",
    "        print(\"\\n2Ô∏è‚É£ Slate tartalmaz relevancia mez≈ët:\")\n",
    "        print(\"   ‚úÖ PASS: relevance mez≈ë van (metrik√°khoz kell)\")\n",
    "        print(\"   üîí Ez rendben van, AM√çG nem ker√ºl a promptba!\")\n",
    "    else:\n",
    "        print(\"\\n2Ô∏è‚É£ Slate NEM tartalmaz relevancia mez≈ët:\")\n",
    "        print(\"   ‚ùå HIBA: relevance mez≈ë hi√°nyzik a slate-b≈ël!\")\n",
    "\n",
    "# 3. Eval/Train split tisztas√°ga\n",
    "print(\"\\n3Ô∏è‚É£ Eval/Train split valid√°ci√≥:\")\n",
    "eval_set = set(eval_query_ids)\n",
    "train_set = set(train_query_ids)\n",
    "overlap = eval_set & train_set\n",
    "\n",
    "if len(overlap) == 0:\n",
    "    print(f\"   ‚úÖ PASS: Nincs kontamin√°ci√≥ (0 √°tfed√©s)\")\n",
    "    print(f\"   ‚Ä¢ Eval split: {len(eval_set)} query\")\n",
    "    print(f\"   ‚Ä¢ Train split: {len(train_set)} query\")\n",
    "else:\n",
    "    print(f\"   ‚ùå HIBA: {len(overlap)} query √°tfed√©s van!\")\n",
    "\n",
    "# √ñsszegz√©s\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä VALID√ÅCI√ì √ñSSZEGZ√âS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_pass = not has_relevance_leak and has_relevance_field and len(overlap) == 0\n",
    "\n",
    "if all_pass:\n",
    "    print(\"‚úÖ MINDEN ELLEN≈êRZ√âS SIKERES!\")\n",
    "    print(\"   ‚Ä¢ Prompt tiszta (nincs adatsziv√°rg√°s)\")\n",
    "    print(\"   ‚Ä¢ Slate strukt√∫ra helyes\")\n",
    "    print(\"   ‚Ä¢ Nincs kontamin√°ci√≥ eval/train k√∂z√∂tt\")\n",
    "    print(\"\\nüí° Az eredm√©nyek megb√≠zhat√≥k √©s interpret√°lhat√≥k!\")\n",
    "else:\n",
    "    print(\"‚ùå VALID√ÅCI√ì SIKERTELEN!\")\n",
    "    if has_relevance_leak:\n",
    "        print(\"   ‚Ä¢ Relevancia c√≠mke BENNE VAN a promptban\")\n",
    "    if not has_relevance_field:\n",
    "        print(\"   ‚Ä¢ Relevancia mez≈ë HI√ÅNYZIK a slate-b≈ël\")\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"   ‚Ä¢ {len(overlap)} query kontamin√°ci√≥\")\n",
    "    print(\"\\n‚ö†Ô∏è  NE haszn√°ld ezeket az eredm√©nyeket, jav√≠tsd a notebook-ot!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courtrankrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
