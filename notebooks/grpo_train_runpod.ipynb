{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CourtRankRL GRPO Training - Chunk-Based, RTX 5090 Optimized\n",
        "\n",
        "## Agents.md Specifik√°ci√≥ (Chunk-Based)\n",
        "\n",
        "Ez a notebook a CourtRankRL GRPO alap√∫ reranking modell tan√≠t√°s√°t v√©gzi el **RTX 5090 GPU-n** (24GB VRAM).\n",
        "\n",
        "### F≈ëbb jellemz≈ëk (Chunk-Based megold√°s):\n",
        "- **Model**: Qwen/Qwen3-4B-Instruct-2507 (4-bit) + QLoRA (rank=64, alpha=128)\n",
        "- **Training**: TRL GRPOTrainer GRPO algoritmussal\n",
        "  - Loss: \"dapo\" (eliminates length bias)\n",
        "  - Reward scaling: \"batch\" (robust - PPO Lite)\n",
        "  - Importance sampling: \"sequence\" (stable - GSPO)\n",
        "- **Dataset**: 98 query (teljes), 20 chunk/slate, **TELJES chunk sz√∂veg** (~500-800 char)\n",
        "- **Slate strat√©gia**: Chunk-level retrieval (nem doc aggreg√°ci√≥!) ‚Üí legrelev√°nsabb chunk-ok\n",
        "- **Baseline**: Slate sorrendje = fusion ranking [0,1,2,...] (BM25+FAISS fusion szerint)\n",
        "- **Hardware**: Batch size 2, grad accumulation 2, 6 generations/prompt\n",
        "- **Training time**: ~45-60 perc (500 steps)\n",
        "- **Input**: training_slates.jsonl (chunk-based prepare_training_slates() kimenet)\n",
        "- **Output**: LoRA adapter weights + metrics JSON\n",
        "\n",
        "### El≈ëfelt√©telek:\n",
        "- **RTX 5090 GPU (24GB VRAM)** vagy hasonl√≥\n",
        "- HF token k√∂rnyezeti v√°ltoz√≥ban: `HUGGINGFACE_TOKEN`\n",
        "- Chunk-based slate JSONL f√°jl: `/workspace/training_slates.jsonl`\n",
        "\n",
        "### Chunk-based slate form√°tum:\n",
        "```json\n",
        "{\n",
        "  \"query_id\": \"magyar query sz√∂veg\",\n",
        "  \"slate\": [\n",
        "    {\n",
        "      \"chunk_id\": \"0302-G_20416_2019_11_0\",\n",
        "      \"doc_id\": \"0302-G_20416_2019_11\",\n",
        "      \"bm25_score\": 12.5,\n",
        "      \"faiss_score\": 0.85,\n",
        "      \"relevance\": 2,\n",
        "      \"court\": \"F≈ëv√°rosi T√∂rv√©nysz√©k\",\n",
        "      \"domain\": \"G\",\n",
        "      \"year\": \"2019\",\n",
        "      \"text\": \"TELJES chunk sz√∂veg (500-800 char) - nem preview!\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Mi√©rt chunk-based?\n",
        "- ‚úÖ **Relev√°ns kontextus**: BM25+FAISS m√°r kiv√°lasztotta a legrelev√°nsabb chunk-okat\n",
        "- ‚úÖ **Teljes sz√∂veg**: A model l√°tja, MI√âRT relev√°ns egy dokumentum\n",
        "- ‚úÖ **Jobb tanul√°s**: A model megtanulja √©rt√©kelni a val√≥di tartalmat, nem csak metaadatokat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K√∂rnyezet setup √©s csomagok telep√≠t√©se\n",
        "# Unsloth + vLLM dependencies (agents.md szerint)\n",
        "%pip install -q unsloth\n",
        "%pip install -q torch transformers peft trl datasets accelerate bitsandbytes\n",
        "%pip install -q numpy scipy scikit-learn huggingface_hub\n",
        "%pip install -q --upgrade pillow\n",
        "\n",
        "print(\"‚úÖ Csomagok telep√≠tve (Unsloth + vLLM + sklearn/scipy)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "# Unsloth API (helyettes√≠ti AutoModelForCausalLM, BitsAndBytesConfig, get_peft_model)\n",
        "from unsloth import FastLanguageModel\n",
        "from trl.trainer.grpo_trainer import GRPOTrainer\n",
        "from trl.trainer.grpo_config import GRPOConfig\n",
        "from huggingface_hub import login\n",
        "# Sklearn for standard NDCG calculation √©s split\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# SciPy entropy\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "# Train/test split (opcion√°lis)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"‚úÖ Importok bet√∂ltve (Unsloth + TRL + sklearn/scipy)\")\n",
        "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
        "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace bejelentkez√©s\n",
        "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"‚úÖ HuggingFace bejelentkez√©s sikeres\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Nincs HUGGINGFACE_TOKEN, a modell let√∂lt√©se korl√°tozott lehet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NDCG implement√°ci√≥ teszt (√∫j sklearn integr√°ci√≥)\n",
        "def test_ndcg_implementation():\n",
        "    \"\"\"NDCG implement√°ci√≥ teszt sklearn ndcg_score haszn√°lat√°val.\"\"\"\n",
        "    print(\"\\nüß™ NDCG implement√°ci√≥ teszt...\")\n",
        "\n",
        "    # Teszt case 1: Standard ranking (relevancia: [2,1,0,1,0])\n",
        "    ranked = [0, 1, 2, 3, 4]  # Model predikci√≥\n",
        "    relevance = [2, 1, 0, 1, 0]  # Ground truth\n",
        "    ndcg = calculate_ndcg(ranked, relevance, k=5)\n",
        "    print(f\"  Standard ranking NDCG@5: {ndcg:.4f}\")\n",
        "\n",
        "    # Teszt case 2: Perfect ranking (relev√°ns elemek el≈ëre)\n",
        "    perfect_ranked = [0, 3, 1, 4, 2]  # 2,1,1,0,0 sorrendben\n",
        "    ndcg_perfect = calculate_ndcg(perfect_ranked, relevance, k=5)\n",
        "    print(f\"  Perfect ranking NDCG@5: {ndcg_perfect:.4f}\")\n",
        "\n",
        "    # Teszt case 3: Worst ranking (irrelev√°ns elemek el≈ëre)\n",
        "    worst_ranked = [2, 4, 1, 3, 0]  # 0,0,1,1,2 sorrendben\n",
        "    ndcg_worst = calculate_ndcg(worst_ranked, relevance, k=5)\n",
        "    print(f\"  Worst ranking NDCG@5: {ndcg_worst:.4f}\")\n",
        "\n",
        "    # Teszt case 4: Edge case (nincs relev√°ns dokumentum)\n",
        "    no_rel_ranked = [0, 1, 2, 3, 4]\n",
        "    no_rel_relevance = [0, 0, 0, 0, 0]\n",
        "    ndcg_no_rel = calculate_ndcg(no_rel_ranked, no_rel_relevance, k=5)\n",
        "    print(f\"  No relevance NDCG@5: {ndcg_no_rel:.4f}\")\n",
        "\n",
        "    print(\"‚úÖ NDCG teszt befejezve\")\n",
        "\n",
        "test_ndcg_implementation()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Konfigur√°ci√≥ (RTX 5090 optimaliz√°lt - agents.md szerint, Unsloth-accelerated)\n",
        "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "\n",
        "# Dataset (agents.md: teljes 98 query, 20 chunk/slate)\n",
        "SLATE_SIZE = 20\n",
        "GROUP_SIZE = 20  # = SLATE_SIZE\n",
        "# (Chunk-based, teljes sz√∂veg - nem preview)\n",
        "\n",
        "# LoRA konfigur√°ci√≥ (agents.md: rank=64, alpha=128)\n",
        "LORA_RANK = 64\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Unsloth specifikus be√°ll√≠t√°sok\n",
        "MAX_SEQ_LENGTH = 8192  # Context window (chunk-based slates: 20√ó640char + metadata ‚âà 5-6k token)\n",
        "GPU_MEMORY_UTILIZATION = 0.8  # vLLM mem√≥ria limit (RTX 5090)\n",
        "USE_GRADIENT_CHECKPOINTING = \"unsloth\"  # Unsloth native checkpointing\n",
        "\n",
        "# Training konfigur√°ci√≥ (RTX 5090 + Unsloth optimized)\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_STEPS = 500  # ~5 epoch (98 query √ó 5)\n",
        "SAVE_STEPS = 500  # Csak final save\n",
        "EVAL_STEPS = 50\n",
        "LOGGING_STEPS = 10\n",
        "WARMUP_STEPS = 50  # 10% warmup\n",
        "\n",
        "# Unsloth + vLLM optimiz√°ci√≥k:\n",
        "# - Batch 4 (vs 2): 50% memory savings from Unsloth gradient checkpointing\n",
        "# - Generations 10 (vs 6): vLLM 2-3x faster inference\n",
        "# - Effective batch 8, 40 generations/step (vs 4 batch, 12 gen/step)\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # Megmarad (stability)\n",
        "NUM_GENERATIONS = 10  # Unsloth + vLLM optimized (6 ‚Üí 10)\n",
        "PER_DEVICE_BATCH_SIZE = 4  # RTX 5090 + Unsloth optimized (2 ‚Üí 4)\n",
        "\n",
        "# GRPO Reward (agents.md szerint)\n",
        "NDCG_K = 10\n",
        "ENTROPY_BONUS = 0.01  # Exploration\n",
        "REWARD_CLIP_MIN = -1.0\n",
        "REWARD_CLIP_MAX = 1.0\n",
        "\n",
        "# Train/Eval split (agents.md: 80/20, seed 42)\n",
        "TRAIN_SPLIT = 0.8\n",
        "SEED = 42\n",
        "\n",
        "# Paths (RunPod workspace)\n",
        "BASE_PATH = Path(os.getenv(\"WORKSPACE_PATH\", \"/workspace\"))\n",
        "SLATE_FILE = BASE_PATH / \"training_slates.jsonl\"\n",
        "OUTPUT_DIR = BASE_PATH / \"artifacts\" / \"grpo_policy\"\n",
        "METRICS_FILE = OUTPUT_DIR / \"metrics.json\"\n",
        "\n",
        "print(\"üìã RTX 5090 + Unsloth Konfigur√°ci√≥:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  LoRA: rank={LORA_RANK}, alpha={LORA_ALPHA}\")\n",
        "print(f\"  Max seq length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"  Gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
        "print(f\"  Batch: {PER_DEVICE_BATCH_SIZE} √ó {GRADIENT_ACCUMULATION_STEPS} = {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Steps: {MAX_STEPS}, Generations: {NUM_GENERATIONS}\")\n",
        "print(f\"  Generations/step: {PER_DEVICE_BATCH_SIZE * NUM_GENERATIONS}\")\n",
        "print(f\"  Slate file: {SLATE_FILE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")\n",
        "\n",
        "# Ellen≈ërz√©s\n",
        "if not SLATE_FILE.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Slate f√°jl nem tal√°lhat√≥: {SLATE_FILE}\")\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seg√©df√ºggv√©nyek\n",
        "\n",
        "def calculate_ndcg(ranked_indices: List[int], true_relevance: List[float], k: int = 10) -> float:\n",
        "    \"\"\"\n",
        "    NDCG@k sz√°m√≠t√°s agents.md szerint sklearn ndcg_score haszn√°lat√°val.\n",
        "    Standard formula: DCG = sum((2^rel_i - 1) / log2(i + 2)), IDCG hasonl√≥.\n",
        "\n",
        "    Args:\n",
        "        ranked_indices: Model √°ltal predikt√°lt ranking indexei [0,1,2,...]\n",
        "        true_relevance: Igazi relevancia √©rt√©kek [0,1,2] lista\n",
        "        k: Top-k dokumentumot vesz√ºnk figyelembe\n",
        "\n",
        "    Returns:\n",
        "        NDCG@k score [0,1] k√∂z√∂tt, vagy 0.0 ha nincs relev√°ns dokumentum\n",
        "    \"\"\"\n",
        "    if not true_relevance or not ranked_indices:\n",
        "        return 0.0\n",
        "\n",
        "    # Edge case: nincs relev√°ns dokumentum a top-k-ban\n",
        "    if max(true_relevance) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Sklearn ndcg_score form√°tumba konvert√°l√°s\n",
        "    # y_true: relevancia √©rt√©kek (nem kell ranked form√°tumba)\n",
        "    # y_score: predikt√°lt ranking alapj√°n rendezett relevanci√°k\n",
        "    y_true = np.array(true_relevance)\n",
        "\n",
        "    # Predikt√°lt ranking alapj√°n y_score k√©sz√≠t√©se (magasabb ranking = magasabb score)\n",
        "    # sklearn elv√°rja, hogy a y_score t√ºkr√∂zze a predikt√°lt sorrendet\n",
        "    max_score = len(ranked_indices)\n",
        "    y_score = np.zeros_like(y_true, dtype=float)\n",
        "\n",
        "    for i, idx in enumerate(ranked_indices[:k]):\n",
        "        if idx < len(y_true):\n",
        "            # Magasabb ranking (kisebb index) = magasabb score\n",
        "            y_score[idx] = max_score - i\n",
        "\n",
        "    # Ha nincs predikt√°lt score, fallback a jelenlegi implement√°ci√≥ra\n",
        "    if np.sum(y_score) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Sklearn ndcg_score haszn√°lata (standard formula: 2^rel - 1 gain)\n",
        "    try:\n",
        "        ndcg = ndcg_score(y_true.reshape(1, -1), y_score.reshape(1, -1), k=k)\n",
        "        return float(ndcg)\n",
        "    except Exception:\n",
        "        # Fallback jelenlegi implement√°ci√≥ra hiba eset√©n\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def calculate_entropy(ranking: List[int]) -> float:\n",
        "    \"\"\"Entropy sz√°m√≠t√°s a ranking diverzit√°s√°hoz (scipy.stats.entropy haszn√°lat√°val).\"\"\"\n",
        "    if not ranking:\n",
        "        return 0.0\n",
        "\n",
        "    counts = {}\n",
        "    for idx in ranking:\n",
        "        counts[idx] = counts.get(idx, 0) + 1\n",
        "\n",
        "    # Relat√≠v gyakoris√°gok sz√°m√≠t√°sa\n",
        "    total = sum(counts.values())\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "\n",
        "    probs = np.array([count / total for count in counts.values()], dtype=float)\n",
        "\n",
        "    # SciPy entropy haszn√°lata (stabil √©s optimaliz√°lt)\n",
        "    entropy = scipy_entropy(probs, base=2)  # Shannon-entropy (base=2)\n",
        "\n",
        "    return float(entropy)\n",
        "\n",
        "\n",
        "def parse_model_ranking(completion: str, slate_size: int = SLATE_SIZE) -> List[int]:\n",
        "    \"\"\"\n",
        "    Model kimenetb≈ël ranking kinyer√©se.\n",
        "    V√°rhat√≥ form√°tum: \"1,3,2,4,0\" vagy \"1, 3, 2, 4, 0\"\n",
        "    \n",
        "    Jav√≠tott fallback (agents.md): random shuffle (nem baseline!) \n",
        "    hogy a model ne tanuljon meg baseline-t outputolni hiba eset√©n.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        numbers = [int(x.strip()) for x in completion.split(\",\") if x.strip().isdigit()]\n",
        "        # Csak valid indexeket tartunk meg\n",
        "        valid_numbers = [n for n in numbers if 0 <= n < slate_size]\n",
        "        \n",
        "        if len(valid_numbers) >= slate_size // 2:\n",
        "            # Ha legal√°bb fele valid, haszn√°ljuk\n",
        "            return valid_numbers[:slate_size]\n",
        "        else:\n",
        "            # Ha t√∫l kev√©s valid sz√°m: random shuffle (b√ºntet√©shez vezet)\n",
        "            indices = list(range(slate_size))\n",
        "            random.shuffle(indices)\n",
        "            return indices\n",
        "    except:\n",
        "        # Parse error: random shuffle (b√ºntet√©shez vezet - agents.md)\n",
        "        indices = list(range(slate_size))\n",
        "        random.shuffle(indices)\n",
        "        return indices\n",
        "\n",
        "\n",
        "print(\"‚úÖ Seg√©df√ºggv√©nyek defini√°lva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slate adatok bet√∂lt√©se\n",
        "print(f\"üìÇ Slate adatok bet√∂lt√©se: {SLATE_FILE}\")\n",
        "\n",
        "slates_data = []\n",
        "with open(SLATE_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line_num, line in enumerate(f, 1):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            slate = json.loads(line)\n",
        "            slates_data.append(slate)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è JSON hiba a {line_num}. sorban: {e}\")\n",
        "            continue\n",
        "\n",
        "if not slates_data:\n",
        "    raise ValueError(\"‚ùå Nincs bet√∂lthet≈ë slate adat!\")\n",
        "\n",
        "print(f\"‚úÖ Bet√∂ltve: {len(slates_data)} slate\")\n",
        "\n",
        "# Slate form√°tum valid√°ci√≥\n",
        "sample = slates_data[0]\n",
        "print(f\"\\nüìã Minta slate strukt√∫ra:\")\n",
        "print(f\"  Query ID: {sample['query_id'][:50]}...\")\n",
        "print(f\"  Slate elemek: {len(sample['slate'])}\")\n",
        "print(f\"  Minta elem kulcsok: {list(sample['slate'][0].keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Test enhanced prompt\n",
        "test_prompt = create_training_prompt(slates_data[0][\"query_id\"], slates_data[0][\"slate\"])\n",
        "print(\"üìù Enhanced learning-to-rank prompt sample:\")\n",
        "print(\"=\"*80)\n",
        "print(test_prompt[:1500])  # First 1500 chars for preview\n",
        "print(\"\\n... (truncated)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä Prompt Statistics:\")\n",
        "print(f\"  Total length: {len(test_prompt)} characters\")\n",
        "print(f\"  Estimated tokens: ~{len(test_prompt.split())*1.2:.0f}\")\n",
        "print(f\"  Average chunk length: {sum(len(c.get('text', '')) for c in slates_data[0]['slate']) / len(slates_data[0]['slate']):.0f} characters\")\n",
        "print(f\"  Number of candidates: {len(slates_data[0]['slate'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset el≈ëk√©sz√≠t√©se TRL GRPOTrainer-hez (chunk-based, shuffled split)\n",
        "# Agents.md: TRL-kompatibilis data passing via global slate lookup dict\n",
        "\n",
        "training_examples = []\n",
        "slate_lookup = {}  # Global dict: query_id -> slate_data (reward function-h√∂z)\n",
        "\n",
        "for slate_data in slates_data:\n",
        "    query_id = slate_data[\"query_id\"]\n",
        "    prompt = create_training_prompt(query_id, slate_data[\"slate\"])\n",
        "    \n",
        "    # Dataset csak a prompt-ot tartalmazza (TRL best practice)\n",
        "    training_examples.append({\n",
        "        \"prompt\": prompt\n",
        "    })\n",
        "    \n",
        "    # Slate metadata k√ºl√∂n t√°rol√°sa (reward function-h√∂z)\n",
        "    slate_lookup[query_id] = slate_data[\"slate\"]\n",
        "\n",
        "# Full dataset\n",
        "full_dataset = Dataset.from_list(training_examples)\n",
        "\n",
        "# Train/eval split (Agents.md: 80/20, sklearn split, seed=42)\n",
        "indices = np.arange(len(full_dataset))\n",
        "train_indices, eval_indices = train_test_split(indices, test_size=1.0 - TRAIN_SPLIT, random_state=SEED, shuffle=True)\n",
        "\n",
        "train_dataset = full_dataset.select(train_indices)\n",
        "eval_dataset = full_dataset.select(eval_indices)\n",
        "\n",
        "print(f\"‚úÖ Dataset l√©trehozva (sklearn split):\")\n",
        "print(f\"  Training: {len(train_dataset)} query (80%)\")\n",
        "print(f\"  Evaluation: {len(eval_dataset)} query (20%)\")\n",
        "print(f\"  Slate lookup: {len(slate_lookup)} entry (global dict)\")\n",
        "print(f\"  Slate size: {SLATE_SIZE} chunk/query\")\n",
        "print(f\"  Random seed: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Unsloth Model & Tokenizer Loading ===\n",
        "print(f\"üîÑ Model bet√∂lt√©se Unsloth-tal: {MODEL_NAME}\")\n",
        "\n",
        "# FastLanguageModel.from_pretrained with vLLM support\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    load_in_4bit=True,\n",
        "    fast_inference=False,\n",
        "    max_lora_rank=LORA_RANK,\n",
        "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model √©s tokenizer bet√∂ltve (Unsloth + vLLM)\")\n",
        "\n",
        "# Unsloth LoRA setup\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    # 7 target modules: agents.md RTX 5090 spec\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    use_gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Unsloth LoRA adapterek alkalmazva:\")\n",
        "print(f\"  Rank: {LORA_RANK}, Alpha: {LORA_ALPHA}\")\n",
        "print(f\"  Target modules: 7 (full coverage)\")\n",
        "print(f\"  Gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
        "print(f\"  vLLM inference: ENABLED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Reward Function (TRL-kompatibilis, chunk-based, jav√≠tott)\n",
        "\n",
        "def reward_function(completions, prompts, **kwargs):\n",
        "    \"\"\"\n",
        "    TRL-kompatibilis GRPO reward function (chunk-based, jav√≠tott).\n",
        "    \n",
        "    Agents.md szerinti jav√≠t√°sok:\n",
        "    - Regex-based query_id parsing (robust)\n",
        "    - Baseline order = slate fusion ranking [0,1,2,...]\n",
        "    - Negative penalty for parse failures (not zero)\n",
        "    - nDCG@10 difference (core GRPO objective)\n",
        "    - Entropy bonus (exploration)\n",
        "    - Reward clipping (stability)\n",
        "    \n",
        "    Args:\n",
        "        completions: Model √°ltal gener√°lt output lista\n",
        "        prompts: Input prompt lista (query_id extraction-h√∂z)\n",
        "        **kwargs: Tov√°bbi TRL argumentumok\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    \n",
        "    for completion, prompt in zip(completions, prompts):\n",
        "        try:\n",
        "            # Query ID kinyer√©se REGEX-szel (robust - agents.md)\n",
        "            match = re.search(r'QUERY:\\s*\"([^\"]+)\"', prompt)\n",
        "            \n",
        "            if not match:\n",
        "                # Parse failure: negative penalty (agents.md)\n",
        "                rewards.append(-0.5)\n",
        "                continue\n",
        "                \n",
        "            query_id = match.group(1)\n",
        "            \n",
        "            if query_id not in slate_lookup:\n",
        "                rewards.append(-0.5)\n",
        "                continue\n",
        "            \n",
        "            # Slate metadata lookup (global dict)\n",
        "            slate = slate_lookup[query_id]\n",
        "            relevance = [doc.get('relevance', 0) for doc in slate]\n",
        "            \n",
        "            # BASELINE ORDER: slate m√°r fusion szerint rendezett (agents.md)\n",
        "            # A slate-ben l√©v≈ë sorrend [0,1,2,...] = fusion baseline!\n",
        "            baseline = list(range(len(slate)))\n",
        "            \n",
        "            # Parse model ranking\n",
        "            predicted = parse_model_ranking(completion, len(slate))\n",
        "            \n",
        "            # GRPO core: nDCG@10 difference\n",
        "            ndcg_baseline = calculate_ndcg(baseline, relevance, k=NDCG_K)\n",
        "            ndcg_policy = calculate_ndcg(predicted, relevance, k=NDCG_K)\n",
        "            reward = ndcg_policy - ndcg_baseline\n",
        "            \n",
        "            # Entropy bonus (exploration - agents.md: 0.01 weight)\n",
        "            if len(predicted) > 1:\n",
        "                unique_ratio = len(set(predicted)) / len(predicted)\n",
        "                reward += ENTROPY_BONUS * unique_ratio\n",
        "            \n",
        "            # Reward clipping (stability - agents.md)\n",
        "            reward = float(np.clip(reward, REWARD_CLIP_MIN, REWARD_CLIP_MAX))\n",
        "            rewards.append(reward)\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Unexpected error: negative penalty\n",
        "            rewards.append(-0.5)\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "\n",
        "print(\"‚úÖ GRPO Reward function defini√°lva (chunk-based, jav√≠tott)\")\n",
        "print(f\"  Query parsing: regex-based (robust)\")\n",
        "print(f\"  Baseline: slate fusion order [0,1,2,...]\")\n",
        "print(f\"  Core: nDCG@{NDCG_K} difference\")\n",
        "print(f\"  Entropy bonus: {ENTROPY_BONUS}\")\n",
        "print(f\"  Parse failure penalty: -0.5\")\n",
        "print(f\"  Clipping: [{REWARD_CLIP_MIN}, {REWARD_CLIP_MAX}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Trainer konfigur√°ci√≥ (RTX 5090 optimaliz√°lt + TRL best practices)\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "\n",
        "    # === GRPO Core (csak biztosan l√©tez≈ë param√©terek) ===\n",
        "    max_steps=MAX_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "\n",
        "    # === GRPO Algorithm ===\n",
        "    epsilon=0.2,  # GRPO clipping\n",
        "\n",
        "    # === Training ===\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\n",
        "        \"use_reentrant\": False,\n",
        "        \"use_unsloth\": True\n",
        "    },\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    # === Logging ===\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_first_step=True,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "\n",
        "    # === Other ===\n",
        "    dataloader_num_workers=2,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GRPO Trainer konfigur√°ci√≥ (Unsloth + TRL best practices):\")\n",
        "print(f\"  Loss: {grpo_config.loss_type} (eliminates length bias)\")\n",
        "print(f\"  Reward scaling: {grpo_config.scale_rewards} (robust)\")\n",
        "print(f\"  Group size: {grpo_config.group_size}\")\n",
        "print(f\"  Batch: {grpo_config.per_device_train_batch_size} √ó {grpo_config.gradient_accumulation_steps} = {grpo_config.per_device_train_batch_size * grpo_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Steps: {grpo_config.max_steps}, Generations: {grpo_config.num_generations}\")\n",
        "print(f\"  Generations/step: {grpo_config.per_device_train_batch_size * grpo_config.num_generations}\")\n",
        "print(f\"  Gradient checkpointing: Unsloth (use_unsloth=True)\")\n",
        "print(f\"  KL coef: {grpo_config.kl_coef} (disabled)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Trainer inicializ√°l√°sa (train/eval split)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=reward_function,\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset,  # 80% (agents.md)\n",
        "    eval_dataset=eval_dataset,     # 20% (agents.md)\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GRPO Trainer inicializ√°lva (train/eval split)\")\n",
        "print(f\"\\nüìä Training inform√°ci√≥k:\")\n",
        "print(f\"  Training queries: {len(train_dataset)}\")\n",
        "print(f\"  Eval queries: {len(eval_dataset)}\")\n",
        "print(f\"  Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Total training steps: {MAX_STEPS}\")\n",
        "print(f\"  GPU mem√≥ria: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training ind√≠t√°sa\n",
        "print(\"\\nüöÄ GRPO TRAINING IND√çT√ÅSA\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ Training sikeresen befejezve!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training hiba: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Artifactumok ment√©se\n",
        "print(\"\\nüíæ Artifactumok ment√©se...\")\n",
        "\n",
        "# Unsloth modell ment√©se (LoRA adapter only - agents.md: cloud-only)\n",
        "model.save_pretrained_merged(\n",
        "    str(OUTPUT_DIR),\n",
        "    tokenizer,\n",
        "    save_method=\"lora\",  # Csak LoRA adapter (agents.md spec)\n",
        ")\n",
        "print(f\"  ‚úÖ LoRA adapter (Unsloth): {OUTPUT_DIR}\")\n",
        "\n",
        "# Metrics ment√©se\n",
        "final_metrics = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"training_samples\": len(dataset),\n",
        "    \"slate_size\": SLATE_SIZE,\n",
        "    \"group_size\": GROUP_SIZE,\n",
        "    \"max_steps\": MAX_STEPS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"lora_rank\": LORA_RANK,\n",
        "    \"lora_alpha\": LORA_ALPHA,\n",
        "    \"final_loss\": trainer.state.log_history[-1].get(\"loss\", 0.0) if trainer.state.log_history else 0.0,\n",
        "    \"total_steps\": len(trainer.state.log_history) if trainer.state.log_history else 0,\n",
        "    \"status\": \"completed\"\n",
        "}\n",
        "\n",
        "with open(METRICS_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"  ‚úÖ Metrics: {METRICS_FILE}\")\n",
        "\n",
        "print(\"\\n‚úÖ Minden artifact sikeresen mentve!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training √∂sszefoglal√≥ (Unsloth + vLLM)\n",
        "\n",
        "### Technol√≥giai stack:\n",
        "- **Framework**: Unsloth + TRL GRPOTrainer\n",
        "- **Inference**: vLLM (2-3x gyorsabb generation)\n",
        "- **Model**: Qwen/Qwen3-4B-Instruct-2507 (4-bit + QLoRA)\n",
        "- **Optimaliz√°ci√≥k**: \n",
        "  - Unsloth gradient checkpointing (50%+ mem√≥ria megtakar√≠t√°s)\n",
        "  - vLLM fast inference (batch generation)\n",
        "  - Optimaliz√°lt batch size (4) √©s generations (10)\n",
        "  - Effective batch: 8, Generations/step: 40\n",
        "\n",
        "### Gener√°lt artifactumok:\n",
        "\n",
        "A training befejezt√©vel a k√∂vetkez≈ë f√°jlok ker√ºltek l√©trehoz√°sra a `/workspace/artifacts/grpo_policy/` k√∂nyvt√°rban:\n",
        "\n",
        "- `adapter_model.bin` - LoRA adapter weights\n",
        "- `adapter_config.json` - LoRA konfigur√°ci√≥\n",
        "- `tokenizer_config.json` - Tokenizer konfigur√°ci√≥\n",
        "- `tokenizer.json` - Tokenizer weights\n",
        "- `metrics.json` - Training metrik√°k √©s konfigur√°ci√≥\n",
        "\n",
        "### K√∂vetkez≈ë l√©p√©sek:\n",
        "\n",
        "1. **Artifactumok let√∂lt√©se:**\n",
        "   ```bash\n",
        "   # RunPod termin√°lb√≥l\n",
        "   cd /workspace/artifacts/grpo_policy\n",
        "   ls -lh\n",
        "   ```\n",
        "\n",
        "2. **Metrics elemz√©se:**\n",
        "   ```bash\n",
        "   cat metrics.json\n",
        "   ```\n",
        "\n",
        "3. **Helyi elemz√©s:**\n",
        "   - T√∂ltsd le a `metrics.json` f√°jlt a lok√°lis `data/models/grpo_policy/` k√∂nyvt√°rba\n",
        "   - Az adapter weights-eket cloud-on kell hagyni (lok√°lis inference nem t√°mogatott)\n",
        "\n",
        "### Agents.md specifik√°ci√≥ szerint:\n",
        "\n",
        "- ‚úÖ Unsloth-accelerated cloud training (RunPod)\n",
        "- ‚úÖ vLLM inference (GRPO generations)\n",
        "- ‚úÖ Qwen/Qwen3-4B-Instruct-2507 + QLoRA (unchanged model)\n",
        "- ‚úÖ Chunk-based slates (full text)\n",
        "- ‚úÖ Optimaliz√°lt hyperparam√©terek (batch=4, gen=10)\n",
        "- ‚úÖ Group size = slate length (20)\n",
        "- ‚úÖ NDCG@10 alap√∫ reward\n",
        "- ‚úÖ Hungarian status messages\n",
        "- ‚úÖ Artifact export `/workspace/artifacts/grpo_policy/`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
