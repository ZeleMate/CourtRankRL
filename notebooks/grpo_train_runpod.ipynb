{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CourtRankRL GRPO Training - Chunk-Based, RTX 5090 Optimized\n",
        "\n",
        "## Agents.md Specifik√°ci√≥ (Chunk-Based)\n",
        "\n",
        "Ez a notebook a CourtRankRL GRPO alap√∫ reranking modell tan√≠t√°s√°t v√©gzi el **RTX 5090 GPU-n** (24GB VRAM).\n",
        "\n",
        "### F≈ëbb jellemz≈ëk (Chunk-Based megold√°s):\n",
        "- **Model**: Qwen/Qwen3-4B-Instruct-2507 (4-bit) + QLoRA (rank=64, alpha=128)\n",
        "- **Training**: TRL GRPOTrainer GRPO algoritmussal\n",
        "  - Loss: \"dapo\" (eliminates length bias)\n",
        "  - Reward scaling: \"batch\" (robust - PPO Lite)\n",
        "  - Importance sampling: \"sequence\" (stable - GSPO)\n",
        "- **Dataset**: 98 query (teljes), 20 chunk/slate, **TELJES chunk sz√∂veg** (~500-800 char)\n",
        "- **Slate strat√©gia**: Chunk-level retrieval (nem doc aggreg√°ci√≥!) ‚Üí legrelev√°nsabb chunk-ok\n",
        "- **Baseline**: Slate sorrendje = fusion ranking [0,1,2,...] (BM25+FAISS fusion szerint)\n",
        "- **Hardware**: Batch size 4, grad accumulation 3, 14 generations/prompt\n",
        "- **Training time**: ~15-25 perc (600 steps, vLLM-mel)\n",
        "\n",
        "### Mi√©rt chunk-based?\n",
        "- ‚úÖ **Relev√°ns kontextus**: BM25+FAISS m√°r kiv√°lasztotta a legrelev√°nsabb chunk-okat\n",
        "- ‚úÖ **Teljes sz√∂veg**: A model l√°tja, MI√âRT relev√°ns egy dokumentum\n",
        "- ‚úÖ **Jobb tanul√°s**: A model megtanulja √©rt√©kelni a val√≥di tartalmat, nem csak metaadatokat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K√∂rnyezet setup √©s csomagok telep√≠t√©se\n",
        "%pip install -q --upgrade pip\n",
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install -q transformers accelerate datasets huggingface_hub\n",
        "%pip install -q numpy scipy scikit-learn pandas\n",
        "%pip install -q peft bitsandbytes\n",
        "%pip install -q trl\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "%pip install -q vllm\n",
        "%pip install -q ranx\n",
        "\n",
        "print(\"‚úÖ Csomagok telep√≠tve (kompatibilis verzi√≥k)\")\n",
        "print(\"‚ö†Ô∏è  FONTOS: RESTART RUNTIME sz√ºks√©ges a haszn√°lat el≈ëtt!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importok\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl.trainer.grpo_trainer import GRPOTrainer\n",
        "from trl.trainer.grpo_config import GRPOConfig\n",
        "from huggingface_hub import login\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ranx import Qrels, Run, evaluate\n",
        "\n",
        "print(\"‚úÖ Importok bet√∂ltve (Unsloth + TRL + sklearn + scipy + ranx)\")\n",
        "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
        "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace bejelentkez√©s\n",
        "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"‚úÖ HuggingFace bejelentkez√©s sikeres\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Nincs HUGGINGFACE_TOKEN, a modell let√∂lt√©se korl√°tozott lehet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Konfigur√°ci√≥\n",
        "MODEL_NAME = \"unsloth/Qwen3-4B-Instruct-2507\"\n",
        "SLATE_SIZE = 20\n",
        "GROUP_SIZE = 8\n",
        "LORA_RANK = 64\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT = 0.05\n",
        "MAX_SEQ_LENGTH = 8192\n",
        "GPU_MEMORY_UTILIZATION = 0.88\n",
        "USE_GRADIENT_CHECKPOINTING = \"unsloth\"\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_STEPS = 600\n",
        "SAVE_STEPS = 600\n",
        "EVAL_STEPS = 50\n",
        "LOGGING_STEPS = 10\n",
        "WARMUP_STEPS = 50\n",
        "GRADIENT_ACCUMULATION_STEPS = 3\n",
        "NUM_GENERATIONS = 14\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "GENERATION_BATCH_SIZE = PER_DEVICE_BATCH_SIZE * NUM_GENERATIONS\n",
        "OPTIMIZER_NAME = \"paged_adamw_8bit\"\n",
        "LR_SCHEDULER_TYPE = \"cosine\"\n",
        "\n",
        "NDCG_K = 10\n",
        "ENTROPY_BONUS = 0.01\n",
        "REWARD_CLIP_MIN = -1.0\n",
        "REWARD_CLIP_MAX = 1.0\n",
        "TRAIN_SPLIT = 0.8\n",
        "SEED = 42\n",
        "\n",
        "BASE_PATH = Path(os.getenv(\"WORKSPACE_PATH\", \"/workspace\"))\n",
        "SLATE_FILE = BASE_PATH / \"training_slates.jsonl\"\n",
        "OUTPUT_DIR = BASE_PATH / \"artifacts\" / \"grpo_policy\"\n",
        "METRICS_FILE = OUTPUT_DIR / \"metrics.json\"\n",
        "\n",
        "print(\"üìã RTX 5090 + Unsloth Konfigur√°ci√≥:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Batch: {PER_DEVICE_BATCH_SIZE} √ó {GRADIENT_ACCUMULATION_STEPS} = {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Steps: {MAX_STEPS}, Generations: {NUM_GENERATIONS}\")\n",
        "\n",
        "if not SLATE_FILE.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Slate f√°jl nem tal√°lhat√≥: {SLATE_FILE}\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seg√©df√ºggv√©nyek\n",
        "def calculate_ndcg(ranked_indices: List[int], true_relevance: List[float], k: int = 10) -> float:\n",
        "    if not true_relevance or not ranked_indices or max(true_relevance) == 0:\n",
        "        return 0.0\n",
        "    y_true = np.array(true_relevance)\n",
        "    max_score = len(ranked_indices)\n",
        "    y_score = np.zeros_like(y_true, dtype=float)\n",
        "    for i, idx in enumerate(ranked_indices[:k]):\n",
        "        if idx < len(y_true):\n",
        "            y_score[idx] = max_score - i\n",
        "    if np.sum(y_score) == 0:\n",
        "        return 0.0\n",
        "    try:\n",
        "        return float(ndcg_score(y_true.reshape(1, -1), y_score.reshape(1, -1), k=k))\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def parse_model_ranking(completion: str, slate_size: int = SLATE_SIZE) -> List[int]:\n",
        "    try:\n",
        "        numbers = [int(x.strip()) for x in completion.split(\",\") if x.strip().isdigit()]\n",
        "        valid_numbers = [n for n in numbers if 0 <= n < slate_size]\n",
        "        if len(valid_numbers) >= slate_size // 2:\n",
        "            return valid_numbers[:slate_size]\n",
        "    except:\n",
        "        pass\n",
        "    indices = list(range(slate_size))\n",
        "    random.shuffle(indices)\n",
        "    return indices\n",
        "\n",
        "def create_training_prompt(query_id: str, slate: List[Dict]) -> str:\n",
        "    prompt = f'''# Document Relevance Ranking Task\n",
        "\n",
        "TASK: Rank the following document excerpts by relevance to the query.\n",
        "\n",
        "QUERY: \"{query_id}\"\n",
        "\n",
        "CANDIDATES ({len(slate)} items):\n",
        "\n",
        "'''\n",
        "    for idx, doc in enumerate(slate):\n",
        "        chunk_text = doc.get('text', '')[:800]\n",
        "        prompt += f'''[{idx}] Doc: {doc.get('doc_id', 'N/A')} | Chunk: {doc.get('chunk_id', 'N/A')}\n",
        "B√≠r√≥s√°g: {doc.get('court', 'N/A')} | Ter√ºlet: {doc.get('domain', 'N/A')} | √âv: {doc.get('year', 'N/A')}\n",
        "BM25: {doc.get('bm25_score', 0):.2f} | FAISS: {doc.get('faiss_score', 0):.3f}\n",
        "Sz√∂veg: {chunk_text}\n",
        "\n",
        "'''\n",
        "    example_indices = \",\".join(str(i) for i in range(len(slate)))\n",
        "    prompt += f'''INSTRUCTION:\n",
        "Rank the documents by query relevance (starting from 0).\n",
        "Provide the ranking as comma-separated indices.\n",
        "Example format: {example_indices}\n",
        "Use each index 0-{len(slate)-1} exactly once.\n",
        "\n",
        "RANKING:'''\n",
        "    return prompt\n",
        "\n",
        "print(\"‚úÖ Seg√©df√ºggv√©nyek defini√°lva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slate adatok bet√∂lt√©se\n",
        "print(f\"üìÇ Slate adatok bet√∂lt√©se: {SLATE_FILE}\")\n",
        "df_slates = pd.read_json(SLATE_FILE, lines=True, encoding='utf-8')\n",
        "slates_data = df_slates.to_dict('records')\n",
        "print(f\"‚úÖ Bet√∂ltve: {len(slates_data)} slate\")\n",
        "\n",
        "sample = slates_data[0]\n",
        "print(f\"\\nüìã Minta slate strukt√∫ra:\")\n",
        "print(f\"  Query ID: {sample['query_id'][:50]}...\")\n",
        "print(f\"  Slate elemek: {len(sample['slate'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompt\n",
        "test_prompt = create_training_prompt(slates_data[0][\"query_id\"], slates_data[0][\"slate\"])\n",
        "print(\"üìù Enhanced learning-to-rank prompt sample:\")\n",
        "print(\"=\"*80)\n",
        "print(test_prompt[:1500])\n",
        "print(\"\\n... (truncated)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset el≈ëk√©sz√≠t√©se\n",
        "print(f\"\\nüìö Dataset el≈ëk√©sz√≠t√©se...\")\n",
        "training_examples = []\n",
        "slate_lookup = {}\n",
        "\n",
        "for slate_data in slates_data:\n",
        "    query_id = slate_data[\"query_id\"]\n",
        "    prompt = create_training_prompt(query_id, slate_data[\"slate\"])\n",
        "    training_examples.append({\"prompt\": prompt})\n",
        "    slate_lookup[query_id] = slate_data[\"slate\"]\n",
        "\n",
        "full_dataset = Dataset.from_list(training_examples)\n",
        "indices = np.arange(len(full_dataset))\n",
        "train_indices, eval_indices = train_test_split(indices, test_size=1.0 - TRAIN_SPLIT, random_state=SEED, shuffle=True)\n",
        "train_dataset = full_dataset.select(train_indices)\n",
        "eval_dataset = full_dataset.select(eval_indices)\n",
        "\n",
        "print(f\"‚úÖ Dataset l√©trehozva:\")\n",
        "print(f\"  Training: {len(train_dataset)} query (80%)\")\n",
        "print(f\"  Evaluation: {len(eval_dataset)} query (20%)\")\n",
        "print(f\"  Slate lookup: {len(slate_lookup)} entry\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model bet√∂lt√©se\n",
        "print(f\"üîÑ Model bet√∂lt√©se Unsloth-tal: {MODEL_NAME}\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        "    fast_inference=True,\n",
        "    max_lora_rank=LORA_RANK,\n",
        "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    use_gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
        "    use_rslora=True,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "print(\"‚úÖ Model √©s tokenizer bet√∂ltve (Unsloth + vLLM + RSLoRA)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reward function\n",
        "def reward_function(completions, prompts, **kwargs):\n",
        "    rewards = []\n",
        "    for completion, prompt in zip(completions, prompts):\n",
        "        try:\n",
        "            match = re.search(r'QUERY:\\s*\"([^\"]+)\"', prompt)\n",
        "            if not match or match.group(1) not in slate_lookup:\n",
        "                rewards.append(-0.5)\n",
        "                continue\n",
        "            \n",
        "            query_id = match.group(1)\n",
        "            slate = slate_lookup[query_id]\n",
        "            relevance = [doc.get('relevance', 0) for doc in slate]\n",
        "            baseline = list(range(len(slate)))\n",
        "            predicted = parse_model_ranking(completion, len(slate))\n",
        "            \n",
        "            ndcg_baseline = calculate_ndcg(baseline, relevance, k=NDCG_K)\n",
        "            ndcg_policy = calculate_ndcg(predicted, relevance, k=NDCG_K)\n",
        "            reward = ndcg_policy - ndcg_baseline\n",
        "            \n",
        "            is_valid_full = len(predicted) == len(slate) and len(set(predicted)) == len(slate)\n",
        "            is_valid_partial = len(predicted) >= len(slate)//2 and len(set(predicted)) >= len(slate)//2\n",
        "            if is_valid_full:\n",
        "                reward += 0.1\n",
        "            elif is_valid_partial:\n",
        "                reward += 0.05\n",
        "            \n",
        "            if len(predicted) > 1:\n",
        "                unique_ratio = len(set(predicted)) / len(predicted)\n",
        "                reward += ENTROPY_BONUS * unique_ratio\n",
        "            \n",
        "            reward = float(np.clip(reward, REWARD_CLIP_MIN, REWARD_CLIP_MAX))\n",
        "            rewards.append(reward)\n",
        "        except:\n",
        "            rewards.append(-0.5)\n",
        "    return rewards\n",
        "\n",
        "print(\"‚úÖ GRPO Reward function defini√°lva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Trainer konfigur√°ci√≥\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    max_steps=MAX_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    optim=OPTIMIZER_NAME,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    loss_type=\"dapo\",\n",
        "    scale_rewards=\"batch\",\n",
        "    importance_sampling_level=\"sequence\",\n",
        "    mask_truncated_completions=True,\n",
        "    epsilon=0.2,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False, \"use_unsloth\": True},\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_first_step=True,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    dataloader_num_workers=2,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GRPO Trainer konfigur√°ci√≥ k√©sz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trainer inicializ√°l√°sa\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=reward_function,\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GRPO Trainer inicializ√°lva\")\n",
        "print(f\"  Training queries: {len(train_dataset)}\")\n",
        "print(f\"  Eval queries: {len(eval_dataset)}\")\n",
        "print(f\"  GPU mem√≥ria: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training ind√≠t√°sa\n",
        "print(\"\\nüöÄ GRPO TRAINING IND√çT√ÅSA\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ Training sikeresen befejezve!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training hiba: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Artifactumok ment√©se\n",
        "print(\"\\nüíæ Artifactumok ment√©se...\")\n",
        "model.save_pretrained_merged(str(OUTPUT_DIR), tokenizer, save_method=\"lora\")\n",
        "print(f\"  ‚úÖ LoRA adapter: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation (ranx)\n",
        "print(\"\\nüìä Evaluation futtat√°sa...\")\n",
        "\n",
        "def evaluate_policy_ranx(dataset_subset, slate_lookup_dict, dataset_name=\"\"):\n",
        "    k_values = [5, 10, 20]\n",
        "    metrics_to_compute = [\"map\", \"mrr\"]\n",
        "    for k in k_values:\n",
        "        metrics_to_compute.extend([f\"ndcg@{k}\", f\"precision@{k}\", f\"recall@{k}\"])\n",
        "    \n",
        "    qrels_dict = {}\n",
        "    baseline_run_dict = {}\n",
        "    policy_run_dict = {}\n",
        "    parse_successes = 0\n",
        "    \n",
        "    for example in dataset_subset:\n",
        "        prompt = example[\"prompt\"]\n",
        "        match = re.search(r'QUERY:\\s*\"([^\"]+)\"', prompt)\n",
        "        if not match or match.group(1) not in slate_lookup_dict:\n",
        "            continue\n",
        "        \n",
        "        query_id = match.group(1)\n",
        "        slate = slate_lookup_dict[query_id]\n",
        "        \n",
        "        query_qrels = {}\n",
        "        for doc in slate:\n",
        "            doc_id = doc.get('doc_id')\n",
        "            relevance = doc.get('relevance', 0)\n",
        "            if doc_id:\n",
        "                query_qrels[doc_id] = relevance\n",
        "        if not query_qrels:\n",
        "            continue\n",
        "        qrels_dict[query_id] = query_qrels\n",
        "        \n",
        "        baseline_indices = list(range(len(slate)))\n",
        "        baseline_docs = {}\n",
        "        for rank, idx in enumerate(baseline_indices):\n",
        "            doc_id = slate[idx].get('doc_id')\n",
        "            if doc_id:\n",
        "                baseline_docs[doc_id] = len(slate) - rank\n",
        "        baseline_run_dict[query_id] = baseline_docs\n",
        "        \n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        model_output = model.generate(inputs.input_ids, max_new_tokens=60)\n",
        "        completion = tokenizer.decode(model_output[0][input_length:], skip_special_tokens=True)\n",
        "        predicted_indices = parse_model_ranking(completion, len(slate))\n",
        "        \n",
        "        is_valid_parse = len(predicted_indices) == len(slate) and len(set(predicted_indices)) == len(slate)\n",
        "        if is_valid_parse:\n",
        "            parse_successes += 1\n",
        "        \n",
        "        policy_docs = {}\n",
        "        for rank, idx in enumerate(predicted_indices):\n",
        "            if idx < len(slate):\n",
        "                doc_id = slate[idx].get('doc_id')\n",
        "                if doc_id:\n",
        "                    policy_docs[doc_id] = len(slate) - rank\n",
        "        policy_run_dict[query_id] = policy_docs\n",
        "    \n",
        "    qrels_ranx = Qrels(qrels_dict)\n",
        "    baseline_run_ranx = Run(baseline_run_dict)\n",
        "    policy_run_ranx = Run(policy_run_dict)\n",
        "    \n",
        "    baseline_metrics = evaluate(qrels_ranx, baseline_run_ranx, metrics_to_compute)\n",
        "    policy_metrics = evaluate(qrels_ranx, policy_run_ranx, metrics_to_compute)\n",
        "    \n",
        "    per_query_results = []\n",
        "    for query_id in qrels_dict.keys():\n",
        "        row = {\"query_id\": query_id}\n",
        "        for metric in metrics_to_compute:\n",
        "            baseline_score = baseline_run_ranx.scores.get(metric, {}).get(query_id, 0.0)\n",
        "            row[f\"baseline_{metric}\"] = float(baseline_score)\n",
        "        for metric in metrics_to_compute:\n",
        "            policy_score = policy_run_ranx.scores.get(metric, {}).get(query_id, 0.0)\n",
        "            row[f\"policy_{metric}\"] = float(policy_score)\n",
        "        baseline_ndcg10 = baseline_run_ranx.scores.get(\"ndcg@10\", {}).get(query_id, 0.0)\n",
        "        policy_ndcg10 = policy_run_ranx.scores.get(\"ndcg@10\", {}).get(query_id, 0.0)\n",
        "        row[\"improvement_ndcg@10\"] = float(policy_ndcg10 - baseline_ndcg10)\n",
        "        per_query_results.append(row)\n",
        "    \n",
        "    improvements = [r[\"improvement_ndcg@10\"] for r in per_query_results]\n",
        "    positive_improvements = sum(1 for imp in improvements if imp > 0)\n",
        "    positive_ratio = positive_improvements / len(improvements) if improvements else 0.0\n",
        "    parse_success_rate = parse_successes / len(per_query_results) if per_query_results else 0.0\n",
        "    \n",
        "    print(f\"  {dataset_name}:\")\n",
        "    print(f\"    Baseline nDCG@10: {baseline_metrics.get('ndcg@10', 0.0):.4f}\")\n",
        "    print(f\"    Policy nDCG@10: {policy_metrics.get('ndcg@10', 0.0):.4f}\")\n",
        "    print(f\"    Improvement: {policy_metrics.get('ndcg@10', 0.0) - baseline_metrics.get('ndcg@10', 0.0):+.4f}\")\n",
        "    \n",
        "    return {\n",
        "        \"baseline_metrics\": {k: float(v) for k, v in baseline_metrics.items()},\n",
        "        \"policy_metrics\": {k: float(v) for k, v in policy_metrics.items()},\n",
        "        \"num_queries\": len(qrels_dict),\n",
        "        \"positive_improvement_count\": positive_improvements,\n",
        "        \"positive_improvement_ratio\": float(positive_ratio),\n",
        "        \"parse_success_rate\": float(parse_success_rate),\n",
        "        \"parse_success_count\": parse_successes,\n",
        "        \"per_query_results\": per_query_results\n",
        "    }\n",
        "\n",
        "train_eval_results = evaluate_policy_ranx(train_dataset, slate_lookup, \"Train set\")\n",
        "eval_eval_results = evaluate_policy_ranx(eval_dataset, slate_lookup, \"Eval set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-query export\n",
        "training_rewards = []\n",
        "for log_entry in trainer.state.log_history:\n",
        "    if \"rewards/mean\" in log_entry:\n",
        "        training_rewards.append(log_entry[\"rewards/mean\"])\n",
        "\n",
        "train_per_query_df = pd.DataFrame(train_eval_results[\"per_query_results\"])\n",
        "train_per_query_csv = OUTPUT_DIR / \"train_per_query_results.csv\"\n",
        "train_per_query_df.to_csv(train_per_query_csv, index=False, encoding='utf-8')\n",
        "print(f\"  ‚úÖ Train per-query results: {train_per_query_csv}\")\n",
        "\n",
        "eval_per_query_df = pd.DataFrame(eval_eval_results[\"per_query_results\"])\n",
        "eval_per_query_csv = OUTPUT_DIR / \"eval_per_query_results.csv\"\n",
        "eval_per_query_df.to_csv(eval_per_query_csv, index=False, encoding='utf-8')\n",
        "print(f\"  ‚úÖ Eval per-query results: {eval_per_query_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics export\n",
        "final_metrics = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"training_samples\": len(train_dataset),\n",
        "    \"eval_samples\": len(eval_dataset),\n",
        "    \"slate_size\": SLATE_SIZE,\n",
        "    \"group_size\": GROUP_SIZE,\n",
        "    \"max_steps\": MAX_STEPS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"lora_rank\": LORA_RANK,\n",
        "    \"lora_alpha\": LORA_ALPHA,\n",
        "    \"final_loss\": trainer.state.log_history[-1].get(\"loss\", 0.0) if trainer.state.log_history else 0.0,\n",
        "    \"total_steps\": len(trainer.state.log_history) if trainer.state.log_history else 0,\n",
        "    \"training_rewards\": {\n",
        "        \"mean\": float(np.mean(training_rewards)) if training_rewards else 0.0,\n",
        "        \"std\": float(np.std(training_rewards)) if training_rewards else 0.0,\n",
        "        \"min\": float(np.min(training_rewards)) if training_rewards else 0.0,\n",
        "        \"max\": float(np.max(training_rewards)) if training_rewards else 0.0,\n",
        "        \"trend\": training_rewards\n",
        "    },\n",
        "    \"train_evaluation\": {\n",
        "        \"baseline_metrics\": train_eval_results[\"baseline_metrics\"],\n",
        "        \"policy_metrics\": train_eval_results[\"policy_metrics\"],\n",
        "        \"num_queries\": train_eval_results[\"num_queries\"],\n",
        "        \"positive_improvement_count\": train_eval_results[\"positive_improvement_count\"],\n",
        "        \"positive_improvement_ratio\": train_eval_results[\"positive_improvement_ratio\"],\n",
        "        \"parse_success_rate\": train_eval_results[\"parse_success_rate\"],\n",
        "        \"parse_success_count\": train_eval_results[\"parse_success_count\"]\n",
        "    },\n",
        "    \"eval_evaluation\": {\n",
        "        \"baseline_metrics\": eval_eval_results[\"baseline_metrics\"],\n",
        "        \"policy_metrics\": eval_eval_results[\"policy_metrics\"],\n",
        "        \"num_queries\": eval_eval_results[\"num_queries\"],\n",
        "        \"positive_improvement_count\": eval_eval_results[\"positive_improvement_count\"],\n",
        "        \"positive_improvement_ratio\": eval_eval_results[\"positive_improvement_ratio\"],\n",
        "        \"parse_success_rate\": eval_eval_results[\"parse_success_rate\"],\n",
        "        \"parse_success_count\": eval_eval_results[\"parse_success_count\"]\n",
        "    },\n",
        "    \"status\": \"completed\"\n",
        "}\n",
        "\n",
        "with open(METRICS_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"  ‚úÖ Metrics: {METRICS_FILE}\")\n",
        "print(\"\\n‚úÖ Minden artifact sikeresen mentve!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training √∂sszefoglal√≥\n",
        "\n",
        "### Technol√≥giai stack:\n",
        "- **Framework**: Unsloth + TRL GRPOTrainer\n",
        "- **Inference**: vLLM (2-3x gyorsabb generation)\n",
        "- **Model**: Qwen/Qwen3-4B-Instruct-2507 (4-bit + QLoRA)\n",
        "- **Optimaliz√°ci√≥k**: Unsloth gradient checkpointing, vLLM inference, batch=4, gen=14\n",
        "\n",
        "### Gener√°lt artifactumok (`/workspace/artifacts/grpo_policy/`):\n",
        "- LoRA adapter weights\n",
        "- `metrics.json` - ranx-alap√∫ extended metrics\n",
        "- `train_per_query_results.csv` - Train metrics (MAP, MRR, NDCG@5/10/20, Precision, Recall)\n",
        "- `eval_per_query_results.csv` - Eval metrics\n",
        "\n",
        "### Agents.md checklist:\n",
        "- ‚úÖ Unsloth + vLLM + RSLoRA\n",
        "- ‚úÖ Chunk-based slates (teljes sz√∂veg)\n",
        "- ‚úÖ GRPO training (dapo loss, batch scaling, sequence IS)\n",
        "- ‚úÖ ranx evaluation\n",
        "- ‚úÖ Hungarian status messages\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
