{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CourtRankRL GRPO Training - Chunk-Based, RTX 5090 Optimized\n",
        "\n",
        "## Agents.md Specifik√°ci√≥ (Chunk-Based)\n",
        "\n",
        "Ez a notebook a CourtRankRL GRPO alap√∫ reranking modell tan√≠t√°s√°t v√©gzi el **RTX 5090 GPU-n** (24GB VRAM).\n",
        "\n",
        "### F≈ëbb jellemz≈ëk (Chunk-Based megold√°s):\n",
        "- **Model**: Qwen/Qwen3-4B-Instruct-2507 (4-bit) + QLoRA (rank=64, alpha=128)\n",
        "- **Training**: TRL GRPOTrainer GRPO algoritmussal\n",
        "  - Loss: \"dapo\" (eliminates length bias)\n",
        "  - Reward scaling: \"batch\" (robust - PPO Lite)\n",
        "  - Importance sampling: \"sequence\" (stable - GSPO)\n",
        "- **Dataset**: 98 query (teljes), 20 chunk/slate, **TELJES chunk sz√∂veg** (~500-800 char)\n",
        "- **Slate strat√©gia**: Chunk-level retrieval (nem doc aggreg√°ci√≥!) ‚Üí legrelev√°nsabb chunk-ok\n",
        "- **Baseline**: Slate sorrendje = fusion ranking [0,1,2,...] (BM25+FAISS fusion szerint)\n",
        "- **Hardware**: Batch size 2, grad accumulation 2, 6 generations/prompt\n",
        "- **Training time**: ~45-60 perc (500 steps)\n",
        "- **Input**: training_slates.jsonl (chunk-based prepare_training_slates() kimenet)\n",
        "- **Output**: LoRA adapter weights + metrics JSON\n",
        "\n",
        "### El≈ëfelt√©telek:\n",
        "- **RTX 5090 GPU (24GB VRAM)** vagy hasonl√≥\n",
        "- HF token k√∂rnyezeti v√°ltoz√≥ban: `HUGGINGFACE_TOKEN`\n",
        "- Chunk-based slate JSONL f√°jl: `/workspace/training_slates.jsonl`\n",
        "\n",
        "### Chunk-based slate form√°tum:\n",
        "```json\n",
        "{\n",
        "  \"query_id\": \"magyar query sz√∂veg\",\n",
        "  \"slate\": [\n",
        "    {\n",
        "      \"chunk_id\": \"0302-G_20416_2019_11_0\",\n",
        "      \"doc_id\": \"0302-G_20416_2019_11\",\n",
        "      \"bm25_score\": 12.5,\n",
        "      \"faiss_score\": 0.85,\n",
        "      \"relevance\": 2,\n",
        "      \"court\": \"F≈ëv√°rosi T√∂rv√©nysz√©k\",\n",
        "      \"domain\": \"G\",\n",
        "      \"year\": \"2019\",\n",
        "      \"text\": \"TELJES chunk sz√∂veg (500-800 char) - nem preview!\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Mi√©rt chunk-based?\n",
        "- ‚úÖ **Relev√°ns kontextus**: BM25+FAISS m√°r kiv√°lasztotta a legrelev√°nsabb chunk-okat\n",
        "- ‚úÖ **Teljes sz√∂veg**: A model l√°tja, MI√âRT relev√°ns egy dokumentum\n",
        "- ‚úÖ **Jobb tanul√°s**: A model megtanulja √©rt√©kelni a val√≥di tartalmat, nem csak metaadatokat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K√∂rnyezet setup √©s csomagok telep√≠t√©se\n",
        "%pip install -q torch transformers peft trl datasets accelerate bitsandbytes\n",
        "%pip install -q numpy huggingface_hub\n",
        "\n",
        "print(\"‚úÖ Csomagok telep√≠tve\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl.trainer.grpo_trainer import GRPOTrainer\n",
        "from trl.trainer.grpo_config import GRPOConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"‚úÖ Importok bet√∂ltve\")\n",
        "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
        "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace bejelentkez√©s\n",
        "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"‚úÖ HuggingFace bejelentkez√©s sikeres\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Nincs HUGGINGFACE_TOKEN, a modell let√∂lt√©se korl√°tozott lehet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Konfigur√°ci√≥ (RTX 5090 optimaliz√°lt - agents.md szerint)\n",
        "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "\n",
        "# Dataset (agents.md: teljes 98 query, 20 chunk/slate)\n",
        "SLATE_SIZE = 20\n",
        "GROUP_SIZE = 20  # = SLATE_SIZE\n",
        "# (Chunk-based, teljes sz√∂veg - nem preview)\n",
        "\n",
        "# LoRA konfigur√°ci√≥ (agents.md: rank=64, alpha=128)\n",
        "LORA_RANK = 64\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Training konfigur√°ci√≥ (RTX 5090 optimized)\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_STEPS = 500  # ~5 epoch (98 query √ó 5)\n",
        "SAVE_STEPS = 500  # Csak final save\n",
        "EVAL_STEPS = 50\n",
        "LOGGING_STEPS = 10\n",
        "WARMUP_STEPS = 50  # 10% warmup\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # RTX 5090: 2 (vs 4)\n",
        "NUM_GENERATIONS = 6  # RTX 5090: 6 (vs 4)\n",
        "PER_DEVICE_BATCH_SIZE = 2  # RTX 5090: batch=2\n",
        "\n",
        "# GRPO Reward (agents.md szerint)\n",
        "NDCG_K = 10\n",
        "ENTROPY_BONUS = 0.01  # Exploration\n",
        "REWARD_CLIP_MIN = -1.0\n",
        "REWARD_CLIP_MAX = 1.0\n",
        "\n",
        "# Train/Eval split (agents.md: 80/20, seed 42)\n",
        "TRAIN_SPLIT = 0.8\n",
        "SEED = 42\n",
        "\n",
        "# Paths (RunPod workspace)\n",
        "BASE_PATH = Path(os.getenv(\"WORKSPACE_PATH\", \"/workspace\"))\n",
        "SLATE_FILE = BASE_PATH / \"training_slates.jsonl\"\n",
        "OUTPUT_DIR = BASE_PATH / \"artifacts\" / \"grpo_policy\"\n",
        "METRICS_FILE = OUTPUT_DIR / \"metrics.json\"\n",
        "\n",
        "print(\"üìã RTX 5090 Konfigur√°ci√≥:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Slate size: {SLATE_SIZE} doc √ó {TEXT_PREVIEW} char\")\n",
        "print(f\"  LoRA: rank={LORA_RANK}, alpha={LORA_ALPHA}\")\n",
        "print(f\"  Batch: {PER_DEVICE_BATCH_SIZE} √ó {GRADIENT_ACCUMULATION_STEPS} = {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Steps: {MAX_STEPS}, Generations: {NUM_GENERATIONS}\")\n",
        "print(f\"  Slate file: {SLATE_FILE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")\n",
        "\n",
        "# Ellen≈ërz√©s\n",
        "if not SLATE_FILE.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Slate f√°jl nem tal√°lhat√≥: {SLATE_FILE}\")\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seg√©df√ºggv√©nyek\n",
        "\n",
        "def calculate_ndcg(ranked_indices: List[int], true_relevance: List[float], k: int = 10) -> float:\n",
        "    \"\"\"\n",
        "    NDCG@k sz√°m√≠t√°s agents.md szerint.\n",
        "    Formula: DCG = sum(rel_i / log2(i + 2))\n",
        "    \"\"\"\n",
        "    if not true_relevance or not ranked_indices:\n",
        "        return 0.0\n",
        "\n",
        "    # DCG calculation\n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(ranked_indices))):\n",
        "        if i < len(true_relevance):\n",
        "            rel = true_relevance[ranked_indices[i]]\n",
        "            dcg += rel / np.log2(i + 2)\n",
        "\n",
        "    # IDCG calculation\n",
        "    sorted_rel = sorted(true_relevance, reverse=True)\n",
        "    idcg = 0.0\n",
        "    for i in range(min(k, len(sorted_rel))):\n",
        "        idcg += sorted_rel[i] / np.log2(i + 2)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "\n",
        "def calculate_entropy(ranking: List[int]) -> float:\n",
        "    \"\"\"Entropy sz√°m√≠t√°s a ranking diverzit√°s√°hoz.\"\"\"\n",
        "    if not ranking:\n",
        "        return 0.0\n",
        "\n",
        "    counts = {}\n",
        "    for idx in ranking:\n",
        "        counts[idx] = counts.get(idx, 0) + 1\n",
        "\n",
        "    probs = [count / len(ranking) for count in counts.values()]\n",
        "    entropy = -sum(p * np.log(p + 1e-10) for p in probs if p > 0)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def parse_model_ranking(completion: str, slate_size: int = SLATE_SIZE) -> List[int]:\n",
        "    \"\"\"\n",
        "    Model kimenetb≈ël ranking kinyer√©se.\n",
        "    V√°rhat√≥ form√°tum: \"1,3,2,4,0\" vagy \"1, 3, 2, 4, 0\"\n",
        "    \n",
        "    Jav√≠tott fallback (agents.md): random shuffle (nem baseline!) \n",
        "    hogy a model ne tanuljon meg baseline-t outputolni hiba eset√©n.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        numbers = [int(x.strip()) for x in completion.split(\",\") if x.strip().isdigit()]\n",
        "        # Csak valid indexeket tartunk meg\n",
        "        valid_numbers = [n for n in numbers if 0 <= n < slate_size]\n",
        "        \n",
        "        if len(valid_numbers) >= slate_size // 2:\n",
        "            # Ha legal√°bb fele valid, haszn√°ljuk\n",
        "            return valid_numbers[:slate_size]\n",
        "        else:\n",
        "            # Ha t√∫l kev√©s valid sz√°m: random shuffle (b√ºntet√©shez vezet)\n",
        "            indices = list(range(slate_size))\n",
        "            random.shuffle(indices)\n",
        "            return indices\n",
        "    except:\n",
        "        # Parse error: random shuffle (b√ºntet√©shez vezet - agents.md)\n",
        "        indices = list(range(slate_size))\n",
        "        random.shuffle(indices)\n",
        "        return indices\n",
        "\n",
        "\n",
        "print(\"‚úÖ Seg√©df√ºggv√©nyek defini√°lva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slate adatok bet√∂lt√©se\n",
        "print(f\"üìÇ Slate adatok bet√∂lt√©se: {SLATE_FILE}\")\n",
        "\n",
        "slates_data = []\n",
        "with open(SLATE_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line_num, line in enumerate(f, 1):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            slate = json.loads(line)\n",
        "            slates_data.append(slate)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è JSON hiba a {line_num}. sorban: {e}\")\n",
        "            continue\n",
        "\n",
        "if not slates_data:\n",
        "    raise ValueError(\"‚ùå Nincs bet√∂lthet≈ë slate adat!\")\n",
        "\n",
        "print(f\"‚úÖ Bet√∂ltve: {len(slates_data)} slate\")\n",
        "\n",
        "# Slate form√°tum valid√°ci√≥\n",
        "sample = slates_data[0]\n",
        "print(f\"\\nüìã Minta slate strukt√∫ra:\")\n",
        "print(f\"  Query ID: {sample['query_id'][:50]}...\")\n",
        "print(f\"  Slate elemek: {len(sample['slate'])}\")\n",
        "print(f\"  Minta elem kulcsok: {list(sample['slate'][0].keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed Learning-to-Rank Prompt Template (Chunk-Based, Full Context)\n",
        "\n",
        "def create_training_prompt(query_id: str, slate: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Comprehensive learning-to-rank prompt for GRPO training.\n",
        "\n",
        "    This prompt is designed to maximize GRPO learning effectiveness by providing:\n",
        "    - Clear task definition and objectives\n",
        "    - Detailed relevance criteria explanation\n",
        "    - Step-by-step reasoning instructions\n",
        "    - Full document context for accurate assessment\n",
        "    - Structured format for consistent parsing\n",
        "\n",
        "    Goal: Optimize nDCG@10 through better document understanding and ranking strategy learning.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for i, candidate in enumerate(slate):\n",
        "        # Extract all available metadata for comprehensive assessment\n",
        "        court = candidate.get('court', 'Unknown Court')\n",
        "        domain = candidate.get('domain', 'Unknown')\n",
        "        year = candidate.get('year', 'Unknown')\n",
        "        bm25 = candidate.get('bm25_score', 0.0)\n",
        "        faiss = candidate.get('faiss_score', 0.0)\n",
        "        doc_id = candidate.get('doc_id', f'unknown_doc_{i}')\n",
        "        chunk_id = candidate.get('chunk_id', f'unknown_chunk_{i}')\n",
        "\n",
        "        # Full chunk text for complete context understanding\n",
        "        text = candidate.get('text', 'No text available')\n",
        "\n",
        "        lines.append(\n",
        "            f\"[{i}] DOCUMENT: {doc_id} | CHUNK: {chunk_id}\\n\"\n",
        "            f\"    COURT: {court} | DOMAIN: {domain} | YEAR: {year}\\n\"\n",
        "            f\"    RETRIEVAL_SCORES: BM25={bm25:.3f}, Dense={faiss:.3f}\\n\"\n",
        "            f\"    CONTENT: {text}\\n\"\n",
        "        )\n",
        "\n",
        "    prompt = f\"\"\"You are an expert legal information retrieval system designed to rank Hungarian court decisions for maximum relevance to user queries.\n",
        "\n",
        "## TASK DEFINITION\n",
        "Your objective is to learn optimal document ranking through reinforcement learning. Given a query and candidate documents, you must produce a ranking that maximizes nDCG@10, focusing on true relevance over superficial features.\n",
        "\n",
        "## PROJECT CONTEXT\n",
        "This is a CourtRankRL system for Hungarian judicial documents. Documents come from various courts (F≈ëv√°rosi T√∂rv√©nysz√©k, etc.), legal domains ( criminal, civil), and years. Each document represents a real court decision that may contain relevant legal precedents, arguments, or conclusions.\n",
        "\n",
        "## RELEVANCE ASSESSMENT CRITERIA\n",
        "When evaluating document relevance to the query, consider:\n",
        "\n",
        "1. **Semantic Match**: How well does the document content address the specific legal concepts, situations, or outcomes mentioned in the query?\n",
        "2. **Legal Domain Alignment**: Does the document's court/domain match the legal area of the query?\n",
        "3. **Temporal Relevance**: Is the document from a relevant time period that would contain applicable precedents?\n",
        "4. **Precedent Value**: Does the document contain similar fact patterns, legal arguments, or conclusions that would be useful?\n",
        "5. **Authority Level**: Higher courts and more recent decisions generally have higher precedential value.\n",
        "\n",
        "## RANKING STRATEGY\n",
        "Follow these steps for optimal ranking:\n",
        "\n",
        "1. **Analyze Query Intent**: Identify the core legal concepts, parties involved, and desired outcomes.\n",
        "2. **Content Evaluation**: Read each document's full text to understand its legal context and relevance.\n",
        "3. **Multi-factor Scoring**: Combine semantic relevance, domain match, and precedential value.\n",
        "4. **Confidence Assessment**: Prioritize documents where relevance is clear and direct.\n",
        "5. **Diversity Consideration**: Include documents from different courts/periods if they provide complementary legal insights.\n",
        "\n",
        "## CANDIDATE DOCUMENTS\n",
        "Below are {len(slate)} candidate document chunks retrieved by hybrid BM25+dense retrieval. Each contains full text content for complete relevance assessment.\n",
        "\n",
        "QUERY: \"{query_id}\"\n",
        "\n",
        "CANDIDATES:\n",
        "{chr(10).join(lines)}\n",
        "\n",
        "## RESPONSE FORMAT\n",
        "Provide your ranking as a comma-separated list of indices in descending relevance order.\n",
        "Format: \"index1,index2,index3,...\" (e.g., \"2,0,15,8,3,11,1,5,9,12\")\n",
        "\n",
        "Guidelines for response:\n",
        "- Include ALL {len(slate)} documents in your ranking\n",
        "- Order by decreasing relevance (most relevant first)\n",
        "- Use only valid indices from [0, {len(slate)-1}]\n",
        "- Separate indices with commas, no spaces\n",
        "- Base your ranking on actual content analysis, not retrieval scores\n",
        "\n",
        "Your ranking will be evaluated using nDCG@10. Focus on identifying truly relevant documents that best match the query's legal intent and requirements.\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# Test enhanced prompt\n",
        "test_prompt = create_training_prompt(slates_data[0][\"query_id\"], slates_data[0][\"slate\"])\n",
        "print(\"üìù Enhanced learning-to-rank prompt sample:\")\n",
        "print(\"=\"*80)\n",
        "print(test_prompt[:1500])  # First 1500 chars for preview\n",
        "print(\"\\n... (truncated)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä Prompt Statistics:\")\n",
        "print(f\"  Total length: {len(test_prompt)} characters\")\n",
        "print(f\"  Estimated tokens: ~{len(test_prompt.split())*1.2:.0f}\")\n",
        "print(f\"  Average chunk length: {sum(len(c.get('text', '')) for c in slates_data[0]['slate']) / len(slates_data[0]['slate']):.0f} characters\")\n",
        "print(f\"  Number of candidates: {len(slates_data[0]['slate'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset el≈ëk√©sz√≠t√©se TRL GRPOTrainer-hez (chunk-based, shuffled split)\n",
        "# Agents.md: TRL-kompatibilis data passing via global slate lookup dict\n",
        "\n",
        "training_examples = []\n",
        "slate_lookup = {}  # Global dict: query_id -> slate_data (reward function-h√∂z)\n",
        "\n",
        "for slate_data in slates_data:\n",
        "    query_id = slate_data[\"query_id\"]\n",
        "    prompt = create_training_prompt(query_id, slate_data[\"slate\"])\n",
        "    \n",
        "    # Dataset csak a prompt-ot tartalmazza (TRL best practice)\n",
        "    training_examples.append({\n",
        "        \"prompt\": prompt\n",
        "    })\n",
        "    \n",
        "    # Slate metadata k√ºl√∂n t√°rol√°sa (reward function-h√∂z)\n",
        "    slate_lookup[query_id] = slate_data[\"slate\"]\n",
        "\n",
        "# Full dataset\n",
        "full_dataset = Dataset.from_list(training_examples)\n",
        "\n",
        "# Train/eval split (Agents.md: 80/20, SHUFFLED, seed=42)\n",
        "shuffled_indices = list(range(len(full_dataset)))\n",
        "random.Random(SEED).shuffle(shuffled_indices)\n",
        "\n",
        "train_size = int(len(full_dataset) * TRAIN_SPLIT)\n",
        "train_indices = shuffled_indices[:train_size]\n",
        "eval_indices = shuffled_indices[train_size:]\n",
        "\n",
        "train_dataset = full_dataset.select(train_indices)\n",
        "eval_dataset = full_dataset.select(eval_indices)\n",
        "\n",
        "print(f\"‚úÖ Dataset l√©trehozva (shuffled split):\")\n",
        "print(f\"  Training: {len(train_dataset)} query (80%)\")\n",
        "print(f\"  Evaluation: {len(eval_dataset)} query (20%)\")\n",
        "print(f\"  Slate lookup: {len(slate_lookup)} entry (global dict)\")\n",
        "print(f\"  Slate size: {SLATE_SIZE} chunk/query\")\n",
        "print(f\"  Random seed: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model √©s tokenizer inicializ√°l√°sa\n",
        "print(f\"üîÑ Model bet√∂lt√©se: {MODEL_NAME}\")\n",
        "\n",
        "# 4-bit quantization konfigur√°ci√≥ (agents.md szerint)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = SentenceTransformer(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"‚úÖ Tokenizer bet√∂ltve\")\n",
        "\n",
        "# Model\n",
        "model = SentenceTransformer(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model bet√∂ltve (4-bit quantized)\")\n",
        "\n",
        "# LoRA konfigur√°ci√≥ (RTX 5090 optimaliz√°lt - agents.md: rank=64, 7 modules)\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # 7 target modules: full Qwen3 coverage (agents.md RTX 5090 spec)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"‚úÖ LoRA adapterek alkalmazva:\")\n",
        "print(f\"  Rank: {LORA_RANK}, Alpha: {LORA_ALPHA}\")\n",
        "print(f\"  Target modules: 7 (full coverage)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Reward Function (TRL-kompatibilis, chunk-based, jav√≠tott)\n",
        "\n",
        "def reward_function(completions, prompts, **kwargs):\n",
        "    \"\"\"\n",
        "    TRL-kompatibilis GRPO reward function (chunk-based, jav√≠tott).\n",
        "    \n",
        "    Agents.md szerinti jav√≠t√°sok:\n",
        "    - Regex-based query_id parsing (robust)\n",
        "    - Baseline order = slate fusion ranking [0,1,2,...]\n",
        "    - Negative penalty for parse failures (not zero)\n",
        "    - nDCG@10 difference (core GRPO objective)\n",
        "    - Entropy bonus (exploration)\n",
        "    - Reward clipping (stability)\n",
        "    \n",
        "    Args:\n",
        "        completions: Model √°ltal gener√°lt output lista\n",
        "        prompts: Input prompt lista (query_id extraction-h√∂z)\n",
        "        **kwargs: Tov√°bbi TRL argumentumok\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    \n",
        "    for completion, prompt in zip(completions, prompts):\n",
        "        try:\n",
        "            # Query ID kinyer√©se REGEX-szel (robust - agents.md)\n",
        "            match = re.search(r'QUERY:\\s*\"([^\"]+)\"', prompt)\n",
        "            \n",
        "            if not match:\n",
        "                # Parse failure: negative penalty (agents.md)\n",
        "                rewards.append(-0.5)\n",
        "                continue\n",
        "                \n",
        "            query_id = match.group(1)\n",
        "            \n",
        "            if query_id not in slate_lookup:\n",
        "                rewards.append(-0.5)\n",
        "                continue\n",
        "            \n",
        "            # Slate metadata lookup (global dict)\n",
        "            slate = slate_lookup[query_id]\n",
        "            relevance = [doc.get('relevance', 0) for doc in slate]\n",
        "            \n",
        "            # BASELINE ORDER: slate m√°r fusion szerint rendezett (agents.md)\n",
        "            # A slate-ben l√©v≈ë sorrend [0,1,2,...] = fusion baseline!\n",
        "            baseline = list(range(len(slate)))\n",
        "            \n",
        "            # Parse model ranking\n",
        "            predicted = parse_model_ranking(completion, len(slate))\n",
        "            \n",
        "            # GRPO core: nDCG@10 difference\n",
        "            ndcg_baseline = calculate_ndcg(baseline, relevance, k=NDCG_K)\n",
        "            ndcg_policy = calculate_ndcg(predicted, relevance, k=NDCG_K)\n",
        "            reward = ndcg_policy - ndcg_baseline\n",
        "            \n",
        "            # Entropy bonus (exploration - agents.md: 0.01 weight)\n",
        "            if len(predicted) > 1:\n",
        "                unique_ratio = len(set(predicted)) / len(predicted)\n",
        "                reward += ENTROPY_BONUS * unique_ratio\n",
        "            \n",
        "            # Reward clipping (stability - agents.md)\n",
        "            reward = max(REWARD_CLIP_MIN, min(REWARD_CLIP_MAX, reward))\n",
        "            rewards.append(reward)\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Unexpected error: negative penalty\n",
        "            rewards.append(-0.5)\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "\n",
        "print(\"‚úÖ GRPO Reward function defini√°lva (chunk-based, jav√≠tott)\")\n",
        "print(f\"  Query parsing: regex-based (robust)\")\n",
        "print(f\"  Baseline: slate fusion order [0,1,2,...]\")\n",
        "print(f\"  Core: nDCG@{NDCG_K} difference\")\n",
        "print(f\"  Entropy bonus: {ENTROPY_BONUS}\")\n",
        "print(f\"  Parse failure penalty: -0.5\")\n",
        "print(f\"  Clipping: [{REWARD_CLIP_MIN}, {REWARD_CLIP_MAX}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Trainer konfigur√°ci√≥ (RTX 5090 optimaliz√°lt + TRL best practices)\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    \n",
        "    # === GRPO Algorithm (TRL best practices) ===\n",
        "    loss_type=\"dapo\",  # Eliminates length bias (TRL default)\n",
        "    scale_rewards=\"batch\",  # Robust (PPO Lite paper)\n",
        "    mask_truncated_completions=True,  # Stability (DAPO paper)\n",
        "    importance_sampling_level=\"sequence\",  # Stable (GSPO paper)\n",
        "    \n",
        "    # === GRPO Core ===\n",
        "    epsilon=0.2,  # GRPO clipping\n",
        "    kl_coef=0.0,  # Disabled (agents.md)\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    group_size=GROUP_SIZE,\n",
        "    max_completion_length=128,\n",
        "    \n",
        "    # === Training (RTX 5090 optimized) ===\n",
        "    max_steps=MAX_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,  # 2 (RTX 5090)\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # 2\n",
        "    \n",
        "    # === Optimization ===\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Modern PyTorch\n",
        "    max_grad_norm=1.0,\n",
        "    \n",
        "    # === Logging ===\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_first_step=True,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    log_completions=True,  # Debug-hoz hasznos\n",
        "    \n",
        "    # === Other ===\n",
        "    dataloader_num_workers=2,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GRPO Trainer konfigur√°ci√≥ (TRL best practices):\")\n",
        "print(f\"  Loss: {grpo_config.loss_type} (eliminates length bias)\")\n",
        "print(f\"  Reward scaling: {grpo_config.scale_rewards} (robust)\")\n",
        "print(f\"  Group size: {grpo_config.group_size}\")\n",
        "print(f\"  Batch: {grpo_config.per_device_train_batch_size} √ó {grpo_config.gradient_accumulation_steps} = {grpo_config.per_device_train_batch_size * grpo_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Steps: {grpo_config.max_steps}, Generations: {grpo_config.num_generations}\")\n",
        "print(f\"  KL coef: {grpo_config.kl_coef} (disabled)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Trainer inicializ√°l√°sa (train/eval split)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=reward_function,\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset,  # 80% (agents.md)\n",
        "    eval_dataset=eval_dataset,     # 20% (agents.md)\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GRPO Trainer inicializ√°lva (train/eval split)\")\n",
        "print(f\"\\nüìä Training inform√°ci√≥k:\")\n",
        "print(f\"  Training queries: {len(train_dataset)}\")\n",
        "print(f\"  Eval queries: {len(eval_dataset)}\")\n",
        "print(f\"  Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Total training steps: {MAX_STEPS}\")\n",
        "print(f\"  GPU mem√≥ria: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training ind√≠t√°sa\n",
        "print(\"\\nüöÄ GRPO TRAINING IND√çT√ÅSA\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ Training sikeresen befejezve!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training hiba: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Artifactumok ment√©se\n",
        "print(\"\\nüíæ Artifactumok ment√©se...\")\n",
        "\n",
        "# LoRA adapter ment√©se\n",
        "trainer.save_model(str(OUTPUT_DIR))\n",
        "print(f\"  ‚úÖ LoRA adapter: {OUTPUT_DIR}\")\n",
        "\n",
        "# Tokenizer ment√©se\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "print(f\"  ‚úÖ Tokenizer: {OUTPUT_DIR}\")\n",
        "\n",
        "# Metrics ment√©se\n",
        "final_metrics = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"training_samples\": len(dataset),\n",
        "    \"slate_size\": SLATE_SIZE,\n",
        "    \"group_size\": GROUP_SIZE,\n",
        "    \"max_steps\": MAX_STEPS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"lora_rank\": LORA_RANK,\n",
        "    \"lora_alpha\": LORA_ALPHA,\n",
        "    \"final_loss\": trainer.state.log_history[-1].get(\"loss\", 0.0) if trainer.state.log_history else 0.0,\n",
        "    \"total_steps\": len(trainer.state.log_history) if trainer.state.log_history else 0,\n",
        "    \"status\": \"completed\"\n",
        "}\n",
        "\n",
        "with open(METRICS_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"  ‚úÖ Metrics: {METRICS_FILE}\")\n",
        "\n",
        "print(\"\\n‚úÖ Minden artifact sikeresen mentve!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training √∂sszefoglal√≥\n",
        "\n",
        "### Gener√°lt artifactumok:\n",
        "\n",
        "A training befejezt√©vel a k√∂vetkez≈ë f√°jlok ker√ºltek l√©trehoz√°sra a `/workspace/artifacts/grpo_policy/` k√∂nyvt√°rban:\n",
        "\n",
        "- `adapter_model.bin` - LoRA adapter weights\n",
        "- `adapter_config.json` - LoRA konfigur√°ci√≥\n",
        "- `tokenizer_config.json` - Tokenizer konfigur√°ci√≥\n",
        "- `tokenizer.json` - Tokenizer weights\n",
        "- `metrics.json` - Training metrik√°k √©s konfigur√°ci√≥\n",
        "\n",
        "### K√∂vetkez≈ë l√©p√©sek:\n",
        "\n",
        "1. **Artifactumok let√∂lt√©se:**\n",
        "   ```bash\n",
        "   # RunPod termin√°lb√≥l\n",
        "   cd /workspace/artifacts/grpo_policy\n",
        "   ls -lh\n",
        "   ```\n",
        "\n",
        "2. **Metrics elemz√©se:**\n",
        "   ```bash\n",
        "   cat metrics.json\n",
        "   ```\n",
        "\n",
        "3. **Helyi elemz√©s:**\n",
        "   - T√∂ltsd le a `metrics.json` f√°jlt a lok√°lis `data/models/grpo_policy/` k√∂nyvt√°rba\n",
        "   - Az adapter weights-eket cloud-on kell hagyni (lok√°lis inference nem t√°mogatott)\n",
        "\n",
        "### Agents.md specifik√°ci√≥ szerint:\n",
        "\n",
        "- ‚úÖ Cloud-only training (RunPod)\n",
        "- ‚úÖ Qwen/Qwen3-4B-Instruct-2507 + QLoRA\n",
        "- ‚úÖ Group size = slate length (20)\n",
        "- ‚úÖ NDCG@10 alap√∫ reward\n",
        "- ‚úÖ Hungarian status messages\n",
        "- ‚úÖ Artifact export `/workspace/artifacts/grpo_policy/`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
