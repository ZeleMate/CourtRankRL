{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CourtRankRL GRPO Training - Chunk-Based, A100 Memory Efficient\n",
    "\n",
    "## Agents.md Specifik√°ci√≥ (Chunk-Based)\n",
    "\n",
    "Ez a notebook a CourtRankRL GRPO alap√∫ reranking modell tan√≠t√°s√°t v√©gzi el **A100 GPU-n** (80GB VRAM, memory-constrained).\n",
    "\n",
    "### F≈ëbb jellemz≈ëk (Chunk-Based megold√°s):\n",
    "- **Model**: Qwen/Qwen3-4B-Instruct-2507 (4-bit) + QLoRA (rank=32, alpha=64)\n",
    "- **Training**: TRL GRPOTrainer GRPO algoritmussal\n",
    "  - Loss: \"dapo\" (eliminates length bias)\n",
    "  - Reward scaling: \"batch\" (robust - PPO Lite)\n",
    "  - Importance sampling: \"sequence\" (stable - GSPO)\n",
    "- **Dataset**: 98 query (teljes), 20 chunk/slate, **TELJES chunk sz√∂veg** (~500-800 char)\n",
    "- **Slate strat√©gia**: Chunk-level retrieval (nem doc aggreg√°ci√≥!) ‚Üí legrelev√°nsabb chunk-ok\n",
    "- **Baseline**: Slate sorrendje = fusion ranking [0,1,2,...] (BM25+FAISS fusion szerint)\n",
    "- **Hardware**: Batch size 2, grad accumulation 6, 6 generations/prompt (MEMORY EFFICIENT)\n",
    "- **Training time**: ~25-35 perc (600 steps, vLLM-mel, cs√∂kkentett gen miatt)\n",
    "\n",
    "### Mi√©rt chunk-based?\n",
    "- ‚úÖ **Relev√°ns kontextus**: BM25+FAISS m√°r kiv√°lasztotta a legrelev√°nsabb chunk-okat\n",
    "- ‚úÖ **Teljes sz√∂veg**: A model l√°tja, MI√âRT relev√°ns egy dokumentum\n",
    "- ‚úÖ **Jobb tanul√°s**: A model megtanulja √©rt√©kelni a val√≥di tartalmat, nem csak metaadatokat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K√∂rnyezet setup √©s csomagok telep√≠t√©se\n",
    "# KRITIKUS: Kompatibilis verzi√≥k telep√≠t√©se (Unsloth + TRL + dependencies)\n",
    "\n",
    "# 1. Core dependencies EL≈êSZ√ñR (stabil verzi√≥k)\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install -q transformers datasets huggingface_hub\n",
    "%pip install -q numpy scipy scikit-learn\n",
    "\n",
    "# 2. Accelerate - FIX√ÅLT VERZI√ì az is_fsdp2 kompatibilit√°s miatt\n",
    "# CRITICAL: accelerate>=0.30.0 sz√ºks√©ges az is_fsdp2 attrib√∫tumhoz, de kompatibilisnek kell lennie Unsloth-tal\n",
    "%pip install -q \"accelerate>=0.30.0\" --upgrade\n",
    "\n",
    "# 3. LoRA √©s kvantiz√°ci√≥\n",
    "%pip install -q peft bitsandbytes\n",
    "\n",
    "# 4. TRL (legfrissebb stabil verzi√≥ - GRPO t√°mogat√°ssal)\n",
    "%pip install -q --upgrade trl\n",
    "\n",
    "# 5. Unsloth telep√≠t√©se UTOLJ√ÅRA (hogy a TRL-hez igazodjon)\n",
    "%pip install -q --upgrade unsloth diffusers\n",
    "\n",
    "# 6. vLLM (opcion√°lis, de aj√°nlott fast_inference-hez)\n",
    "%pip install -q vllm\n",
    "\n",
    "# 7. Egy√©b f√ºgg≈ës√©gek\n",
    "%pip install -q --upgrade typing_extensions ranx\n",
    "\n",
    "print(\"‚úÖ Csomagok telep√≠tve (kompatibilis verzi√≥k)\")\n",
    "print(\"üì¶ Telep√≠tett verzi√≥k ellen≈ërz√©se:\")\n",
    "import unsloth, torch, transformers, trl, accelerate\n",
    "print(f\"  - PyTorch: {torch.__version__}\")\n",
    "print(f\"  - Transformers: {transformers.__version__}\")\n",
    "print(f\"  - Accelerate: {accelerate.__version__} (CRITICAL: >=0.30.0 az is_fsdp2 t√°mogat√°shoz)\")\n",
    "print(f\"  - TRL: {trl.__version__}\")\n",
    "print(f\"  - Unsloth: {unsloth.__version__}\")\n",
    "\n",
    "# FONTOS FIGYELMEZTET√âS a verzi√≥r√≥l\n",
    "if tuple(map(int, accelerate.__version__.split('.')[:2])) < (0, 30):\n",
    "    print(f\"‚ö†Ô∏è  FIGYELEM: Accelerate verzi√≥ ({accelerate.__version__}) < 0.30.0\")\n",
    "    print(f\"   A monkey patch fog futni a training sor√°n az is_fsdp2 kompatibilit√°s√©rt\")\n",
    "    print(f\"   Aj√°nlott: pip install accelerate>=0.30.0 --upgrade && RESTART KERNEL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importok\n",
    "import os, warnings\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"      # hivatalos flag a dynamo kikapcsol√°s√°ra\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # √°rtalmatlan, de seg√≠t fallbackben\n",
    "# (opcion√°lis, ha Unsloth figyeli)\n",
    "os.environ[\"UNSLOTH_COMPILE\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*An output with one or more elements was resized.*\")\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl.trainer.grpo_trainer import GRPOTrainer\n",
    "from trl.trainer.grpo_config import GRPOConfig\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ranx import Qrels, Run, evaluate\n",
    "\n",
    "print(\"‚úÖ Importok bet√∂ltve (Unsloth + TRL + sklearn + scipy + ranx)\")\n",
    "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
    "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importok\n",
    "import os, warnings\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"      # hivatalos flag a dynamo kikapcsol√°s√°ra\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # √°rtalmatlan, de seg√≠t fallbackben\n",
    "# (opcion√°lis, ha Unsloth figyeli)\n",
    "os.environ[\"UNSLOTH_COMPILE\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*An output with one or more elements was resized.*\")\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl.trainer.grpo_trainer import GRPOTrainer\n",
    "from trl.trainer.grpo_config import GRPOConfig\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ranx import Qrels, Run, evaluate\n",
    "\n",
    "print(\"‚úÖ Importok bet√∂ltve (Unsloth + TRL + sklearn + scipy + ranx)\")\n",
    "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
    "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigur√°ci√≥ - MEMORY EFFICIENT (A100 80GB) + CURRICULUM LEARNING\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Instruct-2507\"\n",
    "SLATE_SIZE = 20\n",
    "\n",
    "# Curriculum Learning konfigur√°ci√≥\n",
    "CURRICULUM_ENABLED = True  # Enged√©lyezve - LEGNAGYOBB JAVUL√ÅSI POTENCI√ÅL\n",
    "CURRICULUM_STRATEGY = \"filtered_dataset\"  # \"filtered_dataset\" vagy \"weighted_sampling\"\n",
    "# weighted_sampling: k√∂nnyebb p√©ld√°kat gyakrabban v√°lasztjuk ki\n",
    "# Ez biztos√≠tja, hogy a modell fokozatosan megtanulja j√≥l rangsorolni\n",
    "GROUP_SIZE = 6\n",
    "LORA_RANK = 32  # 64‚Üí32: cs√∂kkentett rank a mem√≥ria miatt\n",
    "LORA_ALPHA = LORA_RANK  # alpha k√∂veti a rank-t\n",
    "LORA_DROPOUT = 0.05\n",
    "MAX_SEQ_LENGTH = 9216  # 8192‚Üí12288: n√∂velt √©rt√©k a hosszabb promptokhoz (9600 token + marg√≥)\n",
    "GPU_MEMORY_UTILIZATION = 0.6  # 0.88‚Üí0.5 cs√∂kkentve a rendelkez√©sre √°ll√≥ szabad mem√≥ria miatt\n",
    "USE_GRADIENT_CHECKPOINTING = \"unsloth\"\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_STEPS = 300\n",
    "SAVE_STEPS = 300  # Save more frequently for checkpoint recovery\n",
    "EVAL_STEPS = 50\n",
    "LOGGING_STEPS = 10\n",
    "WARMUP_STEPS = 50\n",
    "GRADIENT_ACCUMULATION_STEPS = 3  # 3‚Üí6: nagyobb accumulation, kisebb batch\n",
    "NUM_GENERATIONS = 6  # 14‚Üí6: cs√∂kkentett gener√°ci√≥k\n",
    "PER_DEVICE_BATCH_SIZE = 1  # 4‚Üí2: FEL√âRE cs√∂kkentve\n",
    "# KRITIKUS: generation_batch_size-nak oszthat√≥nak KELL lennie:\n",
    "#   1. NUM_GENERATIONS-szel (6)\n",
    "#   2. PER_DEVICE_BATCH_SIZE-zal (2)\n",
    "# Legkisebb k√∂z√∂s t√∂bbsz√∂r√∂s (LCM): 6\n",
    "GENERATION_BATCH_SIZE = 6  # 6√∑6=1, 6√∑2=3 ‚úì\n",
    "OPTIMIZER_NAME = \"paged_adamw_8bit\"\n",
    "LR_SCHEDULER_TYPE = \"linear\"  # Enhanced: cosine ‚Üí linear (more stable final convergence)\n",
    "\n",
    "NDCG_K = 10\n",
    "ENTROPY_BONUS = 0.01\n",
    "# Reward clipping range (updated in reward function)\n",
    "REWARD_CLIP_MIN = -1.0  # Will be updated in enhanced reward function\n",
    "REWARD_CLIP_MAX = 2.0   # Enhanced: wider range for positive improvements\n",
    "TRAIN_SPLIT = 0.8\n",
    "SEED = 42\n",
    "\n",
    "BASE_PATH = Path(os.getenv(\"WORKSPACE_PATH\", \"/workspace\"))\n",
    "\n",
    "BASELINE_METRICS_PATH = BASE_PATH / \"baseline_query_metrics.csv\"\n",
    "SLATE_FILE = BASE_PATH / \"training_slates.jsonl\"\n",
    "OUTPUT_DIR = BASE_PATH / \"artifacts\" / \"grpo_policy\"\n",
    "METRICS_FILE = OUTPUT_DIR / \"metrics.json\"\n",
    "\n",
    "print(\"üìã A100 80GB - Memory Efficient Konfigur√°ci√≥:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA Rank: {LORA_RANK} (cs√∂kkentve mem√≥ria miatt)\")\n",
    "print(f\"  GPU Memory Utilization: {GPU_MEMORY_UTILIZATION} (0.5 = 50%)\")\n",
    "print(f\"  Batch: {PER_DEVICE_BATCH_SIZE} √ó {GRADIENT_ACCUMULATION_STEPS} = {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} (effective)\")\n",
    "print(f\"  Generations: {NUM_GENERATIONS}, Generation batch: {GENERATION_BATCH_SIZE}\")\n",
    "print(f\"  Steps per generation: {GENERATION_BATCH_SIZE // PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"  Steps: {MAX_STEPS}\")\n",
    "print(f\"  ‚ö†Ô∏è  Mem√≥ria optimaliz√°lt be√°ll√≠t√°sok akt√≠vak!\")\n",
    "\n",
    "if not SLATE_FILE.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Slate f√°jl nem tal√°lhat√≥: {SLATE_FILE}\")\n",
    "\n",
    "if not BASELINE_METRICS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Baseline metrik√°k f√°jl nem tal√°lhat√≥: {BASELINE_METRICS_PATH}\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seg√©df√ºggv√©nyek\n",
    "def calculate_ndcg(ranked_indices: List[int], true_relevance: List[float], k: int = 10) -> float:\n",
    "    if not true_relevance or not ranked_indices or max(true_relevance) == 0:\n",
    "        return 0.0\n",
    "    y_true = np.array(true_relevance)\n",
    "    max_score = len(ranked_indices)\n",
    "    y_score = np.zeros_like(y_true, dtype=float)\n",
    "    for i, idx in enumerate(ranked_indices[:k]):\n",
    "        if idx < len(y_true):\n",
    "            y_score[idx] = max_score - i\n",
    "    if np.sum(y_score) == 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(ndcg_score(y_true.reshape(1, -1), y_score.reshape(1, -1), k=k))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def parse_model_ranking(completion: str, slate_size: int = SLATE_SIZE) -> List[int]:\n",
    "    \"\"\"\n",
    "    Megenged≈ëbb parsing - kev√©sb√© szigor√∫ form√°tum k√∂vetelm√©ny.\n",
    "    Ha nincs el√©g valid index, kieg√©sz√≠tj√ºk a hi√°nyz√≥kat a baseline order szerint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pr√≥b√°lunk sz√°mokat kinyerni k√ºl√∂nb√∂z≈ë m√≥don\n",
    "        numbers = []\n",
    "        \n",
    "        # 1. Vessz≈ëvel elv√°lasztott sz√°mok\n",
    "        parts = completion.split(\",\")\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            # K√©rj√ºk el minden sz√°mot (ak√°r t√∂bb sz√°m egy cell√°ban)\n",
    "            for word in part.split():\n",
    "                word = word.strip().rstrip(\".,;\")\n",
    "                if word.isdigit():\n",
    "                    num = int(word)\n",
    "                    if 0 <= num < slate_size:\n",
    "                        numbers.append(num)\n",
    "        \n",
    "        # 2. Ha nincs el√©g, pr√≥b√°lunk sz√≥k√∂zzel elv√°lasztott form√°tumot\n",
    "        if len(numbers) < slate_size // 3:\n",
    "            for word in completion.split():\n",
    "                word = word.strip().rstrip(\".,;\")\n",
    "                if word.isdigit():\n",
    "                    num = int(word)\n",
    "                    if 0 <= num < slate_size and num not in numbers:\n",
    "                        numbers.append(num)\n",
    "        \n",
    "        # 3. Valid sz√°mok gy≈±jt√©se\n",
    "        valid_numbers = []\n",
    "        seen = set()\n",
    "        for n in numbers:\n",
    "            if n not in seen:\n",
    "                valid_numbers.append(n)\n",
    "                seen.add(n)\n",
    "        \n",
    "        # 4. Ha van el√©g valid sz√°m (legal√°bb 1/3-a a slate-nek), haszn√°ljuk\n",
    "        if len(valid_numbers) >= max(1, slate_size // 3):\n",
    "            # Kieg√©sz√≠tj√ºk a hi√°nyz√≥ indexeket a baseline order szerint\n",
    "            missing_indices = [i for i in range(slate_size) if i not in valid_numbers]\n",
    "            # A hi√°nyz√≥kat a v√©g√©re tesz√ºk (nem zavarjuk meg a modell rankingj√©t)\n",
    "            result = valid_numbers + missing_indices\n",
    "            return result[:slate_size]\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: baseline order (nem random shuffle - √≠gy jobb mint teljesen random)\n",
    "    return list(range(slate_size))\n",
    "\n",
    "def create_training_prompt(query_id: str, slate: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced prompt generation for GRPO training with explicit relevance labels and few-shot examples.\n",
    "    The prompt uses explicit instructions in ENGLISH to help the model learn proper relevance-based ranking.\n",
    "    Only the data content (queries, document texts, metadata) remains in HUNGARIAN as it comes from the dataset.\n",
    "    \n",
    "    Improvements:\n",
    "    1. Explicit relevance labels (0/1/2) for each document\n",
    "    2. Few-shot example ranking output\n",
    "    3. More detailed relevance criteria\n",
    "    4. Clearer output format specification\n",
    "    \"\"\"\n",
    "    prompt = f'''# Legal Document Relevance Assessment and Ranking\n",
    "\n",
    "## TASK\n",
    "You are helping to rank Hungarian court decision documents by their relevance to a search query.\n",
    "\n",
    "## SEARCH QUERY\n",
    "\"{query_id}\"\n",
    "\n",
    "## WHAT TO CONSIDER\n",
    "\n",
    "When ranking documents, think about:\n",
    "- How well the document topic matches the query\n",
    "- Whether the content directly addresses what the query is asking\n",
    "- The legal domain and court type (may or may not be relevant)\n",
    "- The relevance labels (0=not relevant, 1=relevant, 2=highly relevant) - these can guide you\n",
    "- BM25 and FAISS scores show some similarity signals, but content is most important\n",
    "\n",
    "## RANKING INSTRUCTIONS\n",
    "\n",
    "You need to rank the {len(slate)} documents by their relevance to the search query.\n",
    "\n",
    "'''\n",
    "    \n",
    "    separator_line = \"\\u2500\" * 20\n",
    "    \n",
    "    # Enhanced document presentation with EXPLICIT RELEVANCE LABELS\n",
    "    for idx, doc in enumerate(slate):\n",
    "        chunk_text = doc.get('text', '')[:800]\n",
    "        bm25 = doc.get('bm25_score', 0)\n",
    "        faiss = doc.get('faiss_score', 0)\n",
    "        relevance = doc.get('relevance', 0)\n",
    "        court = doc.get('court', 'N/A')\n",
    "        domain = doc.get('domain', 'N/A')\n",
    "        year = doc.get('year', 'N/A')\n",
    "        \n",
    "        # Relevance label mapping\n",
    "        rel_label = \"NOT RELEVANT\" if relevance == 0 else (\"RELEVANT\" if relevance == 1 else \"HIGHLY RELEVANT\")\n",
    "        \n",
    "        prompt += f'''{separator_line}\n",
    "DOCUMENT [{idx}]\n",
    "\n",
    "Relevance: {rel_label} (Grade {relevance}/2)\n",
    "ID: {doc.get('doc_id', 'N/A')} | Court: {court} | Domain: {domain} | Year: {year}\n",
    "Scores: BM25={bm25:.2f}, FAISS={faiss:.3f}\n",
    "\n",
    "Content:\n",
    "{chunk_text}\n",
    "\n",
    "'''\n",
    "    \n",
    "    # Strict ranking instructions to force tangible outputs\n",
    "    prompt += f'''{separator_line}\n",
    "\n",
    "\n",
    "### OUTPUT FORMAT (STRICT):\n",
    "- Respond with EXACTLY one line in this format ‚Üí RANKING: <comma-separated indices>\n",
    "- Use document indices 0 to {len(slate)-1}, each exactly once, without extra spaces\n",
    "- Do NOT include headings, bullet points, Markdown, or explanations before or after the line\n",
    "\n",
    "Example:\n",
    "RANKING: 3,0,5,1,4,2,8,6,7,9\n",
    "\n",
    "RANKING:\n",
    "'''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"‚úÖ Seg√©df√ºggv√©nyek defini√°lva\")\n",
    "print(\"  üìù Enhanced learning-to-rank prompt template:\")\n",
    "print(\"     - English instructions with Hungarian data\")\n",
    "print(\"     - Explicit ranking criteria\")\n",
    "print(\"     - Step-by-step guidance\")\n",
    "print(\"     - Clear do's and don'ts\")\n",
    "print(\"     - Structured document presentation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slate adatok bet√∂lt√©se\n",
    "print(f\"üìÇ Slate adatok bet√∂lt√©se: {SLATE_FILE}\")\n",
    "df_slates = pd.read_json(SLATE_FILE, lines=True, encoding='utf-8')\n",
    "slates_data = df_slates.to_dict('records')\n",
    "for slate in slates_data:\n",
    "    slate[\"slate\"] = slate[\"slate\"][:20]\n",
    "print(f\"‚úÖ Bet√∂ltve: {len(slates_data)} slate\")\n",
    "\n",
    "sample = slates_data[0]\n",
    "print(f\"\\nüìã Minta slate strukt√∫ra:\")\n",
    "print(f\"  Query ID: {sample['query_id'][:50]}...\")\n",
    "print(f\"  Slate elemek: {len(sample['slate'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Multi-Metrika Reward Setup\n",
    "\n",
    "A standard nDCG@10 optimaliz√°ci√≥ helyett **konvex kombin√°ci√≥t** alkalmazunk:\n",
    "- **nDCG@10** (50%): Rangsor min≈ës√©g\n",
    "- **MRR@5** (35%): Els≈ë relev√°ns tal√°lat poz√≠ci√≥ja (√ºzleti priorit√°s!)\n",
    "- **Recall@20** (15%): Completeness\n",
    "\n",
    "Ez biztos√≠tja, hogy a model a **val√≥di √ºzleti √©rt√©ket** maximaliz√°lja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Metrika Reward Helper Functions (inline defini√°lva - nincs k√ºls≈ë import)\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def compute_mrr(relevances: List[int], k: Optional[int] = None) -> float:\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank sz√°m√≠t√°sa.\n",
    "    \n",
    "    MRR@k = 1 / rank_first_relevant (ha van relev√°ns a top-k-ban)\n",
    "           = 0 (ha nincs relev√°ns a top-k-ban)\n",
    "    \"\"\"\n",
    "    if k is not None:\n",
    "        relevances = relevances[:k]\n",
    "    \n",
    "    for i, rel in enumerate(relevances, start=1):\n",
    "        if rel > 0:\n",
    "            return 1.0 / i\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def compute_recall(relevances: List[int], k: int, total_relevant: Optional[int] = None) -> float:\n",
    "    \"\"\"\n",
    "    Recall@k sz√°m√≠t√°sa.\n",
    "    \n",
    "    Recall@k = |{relevant docs in top-k}| / |{all relevant docs}|\n",
    "    \"\"\"\n",
    "    if not relevances:\n",
    "        return 0.0\n",
    "    \n",
    "    # Relevantes doc-ok sz√°ma top-k-ban\n",
    "    relevant_in_topk = sum(1 for rel in relevances[:k] if rel > 0)\n",
    "    \n",
    "    # √ñsszes relev√°ns (ha nincs megadva, akkor az eg√©sz list√°b√≥l sz√°moljuk)\n",
    "    if total_relevant is None:\n",
    "        total_relevant = sum(1 for rel in relevances if rel > 0)\n",
    "    \n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return relevant_in_topk / total_relevant\n",
    "\n",
    "\n",
    "def compute_ndcg_metric(relevances: List[int], k: int) -> float:\n",
    "    \"\"\"\n",
    "    nDCG@k sz√°m√≠t√°sa sklearn haszn√°lat√°val.\n",
    "    \n",
    "    Args:\n",
    "        relevances: Relevancia √©rt√©kek ranking sorrendben [rel_1, rel_2, ...]\n",
    "        k: Top-k truncation\n",
    "    \n",
    "    Returns:\n",
    "        nDCG@k √©rt√©k [0, 1]\n",
    "    \"\"\"\n",
    "    if not relevances or k <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # sklearn ndcg_score: [true_relevances], [predicted_scores]\n",
    "    # predicted_scores: magasabb score = jobb ranking (ford√≠tott sorrend)\n",
    "    relevances_truncated = relevances[:k]\n",
    "    scores = list(range(len(relevances_truncated), 0, -1))  # [n, n-1, ..., 1]\n",
    "    \n",
    "    try:\n",
    "        ndcg = ndcg_score(\n",
    "            y_true=[relevances_truncated],\n",
    "            y_score=[scores],\n",
    "            k=k\n",
    "        )\n",
    "        return float(ndcg)\n",
    "    except Exception:\n",
    "        # Edge case: nincs relev√°ns dokumentum\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def compute_multi_metric_reward(\n",
    "    predicted_indices: List[int],\n",
    "    slate: List[Dict],\n",
    "    baseline_metrics: Dict[str, float],\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "    clip_range: Tuple[float, float] = (-1.0, 1.0),\n",
    "    use_sigmoid: bool = True,\n",
    "    mrr_tiebreak_bonus: float = 0.02,\n",
    "    diversity_penalty_weight: float = 0.0\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Multi-metrika reward sz√°m√≠t√°sa sigmoid-alap√∫ stabiliz√°ci√≥val.\n",
    "    \n",
    "    Jav√≠t√°sok:\n",
    "    - Sigmoid transzform√°ci√≥ a delta-kra ‚Üí stabilabb gradiens\n",
    "    - MRR tie-break bonus: ha nDCG/Recall stagn√°l de MRR javul\n",
    "    - Diverzit√°s b√ºntet√©s: ugyanazon court/domain t√∫ls√∫ly a top-10-ben\n",
    "    \n",
    "    Args:\n",
    "        predicted_indices: Model √°ltal predikt√°lt ranking indexek\n",
    "        slate: Slate adatok [{\"relevance\": int, \"court\": str, \"domain\": str, ...}, ...]\n",
    "        baseline_metrics: Baseline metrik√°k {\"ndcg@10\": float, \"mrr@5\": float, ...}\n",
    "        weights: Metrika s√∫lyok {\"ndcg10\": 0.6, \"mrr5\": 0.3, \"recall20\": 0.1}\n",
    "        clip_range: Reward clipping tartom√°ny (min, max)\n",
    "        use_sigmoid: Ha True, sigmoid(delta) transzform√°ci√≥\n",
    "        mrr_tiebreak_bonus: Bonus ha csak MRR javul (+Œµ)\n",
    "        diversity_penalty_weight: Diverzit√°s b√ºntet√©s s√∫lya (0 = kikapcsolva)\n",
    "    \n",
    "    Returns:\n",
    "        (total_reward, components_dict)\n",
    "    \"\"\"\n",
    "    # Default weights (updated: 0.6/0.3/0.1)\n",
    "    if weights is None:\n",
    "        weights = {\"ndcg10\": 0.60, \"mrr5\": 0.30, \"recall20\": 0.10}\n",
    "    \n",
    "    # Relevancia array a predicted ranking szerint\n",
    "    relevances = [slate[i][\"relevance\"] for i in predicted_indices]\n",
    "    \n",
    "    # Policy metrik√°k\n",
    "    policy_ndcg10 = compute_ndcg_metric(relevances, k=10)\n",
    "    policy_mrr5 = compute_mrr(relevances, k=5)\n",
    "    \n",
    "    # Recall@20: total_relevant a teljes slate-b≈ël (fix√°lt candidate pool!)\n",
    "    total_relevant = sum(1 for doc in slate if doc.get(\"relevance\", 0) > 0)\n",
    "    policy_recall20 = compute_recall(relevances, k=20, total_relevant=total_relevant)\n",
    "    \n",
    "    # Baseline metrik√°k\n",
    "    baseline_ndcg10 = baseline_metrics.get(\"ndcg@10\", 0.0)\n",
    "    baseline_mrr5 = baseline_metrics.get(\"mrr@5\", 0.0)\n",
    "    baseline_recall20 = baseline_metrics.get(\"recall@20\", 0.0)\n",
    "    \n",
    "    # Delta sz√°m√≠t√°s\n",
    "    delta_ndcg10 = policy_ndcg10 - baseline_ndcg10\n",
    "    delta_mrr5 = policy_mrr5 - baseline_mrr5\n",
    "    delta_recall20 = policy_recall20 - baseline_recall20\n",
    "    \n",
    "    # Sigmoid transzform√°ci√≥ (stabilabb reward signal)\n",
    "    if use_sigmoid:\n",
    "        # sigmoid(x) = 1 / (1 + exp(-k*x)), k=5 ‚Üí √©rz√©kenys√©g\n",
    "        def sigmoid_transform(delta, k=5.0):\n",
    "            return 2.0 / (1.0 + np.exp(-k * delta)) - 1.0  # range: [-1, 1]\n",
    "        \n",
    "        component_ndcg = weights[\"ndcg10\"] * sigmoid_transform(delta_ndcg10)\n",
    "        component_mrr = weights[\"mrr5\"] * sigmoid_transform(delta_mrr5)\n",
    "        component_recall = weights[\"recall20\"] * sigmoid_transform(delta_recall20)\n",
    "    else:\n",
    "        # Line√°ris (original)\n",
    "        component_ndcg = weights[\"ndcg10\"] * delta_ndcg10\n",
    "        component_mrr = weights[\"mrr5\"] * delta_mrr5\n",
    "        component_recall = weights[\"recall20\"] * delta_recall20\n",
    "    \n",
    "    # Total reward\n",
    "    reward_raw = component_ndcg + component_mrr + component_recall\n",
    "    \n",
    "    # ====== TIE-BREAK BONUS: MRR javul√°s ======\n",
    "    # Ha nDCG/Recall nem v√°ltozik jelent≈ësen, de MRR javul ‚Üí felhaszn√°l√≥i √©lm√©ny++\n",
    "    if abs(delta_ndcg10) < 0.01 and abs(delta_recall20) < 0.01 and delta_mrr5 > 0.05:\n",
    "        reward_raw += mrr_tiebreak_bonus\n",
    "    \n",
    "    # ====== DIVERZIT√ÅS B√úNTET√âS ======\n",
    "    # Ha a top-10 t√∫l homog√©n (ugyanaz a court/domain domin√°l) ‚Üí -Œµ\n",
    "    if diversity_penalty_weight > 0:\n",
    "        top10_indices = predicted_indices[:10]\n",
    "        top10_courts = [slate[i].get(\"court\", \"N/A\") for i in top10_indices if i < len(slate)]\n",
    "        top10_domains = [slate[i].get(\"domain\", \"N/A\") for i in top10_indices if i < len(slate)]\n",
    "        \n",
    "        # Shannon entropy (magas = diverzit√°s, alacsony = homog√©n)\n",
    "        def shannon_entropy(items):\n",
    "            counts = Counter(items)\n",
    "            total = len(items)\n",
    "            if total == 0:\n",
    "                return 0.0\n",
    "            probs = [c/total for c in counts.values()]\n",
    "            return -sum(p * np.log(p + 1e-9) for p in probs)\n",
    "        \n",
    "        court_entropy = shannon_entropy(top10_courts)\n",
    "        domain_entropy = shannon_entropy(top10_domains)\n",
    "        \n",
    "        # Normaliz√°l√°s: max entropy = log(10) ‚âà 2.3\n",
    "        max_entropy = np.log(10)\n",
    "        diversity_score = (court_entropy + domain_entropy) / (2 * max_entropy)\n",
    "        \n",
    "        # B√ºntet√©s ha alacsony diverzit√°s (< 0.5 ‚Üí homog√©n)\n",
    "        if diversity_score < 0.5:\n",
    "            penalty = diversity_penalty_weight * (0.5 - diversity_score)\n",
    "            reward_raw -= penalty\n",
    "    \n",
    "    # Clipping\n",
    "    reward_clipped = np.clip(reward_raw, clip_range[0], clip_range[1])\n",
    "    \n",
    "    # Komponensek dictionary (r√©szletes tracking)\n",
    "    components = {\n",
    "        \"reward_total\": float(reward_clipped),\n",
    "        \"reward_raw\": float(reward_raw),\n",
    "        \"component_ndcg10\": float(component_ndcg),\n",
    "        \"component_mrr5\": float(component_mrr),\n",
    "        \"component_recall20\": float(component_recall),\n",
    "        \"delta_ndcg10\": float(delta_ndcg10),\n",
    "        \"delta_mrr5\": float(delta_mrr5),\n",
    "        \"delta_recall20\": float(delta_recall20),\n",
    "        \"policy_ndcg10\": float(policy_ndcg10),\n",
    "        \"policy_mrr5\": float(policy_mrr5),\n",
    "        \"policy_recall20\": float(policy_recall20),\n",
    "        \"baseline_ndcg10\": float(baseline_ndcg10),\n",
    "        \"baseline_mrr5\": float(baseline_mrr5),\n",
    "        \"baseline_recall20\": float(baseline_recall20),\n",
    "    }\n",
    "    \n",
    "    return float(reward_clipped), components\n",
    "\n",
    "print(\"‚úÖ Multi-Metrika Reward Helper Functions defini√°lva (inline)\")\n",
    "print(f\"   üéØ Multi-metric reward: sigmoid-based, 0.6/0.3/0.1 weights\")\n",
    "print(f\"   üì¶ Nincs k√ºls≈ë import - minden inline defini√°lva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrik√°k bet√∂lt√©se (pre-computed from baseline_evaluation.ipynb)\n",
    "BASELINE_METRICS_PATH = BASE_PATH / \"baseline_query_metrics.csv\"\n",
    "\n",
    "if not BASELINE_METRICS_PATH.exists():\n",
    "    print(f\"‚ö†Ô∏è  FIGYELEM: Baseline metrics f√°jl nem tal√°lhat√≥: {BASELINE_METRICS_PATH}\")\n",
    "    print(f\"   El≈ëbb futtasd le a baseline_evaluation.ipynb notebookot!\")\n",
    "    print(f\"   Fallback: Baseline metrik√°k inicializ√°l√°sa 0-ra (NEM AJ√ÅNLOTT)\")\n",
    "    baseline_metrics_dict = {}\n",
    "else:\n",
    "    baseline_df = pd.read_csv(BASELINE_METRICS_PATH, encoding='utf-8')\n",
    "    baseline_metrics_dict = {\n",
    "        row[\"query_id\"]: row.to_dict()\n",
    "        for _, row in baseline_df.iterrows()\n",
    "    }\n",
    "    print(f\"‚úÖ Baseline metrik√°k bet√∂ltve: {len(baseline_metrics_dict)} query\")\n",
    "    \n",
    "    # Statisztik√°k\n",
    "    if baseline_metrics_dict:\n",
    "        sample_query = list(baseline_metrics_dict.keys())[0]\n",
    "        sample_metrics = baseline_metrics_dict[sample_query]\n",
    "        print(f\"\\nüìä El√©rhet≈ë metrik√°k (p√©lda query):\")\n",
    "        for key in sorted(sample_metrics.keys()):\n",
    "            if key != \"query_id\" and isinstance(sample_metrics[key], (int, float)):\n",
    "                print(f\"  ‚Ä¢ {key}: {sample_metrics[key]:.4f}\")\n",
    "        \n",
    "        # Aggreg√°lt baseline teljes√≠tm√©ny\n",
    "        avg_ndcg10 = np.mean([m.get(\"ndcg@10\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "        avg_mrr5 = np.mean([m.get(\"mrr@5\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "        avg_recall20 = np.mean([m.get(\"recall@20\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "        \n",
    "        print(f\"\\nüéØ Baseline teljes√≠tm√©ny (√°tlag):\")\n",
    "        print(f\"  ‚Ä¢ nDCG@10:   {avg_ndcg10:.4f}\")\n",
    "        print(f\"  ‚Ä¢ MRR@5:     {avg_mrr5:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Recall@20: {avg_recall20:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval split export√°l√°sa √∂sszehasonl√≠t√≥ √©rt√©kel√©shez\n",
    "# CRITICAL: Az eval split query_id-k export√°l√°sa determinisztikusan reproduk√°lhat√≥ form√°tumban\n",
    "# Ez biztos√≠tja, hogy a baseline √©s GRPO modell ugyanazon eval halmazon legyen ki√©rt√©kelve\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì§ Eval Split Export√°l√°sa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Query ID-k kinyer√©se az eval_dataset-b≈ël\n",
    "eval_query_ids = []\n",
    "\n",
    "for example in eval_dataset:\n",
    "    prompt = example[\"prompt\"]\n",
    "    # Ugyanaz a regex parsing logika, mint a reward function-ben\n",
    "    match = re.search(r'(?:SEARCH QUERY|Query)[:\\s\"]*([^\"\\n]+)', prompt)\n",
    "    query_id = None\n",
    "    if match:\n",
    "        query_id = match.group(1)\n",
    "    else:\n",
    "        match2 = re.search(r'##\\s*SEARCH\\s*QUERY\\s*\\n\\s*\"([^\"]+)\"', prompt)\n",
    "        if match2:\n",
    "            query_id = match2.group(1)\n",
    "    \n",
    "    if query_id and query_id not in eval_query_ids:\n",
    "        eval_query_ids.append(query_id)\n",
    "\n",
    "# Rendez√©s a determinisztikus kimenet√©rt\n",
    "eval_query_ids.sort()\n",
    "\n",
    "print(f\"\\nüìä Eval split statisztik√°k:\")\n",
    "print(f\"  Eval query-k sz√°ma: {len(eval_query_ids)}\")\n",
    "print(f\"  Train query-k sz√°ma: {len(train_dataset)}\")\n",
    "print(f\"  Split ar√°ny: {len(eval_query_ids) / (len(eval_query_ids) + len(train_dataset)):.1%}\")\n",
    "\n",
    "# Eval split export JSON form√°tumban\n",
    "eval_split_data = {\n",
    "    \"query_ids\": eval_query_ids,\n",
    "    \"num_queries\": len(eval_query_ids),\n",
    "    \"split_params\": {\n",
    "        \"train_split\": TRAIN_SPLIT,\n",
    "        \"seed\": SEED,\n",
    "        \"shuffle\": True\n",
    "    },\n",
    "    \"created_at\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "eval_split_path = OUTPUT_DIR / \"eval_split.json\"\n",
    "with open(eval_split_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_split_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Eval split export√°lva: {eval_split_path}\")\n",
    "print(f\"   Ez a f√°jl haszn√°lva lesz a baseline √©s GRPO √∂sszehasonl√≠t√≥ notebookban\")\n",
    "print(f\"   ugyanazon eval halmazon val√≥ ki√©rt√©kel√©shez.\")\n",
    "\n",
    "# Opcion√°lis: CSV export is (egyszer≈±bb bet√∂lt√©shez)\n",
    "eval_split_csv = OUTPUT_DIR / \"eval_split.csv\"\n",
    "eval_df = pd.DataFrame({\"query_id\": eval_query_ids})\n",
    "eval_df.to_csv(eval_split_csv, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Eval split CSV: {eval_split_csv}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bet√∂lt√©se - Memory Efficient\n",
    "print(f\"üîÑ Model bet√∂lt√©se Unsloth-tal: {MODEL_NAME}\")\n",
    "print(f\"  ‚öôÔ∏è  GPU Memory Utilization: {GPU_MEMORY_UTILIZATION}\")\n",
    "print(f\"  ‚öôÔ∏è  Max LoRA Rank: {LORA_RANK}\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    fast_inference=True,  # CRITICAL: Enable vLLM for GRPO rollouts (2-3x faster inference)\n",
    "    max_lora_rank=LORA_RANK,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    max_model_len=MAX_SEQ_LENGTH - 1,  # CRITICAL: vLLM max_model_len must match max_seq_length for long prompts\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    use_rslora=True,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# Tokenizer padding fix (tensor shape stability)\n",
    "# CRITICAL: Csak padding_side √°ll√≠t√°sa, NE √°ll√≠tsuk √°t a pad_token-et, mert a tokenizer √∫jra√°ll√≠t√°sa\n",
    "# tenzorm√©ret elt√©r√©st okozhat a GRPO trainer vLLM bels≈ë logik√°j√°val\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Note: pad_token be√°ll√≠t√°sa kihagyva - a GRPO trainer √©s vLLM automatikusan kezeli\n",
    "# Az explicit pad_token √°t√°ll√≠t√°s √ºtk√∂zhet a modell config-tal √©s tenzor m√©ret probl√©m√°kat okozhat\n",
    "\n",
    "print(\"‚úÖ Model √©s tokenizer bet√∂ltve (Unsloth + vLLM + RSLoRA)\")\n",
    "print(f\"  ‚úì fast_inference=True: vLLM enged√©lyezve GRPO rollouts-hoz (2-3x gyorsabb)\")\n",
    "print(f\"  ‚úì Padding side: {tokenizer.padding_side}\")\n",
    "print(f\"  ‚úì Pad token ID: {tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id else 'N/A (auto-managed)'}\")\n",
    "print(f\"  ‚úì Note: GRPO trainer automatikusan kezeli a tokenizer be√°ll√≠t√°sokat vLLM-mel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU mem√≥ria monitoring model bet√∂lt√©s ut√°n\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    free = total - allocated\n",
    "    \n",
    "    print(f\"\\nüìä GPU Mem√≥ria Model Bet√∂lt√©s Ut√°n:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Free: {free:.2f} GB\")\n",
    "    print(f\"  Free %: {(free/total)*100:.1f}%\")\n",
    "    \n",
    "    if free < 10:\n",
    "        print(f\"  ‚ö†Ô∏è  FIGYELEM: Kev√©s szabad mem√≥ria ({free:.2f} GB)!\")\n",
    "        print(f\"  üí° Opci√≥: Tov√°bbi cs√∂kkent√©s lehet sz√ºks√©ges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== MULTI-METRIKA REWARD FUNCTION (Sigmoid-Based) ======\n",
    "# Konvex kombin√°ci√≥ja h√°rom f≈ë metrik√°nak sigmoid transzform√°ci√≥val:\n",
    "# - nDCG@10 (60%): Rangsor min≈ës√©g (n√∂velt s√∫ly)\n",
    "# - MRR@5 (30%): Els≈ë relev√°ns tal√°lat (gyakorlati haszn√°lhat√≥s√°g)\n",
    "# - Recall@20 (10%): Completeness\n",
    "#\n",
    "# Jav√≠t√°sok:\n",
    "# - Sigmoid(delta) ‚Üí stabilabb gradiens, nem satur√°l√≥dik\n",
    "# - MRR tie-break bonus: ha nDCG/Recall stagn√°l, de MRR javul ‚Üí +0.02\n",
    "# - Diverzit√°s b√ºntet√©s: homog√©n top-10 (court/domain) ‚Üí -Œµ\n",
    "\n",
    "# Reward konfigur√°ci√≥ (config.py alapj√°n)\n",
    "REWARD_WEIGHTS = {\n",
    "    \"ndcg10\": 0.60,    # 50% ‚Üí 60%\n",
    "    \"mrr5\": 0.30,      # 35% ‚Üí 30%\n",
    "    \"recall20\": 0.10,  # 15% ‚Üí 10%\n",
    "}\n",
    "REWARD_CLIP_RANGE = (-1.0, 1.0)\n",
    "REWARD_USE_SIGMOID = True\n",
    "REWARD_MRR_TIEBREAK_BONUS = 0.02\n",
    "REWARD_DIVERSITY_PENALTY = 0.05  # Diverzit√°s b√ºntet√©s s√∫lya\n",
    "\n",
    "# Komponens tracking glob√°lis v√°ltoz√≥ (logging c√©lokra)\n",
    "reward_components_log = []\n",
    "\n",
    "def multi_metric_reward_function(completions, prompts, **kwargs):\n",
    "    \"\"\"\n",
    "    Multi-metrika GRPO reward f√ºggv√©ny sigmoid transzform√°ci√≥val.\n",
    "    \n",
    "    A reward h√°rom metrika s√∫lyozott kombin√°ci√≥ja:\n",
    "    1. nDCG@10 (60%): Rangsor min≈ës√©g\n",
    "    2. MRR@5 (30%): Els≈ë relev√°ns tal√°lat poz√≠ci√≥ja (UX priorit√°s!)\n",
    "    3. Recall@20 (10%): Relev√°ns tal√°latok lefedetts√©ge\n",
    "    \n",
    "    Speci√°lis funkci√≥k:\n",
    "    - Sigmoid(delta): stabil gradiens, nem satur√°l√≥dik\n",
    "    - MRR tie-break: +bonus ha csak MRR javul (felhaszn√°l√≥i √©lm√©ny++)\n",
    "    - Diverzit√°s check: homog√©n top-10 ‚Üí penalty\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: Reward √©rt√©kek minden completion-h√∂z\n",
    "    \"\"\"\n",
    "    global reward_components_log\n",
    "    rewards = []\n",
    "    batch_components = []\n",
    "    \n",
    "    for completion, prompt in zip(completions, prompts):\n",
    "        try:\n",
    "            # 1. Query ID extraction\n",
    "            match = re.search(r'(?:SEARCH QUERY|Query)[:\\s\"]*([^\"\\n]+)', prompt)\n",
    "            query_id = None\n",
    "            if match:\n",
    "                query_id = match.group(1)\n",
    "            else:\n",
    "                match2 = re.search(r'##\\s*SEARCH\\s*QUERY\\s*\\n\\s*\"([^\"]+)\"', prompt)\n",
    "                if match2:\n",
    "                    query_id = match2.group(1)\n",
    "            \n",
    "            if not query_id or query_id not in slate_lookup:\n",
    "                # Parsing hiba - enyhe b√ºntet√©s\n",
    "                rewards.append(-0.1)\n",
    "                batch_components.append({\"error\": \"query_id_not_found\"})\n",
    "                continue\n",
    "            \n",
    "            # 2. Slate lookup\n",
    "            slate = slate_lookup[query_id]\n",
    "            \n",
    "            # 3. Parse model output\n",
    "            predicted_indices = parse_model_ranking(completion, len(slate))\n",
    "            \n",
    "            # 4. Baseline metrik√°k lookup\n",
    "            if query_id not in baseline_metrics_dict:\n",
    "                # Nincs baseline adat - fallback to zero baseline (nem ide√°lis, de biztons√°gos)\n",
    "                baseline_metrics = {\n",
    "                    \"ndcg@10\": 0.0,\n",
    "                    \"mrr@5\": 0.0,\n",
    "                    \"mrr@10\": 0.0,  # Kompatibilit√°s\n",
    "                    \"recall@20\": 0.0,\n",
    "                }\n",
    "            else:\n",
    "                baseline_metrics = baseline_metrics_dict[query_id]\n",
    "\n",
    "        \n",
    "            \n",
    "            # 5. Multi-metric reward sz√°m√≠t√°s (helper function from src.reward_metrics)\n",
    "            reward, components = compute_multi_metric_reward(\n",
    "                predicted_indices=predicted_indices,\n",
    "                slate=slate,\n",
    "                baseline_metrics=baseline_metrics,\n",
    "                weights=REWARD_WEIGHTS,\n",
    "                clip_range=REWARD_CLIP_RANGE,\n",
    "                use_sigmoid=REWARD_USE_SIGMOID,\n",
    "                mrr_tiebreak_bonus=REWARD_MRR_TIEBREAK_BONUS,\n",
    "                diversity_penalty_weight=REWARD_DIVERSITY_PENALTY,\n",
    "            )\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            batch_components.append(components)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Exception eset√©n enyhe b√ºntet√©s\n",
    "            rewards.append(-0.1)\n",
    "            batch_components.append({\"error\": str(e)[:100]})\n",
    "    \n",
    "    # Komponensek ment√©se logginghoz\n",
    "    reward_components_log.extend(batch_components)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"‚úÖ Multi-Metrika GRPO Reward function defini√°lva (Sigmoid-Based v2.0)\")\n",
    "print(f\"   üìä Reward s√∫lyok:\")\n",
    "print(f\"      ‚Ä¢ nDCG@10:   {REWARD_WEIGHTS['ndcg10']:.0%} (rangsor min≈ës√©g)\")\n",
    "print(f\"      ‚Ä¢ MRR@5:     {REWARD_WEIGHTS['mrr5']:.0%} (els≈ë relev√°ns tal√°lat)\")\n",
    "print(f\"      ‚Ä¢ Recall@20: {REWARD_WEIGHTS['recall20']:.0%} (completeness)\")\n",
    "print(f\"   üéØ Sigmoid transzform√°ci√≥: {REWARD_USE_SIGMOID}\")\n",
    "print(f\"   üéÅ MRR tie-break bonus: +{REWARD_MRR_TIEBREAK_BONUS}\")\n",
    "print(f\"   üåà Diverzit√°s b√ºntet√©s: {REWARD_DIVERSITY_PENALTY}\")\n",
    "print(f\"   üìè Reward clipping: {REWARD_CLIP_RANGE}\")\n",
    "print(f\"   üìà Komponens tracking: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Trainer konfigur√°ci√≥ - MEMORY EFFICIENT + CURRICULUM LEARNING\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    generation_batch_size=GENERATION_BATCH_SIZE,  # Explicit: LCM(6, 2) = 6\n",
    "    optim=OPTIMIZER_NAME,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    loss_type=\"dr_grpo\",\n",
    "    scale_rewards=\"batch\",\n",
    "    use_vllm=True,\n",
    "    vllm_importance_sampling_correction=False,  # vLLM token logprobs nem egyeznek az alap modell√©vel\n",
    "    importance_sampling_level=\"sequence\",\n",
    "    mask_truncated_completions=True,\n",
    "    epsilon=0.1,  # Enhanced: 0.2 ‚Üí 0.1 (smaller policy updates, more stable)\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False, \"use_unsloth\": True},\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    dataloader_num_workers=1,  # 2‚Üí1: cs√∂kkentett worker\n",
    "    seed=SEED,\n",
    "    max_completion_length=8192,  # CRITICAL: explicit completion length (default 256 t√∫l kev√©s 30 dokumentumhoz)\n",
    ")\n",
    "\n",
    "grpo_config.unsloth_num_chunks = 1\n",
    "\n",
    "print(\"\\n‚úÖ GRPO Trainer konfigur√°ci√≥ k√©sz (Memory Efficient + Curriculum Learning)\")\n",
    "print(f\"  ‚úì generation_batch_size={GENERATION_BATCH_SIZE}\")\n",
    "print(f\"    - oszthat√≥ num_generations={NUM_GENERATIONS}-szel: {GENERATION_BATCH_SIZE}√∑{NUM_GENERATIONS}={GENERATION_BATCH_SIZE//NUM_GENERATIONS}\")\n",
    "print(f\"    - oszthat√≥ per_device_batch={PER_DEVICE_BATCH_SIZE}-gyel: {GENERATION_BATCH_SIZE}√∑{PER_DEVICE_BATCH_SIZE}={GENERATION_BATCH_SIZE//PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"  ‚úì Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚úì Mem√≥ria optimaliz√°ci√≥: 50% GPU util, 1 worker, rank 32\")\n",
    "if CURRICULUM_ENABLED:\n",
    "    print(f\"  ‚úì Curriculum Learning: {CURRICULUM_STRATEGY} strat√©gia aktiv√°lva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Multi-Metrika Reward Validation\n",
    "\n",
    "Most tesztelj√ºk a friss√≠tett multi-metrika reward f√ºggv√©nyt:\n",
    "- Sigmoid transzform√°ci√≥ m≈±k√∂d√©se\n",
    "- Komponensek (nDCG/MRR/Recall) sz√°m√≠t√°sa\n",
    "- MRR tie-break bonus\n",
    "- Diverzit√°s check\n",
    "- Baseline metrics bet√∂lt√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ind√≠t√°sa\n",
    "print(\"\\nüöÄ GRPO TRAINING IND√çT√ÅSA\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if CURRICULUM_ENABLED:\n",
    "    print(f\"üìö Curriculum Learning akt√≠v: {CURRICULUM_STRATEGY}\")\n",
    "    if CURRICULUM_STRATEGY == \"filtered_dataset\":\n",
    "        print(\"   Training k√∂nnyebb p√©ld√°kkal (easy + medium) - jobb tanul√°si signal\")\n",
    "    print()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Training sikeresen befejezve!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training hiba: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Nincs el√©rhet≈ë trainer.state.log_history √©s a metrics.json f√°jl sem tal√°lhat√≥.\n"
     ]
    }
   ],
   "source": [
    "# Robusztus tr√©ning log vizualiz√°ci√≥ (hibaellen≈ërz√©ssel)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Megpr√≥b√°ljuk a TRL trainer log_history-j√°t haszn√°lni; ha nincs, es√ºnk vissza a metrics.json-re\n",
    "log_history = None\n",
    "try:\n",
    "    _ = trainer  # ellen≈ërz√©s, hogy l√©tezik-e\n",
    "    log_history = getattr(trainer.state, \"log_history\", None)\n",
    "except NameError:\n",
    "    log_history = None\n",
    "\n",
    "if log_history:\n",
    "    logs = pd.DataFrame(log_history)\n",
    "else:\n",
    "    # Fallback: metrics.json bet√∂lt√©se √©s csak a reward trend megjelen√≠t√©se\n",
    "    if 'METRICS_FILE' in globals():\n",
    "        metrics_path = METRICS_FILE\n",
    "    else:\n",
    "        metrics_path = Path(\"data/models/grpo_policy/metrics.json\")\n",
    "\n",
    "    if Path(metrics_path).exists():\n",
    "        with open(metrics_path, 'r', encoding='utf-8') as f:\n",
    "            metrics = json.load(f)\n",
    "        rewards_trend = metrics.get(\"training_rewards\", {}).get(\"trend\", [])\n",
    "        if rewards_trend:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(rewards_trend, color=\"royalblue\")\n",
    "            plt.title(\"Jutalom trend (metrics.json)\")\n",
    "            plt.xlabel(\"L√©p√©s\")\n",
    "            plt.ylabel(\"Jutalom\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Nincs el√©rhet≈ë trainer.state.log_history √©s a metrics.json nem tartalmaz 'training_rewards.trend'-et.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nincs el√©rhet≈ë trainer.state.log_history √©s a metrics.json f√°jl sem tal√°lhat√≥.\")\n",
    "    raise SystemExit(0)  # Nincs teljes logs, l√©p√ºnk ki a cell√°b√≥l a tov√°bbi √°br√°k n√©lk√ºl\n",
    "\n",
    "# Innent≈ël a log_history alapj√°n dolgozunk\n",
    "if isinstance(logs, pd.DataFrame) and not logs.empty:\n",
    "    # Csak numerikus oszlopok megtart√°sa\n",
    "    numeric_cols = [c for c in logs.columns if np.issubdtype(pd.Series(logs[c]).dropna().dtype, np.number)]\n",
    "    if not numeric_cols:\n",
    "        print(\"‚ö†Ô∏è A logokban nincs numerikus oszlop a megjelen√≠t√©shez.\")\n",
    "    else:\n",
    "        logs = logs[numeric_cols].copy()\n",
    "\n",
    "        def pick_col(df, candidates):\n",
    "            for c in candidates:\n",
    "                if c in df.columns:\n",
    "                    return c\n",
    "            return None\n",
    "\n",
    "        # Reward (mean) oszlop kiv√°laszt√°sa t√∂bb lehets√©ges kulccsal\n",
    "        reward_candidates = [\n",
    "            \"rewards/reward_function/mean\",  # TRL GRPO gyakori\n",
    "            \"rewards/mean\",\n",
    "            \"reward/mean\",\n",
    "            \"rewards/reward\",\n",
    "            \"reward\",\n",
    "        ]\n",
    "        reward_col = pick_col(logs, reward_candidates)\n",
    "        if reward_col is None:\n",
    "            print(\"‚ö†Ô∏è Nem tal√°lhat√≥ reward √°tlag oszlop a logokban. Kihagyom az 1. √°br√°t.\")\n",
    "\n",
    "        # Reward sz√≥r√°s oszlop (opcion√°lis)\n",
    "        reward_std_candidates = [\n",
    "            \"rewards/reward_function/std\",\n",
    "            \"rewards/std\",\n",
    "            \"reward/std\",\n",
    "        ]\n",
    "        reward_std_col = pick_col(logs, reward_std_candidates)\n",
    "\n",
    "        # Loss √©s KL oszlopok (opcion√°lis)\n",
    "        loss_col = pick_col(logs, [\"loss\", \"train/loss\"])  \n",
    "        kl_col = pick_col(logs, [\"kl\", \"train/kl\", \"kl_divergence\"]) \n",
    "\n",
    "        # √Åbr√°k l√©trehoz√°sa dinamikusan az el√©rhet≈ë metrik√°k alapj√°n\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(13, 8))\n",
    "        axes = axes.flatten()\n",
    "        ax_idx = 0\n",
    "\n",
    "        # 1) Reward trend (sim√≠tott) + opcion√°lis sz√≥r√°s s√°v\n",
    "        if reward_col is not None:\n",
    "            reward_smooth = logs[reward_col].rolling(window=10, min_periods=1).mean()\n",
    "            axes[ax_idx].plot(reward_smooth, label=f\"Jutalom (√°tlag, {reward_col})\", color=\"royalblue\")\n",
    "            if reward_std_col is not None:\n",
    "                lo = reward_smooth - logs[reward_std_col]\n",
    "                hi = reward_smooth + logs[reward_std_col]\n",
    "                axes[ax_idx].fill_between(logs.index, lo, hi, alpha=0.2, color=\"royalblue\", label=\"¬± sz√≥r√°s\")\n",
    "            axes[ax_idx].set_title(\"Jutalom trend\")\n",
    "            axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "            axes[ax_idx].set_ylabel(\"Jutalom\")\n",
    "            axes[ax_idx].legend()\n",
    "            ax_idx += 1\n",
    "\n",
    "        # 2) Loss g√∂rbe\n",
    "        if loss_col is not None and ax_idx < len(axes):\n",
    "            axes[ax_idx].plot(logs[loss_col], label=\"Vesztes√©g (loss)\", color=\"firebrick\")\n",
    "            axes[ax_idx].set_title(\"Tan√≠t√°si vesztes√©g\")\n",
    "            axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "            axes[ax_idx].set_ylabel(\"Loss\")\n",
    "            axes[ax_idx].legend()\n",
    "            ax_idx += 1\n",
    "\n",
    "        # 3) KL g√∂rbe\n",
    "        if kl_col is not None and ax_idx < len(axes):\n",
    "            axes[ax_idx].plot(logs[kl_col], label=\"KL divergencia\", color=\"darkorange\")\n",
    "            axes[ax_idx].set_title(\"KL divergencia alakul√°sa\")\n",
    "            axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "            axes[ax_idx].set_ylabel(\"KL\")\n",
    "            axes[ax_idx].legend()\n",
    "            ax_idx += 1\n",
    "\n",
    "        # 4) Reward sz√≥r√°s k√ºl√∂n (ha van √©s maradt hely)\n",
    "        if reward_std_col is not None and ax_idx < len(axes):\n",
    "            axes[ax_idx].plot(logs[reward_std_col], label=\"Jutalom sz√≥r√°s\", color=\"seagreen\")\n",
    "            axes[ax_idx].set_title(\"Jutalom sz√≥r√°s\")\n",
    "            axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "            axes[ax_idx].set_ylabel(\"Sz√≥r√°s\")\n",
    "            axes[ax_idx].legend()\n",
    "            ax_idx += 1\n",
    "\n",
    "        # Ha kevesebb metrika √°ll rendelkez√©sre, a marad√©k tengelyeket rejts√ºk el\n",
    "        for i in range(ax_idx, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è A logok DataFrame √ºres.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Prompt Quality Check - Sample Completions\n",
    "# Futtasd TRAINING EL≈êTT, hogy l√°sd, milyen outputokat gener√°l a modell\n",
    "\n",
    "print(\"üî¨ PROMPT QUALITY CHECK - Sample Completions Gener√°l√°sa\\n\")\n",
    "print(\"Ez a cella teszteli, hogy a modell milyen outputokat gener√°l az √∫j prompttal.\")\n",
    "print(\"FONTOS: Ez optional, csak debug c√©lra.\\n\")\n",
    "\n",
    "# Egy random query kiv√°laszt√°sa\n",
    "import random\n",
    "random.seed(42)\n",
    "test_slate_idx = random.randint(0, len(slates_data)-1)\n",
    "test_slate_data = slates_data[test_slate_idx]\n",
    "test_prompt_text = create_training_prompt(test_slate_data[\"query_id\"], test_slate_data[\"slate\"])\n",
    "\n",
    "# Gener√°lunk n√©h√°ny completion-t\n",
    "print(f\"Query: {test_slate_data['query_id'][:80]}...\")\n",
    "print(f\"Slate size: {len(test_slate_data['slate'])}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    # Model inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(test_prompt_text, return_tensors=\"pt\", truncation=False, max_length=MAX_SEQ_LENGTH)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    print(\"\\nüé≤ 3 Sample Completion Gener√°l√°sa...\\n\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,  # N√∂velve 100-r√≥l 2048-ra, hogy elegend≈ë legyen 30 dokumentum rangsorol√°s√°hoz\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,  # Stop sequence hozz√°ad√°sa\n",
    "            )\n",
    "        \n",
    "        completion = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        parsed = parse_model_ranking(completion, len(test_slate_data[\"slate\"]))\n",
    "        \n",
    "        print(f\"Completion {i+1}:\")\n",
    "        print(f\"  Raw output: {completion[:150]}\")\n",
    "        print(f\"  Parsed indices: {parsed[:10]}\")  # First 10 indices\n",
    "        print(f\"  Valid? {len(parsed) == len(test_slate_data['slate']) and len(set(parsed)) == len(parsed)}\")\n",
    "        print(f\"  Teljes ranking: {parsed}\")\n",
    "        print(completion[-200:])\n",
    "        print(len(tokenizer(test_prompt_text).input_ids))\n",
    "    \n",
    "    # Relevance info\n",
    "    relevances = [doc.get('relevance', 0) for doc in test_slate_data[\"slate\"]]\n",
    "    print(f\"üìä Ground truth relevance distribution:\")\n",
    "    print(f\"  Relevant docs (rel>=1): {sum(1 for r in relevances if r >= 1)}/{len(relevances)}\")\n",
    "    print(f\"  Highly relevant (rel=2): {sum(1 for r in relevances if r == 2)}/{len(relevances)}\")\n",
    "    print(f\"  Baseline order nDCG@10: {calculate_ndcg(list(range(len(relevances))), relevances, k=10):.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\\n‚úÖ Prompt quality check complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during quality check: {e}\")\n",
    "    print(\"Ez nem probl√©ma, a training ett≈ël m√©g futhat.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifactumok ment√©se\n",
    "print(\"\\nüíæ Artifactumok ment√©se...\")\n",
    "model.save_pretrained_merged(str(OUTPUT_DIR), tokenizer, save_method=\"lora\")\n",
    "print(f\"  ‚úÖ LoRA adapter: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Modell √ñsszehasonl√≠t√°s\n",
    "\n",
    "Az √©rt√©kel√©s √©s a baseline vs GRPO √∂sszehasonl√≠t√°s a **`notebooks/model_comparison.ipynb`** notebookban t√∂rt√©nik.\n",
    "\n",
    "Ez biztos√≠tja, hogy a baseline √©s GRPO modell **ugyanazon eval halmazon** legyen ki√©rt√©kelve, √≠gy a metrik√°k √∂sszevethet≈ëk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (ranx)\n",
    "print(\"\\nüìä √ârt√©kel√©s futtat√°sa (ranx)...\")\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_policy_ranx(dataset_subset, slate_lookup_dict, dataset_name=\"\", batch_size=4, min_tokens=64, max_tokens=512):\n",
    "    k_values = [5, 10, 20]\n",
    "    metrics_to_compute = [\"map\", \"mrr\"]\n",
    "    for k in k_values:\n",
    "        metrics_to_compute.extend([f\"ndcg@{k}\", f\"precision@{k}\", f\"recall@{k}\"])\n",
    "\n",
    "    qrels_dict = {}\n",
    "    baseline_run_dict = {}\n",
    "    policy_run_dict = {}\n",
    "    parse_successes = 0\n",
    "    entries = []\n",
    "\n",
    "    for example in dataset_subset:\n",
    "        prompt = example[\"prompt\"]\n",
    "        match = re.search(r'QUERY:\\s*\"([^\"]+)\"', prompt)\n",
    "        if match:\n",
    "            query_id = match.group(1)\n",
    "        else:\n",
    "            match2 = re.search(r'##\\s*SEARCH\\s*QUERY\\s*\\n\\s*\"([^\"]+)\"', prompt)\n",
    "            query_id = match2.group(1) if match2 else None\n",
    "        if not query_id or query_id not in slate_lookup_dict:\n",
    "            continue\n",
    "\n",
    "        slate = slate_lookup_dict[query_id]\n",
    "        query_qrels = {}\n",
    "        for doc in slate:\n",
    "            doc_id = doc.get('doc_id')\n",
    "            relevance = doc.get('relevance', 0)\n",
    "            if doc_id is not None:\n",
    "                query_qrels[doc_id] = relevance\n",
    "        if not query_qrels:\n",
    "            continue\n",
    "        qrels_dict[query_id] = query_qrels\n",
    "\n",
    "        baseline_docs = {}\n",
    "        for rank, doc in enumerate(slate):\n",
    "            doc_id = doc.get('doc_id')\n",
    "            if doc_id is not None:\n",
    "                baseline_docs[doc_id] = len(slate) - rank\n",
    "        baseline_run_dict[query_id] = baseline_docs\n",
    "\n",
    "        entries.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"query_id\": query_id,\n",
    "            \"slate\": slate,\n",
    "        })\n",
    "\n",
    "    if not entries:\n",
    "        print(f\"  {dataset_name}: nincs √©rt√©kelhet≈ë lek√©rdez√©s.\")\n",
    "        return {\n",
    "            \"baseline_metrics\": {},\n",
    "            \"policy_metrics\": {},\n",
    "            \"num_queries\": 0,\n",
    "            \"positive_improvement_count\": 0,\n",
    "            \"positive_improvement_ratio\": 0.0,\n",
    "            \"parse_success_rate\": 0.0,\n",
    "            \"parse_success_count\": 0,\n",
    "            \"per_query_results\": [],\n",
    "        }\n",
    "\n",
    "    total_entries = len(entries)\n",
    "    num_batches = math.ceil(total_entries / batch_size)\n",
    "    device = model.device\n",
    "\n",
    "    print(f\"  {dataset_name}: {total_entries} lek√©rdez√©s feldolgoz√°sa batchelt gener√°l√°ssal...\")\n",
    "    progress_desc = f\"{dataset_name} batch feldolgoz√°s\" if dataset_name else \"√ârt√©kel√©s batch feldolgoz√°s\"\n",
    "    progress_bar = tqdm(total=num_batches, desc=progress_desc, unit=\"batch\")\n",
    "    original_padding_side = getattr(tokenizer, \"padding_side\", \"right\")\n",
    "    if getattr(tokenizer, \"pad_token\", None) is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            for start in range(0, total_entries, batch_size):\n",
    "                batch_entries = entries[start:start + batch_size]\n",
    "                prompts = [entry[\"prompt\"] for entry in batch_entries]\n",
    "                encodings = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "                input_lengths = encodings.attention_mask.sum(dim=1)\n",
    "\n",
    "                dynamic_max_new_tokens = 0\n",
    "                for entry in batch_entries:\n",
    "                    slate_len = len(entry[\"slate\"])\n",
    "                    dynamic_max_new_tokens = max(dynamic_max_new_tokens, slate_len * 6)\n",
    "                dynamic_max_new_tokens = int(max(min_tokens, min(max_tokens, dynamic_max_new_tokens)))\n",
    "\n",
    "                outputs = model.generate(\n",
    "                    **encodings,\n",
    "                    max_new_tokens=dynamic_max_new_tokens,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.0,\n",
    "                    num_beams=1,\n",
    "                )\n",
    "\n",
    "                for idx, entry in enumerate(batch_entries):\n",
    "                    input_len = int(input_lengths[idx].item())\n",
    "                    completion = tokenizer.decode(outputs[idx][input_len:], skip_special_tokens=True)\n",
    "                    slate = entry[\"slate\"]\n",
    "                    query_id = entry[\"query_id\"]\n",
    "\n",
    "                    predicted_indices = parse_model_ranking(completion, len(slate))\n",
    "                    is_valid_parse = len(predicted_indices) == len(slate) and len(set(predicted_indices)) == len(slate)\n",
    "                    if not is_valid_parse:\n",
    "                        predicted_indices = list(range(len(slate)))\n",
    "                    else:\n",
    "                        parse_successes += 1\n",
    "\n",
    "                    policy_docs = {}\n",
    "                    for rank, slate_idx in enumerate(predicted_indices):\n",
    "                        if slate_idx < len(slate):\n",
    "                            doc_id = slate[slate_idx].get(\"doc_id\")\n",
    "                            if doc_id is not None:\n",
    "                                policy_docs[doc_id] = len(slate) - rank\n",
    "                    policy_run_dict[query_id] = policy_docs\n",
    "\n",
    "                progress_bar.update(1)\n",
    "    finally:\n",
    "        tokenizer.padding_side = original_padding_side\n",
    "        progress_bar.close()\n",
    "    qrels_ranx = Qrels(qrels_dict)\n",
    "    baseline_run_ranx = Run(baseline_run_dict)\n",
    "    policy_run_ranx = Run(policy_run_dict)\n",
    "\n",
    "    baseline_metrics = evaluate(qrels_ranx, baseline_run_ranx, metrics_to_compute)\n",
    "    policy_metrics = evaluate(qrels_ranx, policy_run_ranx, metrics_to_compute)\n",
    "\n",
    "    per_query_results = []\n",
    "    for query_id in qrels_dict.keys():\n",
    "        row = {\"query_id\": query_id}\n",
    "        for metric in metrics_to_compute:\n",
    "            baseline_score = baseline_run_ranx.scores.get(metric, {}).get(query_id, 0.0)\n",
    "            row[f\"baseline_{metric}\"] = float(baseline_score)\n",
    "        for metric in metrics_to_compute:\n",
    "            policy_score = policy_run_ranx.scores.get(metric, {}).get(query_id, 0.0)\n",
    "            row[f\"policy_{metric}\"] = float(policy_score)\n",
    "        baseline_ndcg10 = baseline_run_ranx.scores.get(\"ndcg@10\", {}).get(query_id, 0.0)\n",
    "        policy_ndcg10 = policy_run_ranx.scores.get(\"ndcg@10\", {}).get(query_id, 0.0)\n",
    "        row[\"improvement_ndcg@10\"] = float(policy_ndcg10 - baseline_ndcg10)\n",
    "        per_query_results.append(row)\n",
    "\n",
    "    improvements = [row[\"improvement_ndcg@10\"] for row in per_query_results]\n",
    "    positive_improvements = sum(1 for delta in improvements if delta > 0)\n",
    "    positive_ratio = positive_improvements / len(improvements) if improvements else 0.0\n",
    "    parse_success_rate = parse_successes / len(per_query_results) if per_query_results else 0.0\n",
    "\n",
    "    print(f\"  {dataset_name}: v√©ge. Baseline nDCG@10 = {baseline_metrics.get('ndcg@10', 0.0):.4f}, Policy nDCG@10 = {policy_metrics.get('ndcg@10', 0.0):.4f}, Javul√°s = {policy_metrics.get('ndcg@10', 0.0) - baseline_metrics.get('ndcg@10', 0.0):+.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"baseline_metrics\": {k: float(v) for k, v in baseline_metrics.items()},\n",
    "        \"policy_metrics\": {k: float(v) for k, v in policy_metrics.items()},\n",
    "        \"num_queries\": len(qrels_dict),\n",
    "        \"positive_improvement_count\": positive_improvements,\n",
    "        \"positive_improvement_ratio\": float(positive_ratio),\n",
    "        \"parse_success_rate\": float(parse_success_rate),\n",
    "        \"parse_success_count\": parse_successes,\n",
    "        \"per_query_results\": per_query_results,\n",
    "    }\n",
    "\n",
    "train_eval_results = evaluate_policy_ranx(train_dataset, slate_lookup, \"Train halmaz\")\n",
    "eval_eval_results = evaluate_policy_ranx(eval_dataset, slate_lookup, \"Eval halmaz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-query export\n",
    "training_rewards = []\n",
    "for log_entry in trainer.state.log_history:\n",
    "    if \"rewards/mean\" in log_entry:\n",
    "        training_rewards.append(log_entry[\"rewards/mean\"]) \n",
    "\n",
    "train_per_query_df = pd.DataFrame(train_eval_results[\"per_query_results\"])\n",
    "train_per_query_csv = OUTPUT_DIR / \"train_per_query_results.csv\"\n",
    "train_per_query_df.to_csv(train_per_query_csv, index=False, encoding='utf-8')\n",
    "print(f\"  ‚úÖ Train per-query results: {train_per_query_csv}\")\n",
    "\n",
    "eval_per_query_df = pd.DataFrame(eval_eval_results[\"per_query_results\"])\n",
    "eval_per_query_csv = OUTPUT_DIR / \"eval_per_query_results.csv\"\n",
    "eval_per_query_df.to_csv(eval_per_query_csv, index=False, encoding='utf-8')\n",
    "print(f\"  ‚úÖ Eval per-query results: {eval_per_query_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics export\n",
    "final_metrics = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"training_samples\": len(train_dataset),\n",
    "    \"eval_samples\": len(eval_dataset),\n",
    "    \"slate_size\": SLATE_SIZE,\n",
    "    \"group_size\": GROUP_SIZE,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"lora_rank\": LORA_RANK,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"final_loss\": trainer.state.log_history[-1].get(\"loss\", 0.0) if trainer.state.log_history else 0.0,\n",
    "    \"total_steps\": len(trainer.state.log_history) if trainer.state.log_history else 0,\n",
    "    \"training_rewards\": {\n",
    "        \"mean\": float(np.mean(training_rewards)) if training_rewards else 0.0,\n",
    "        \"std\": float(np.std(training_rewards)) if training_rewards else 0.0,\n",
    "        \"min\": float(np.min(training_rewards)) if training_rewards else 0.0,\n",
    "        \"max\": float(np.max(training_rewards)) if training_rewards else 0.0,\n",
    "        \"trend\": training_rewards\n",
    "    },\n",
    "    \"train_evaluation\": {\n",
    "        \"baseline_metrics\": train_eval_results[\"baseline_metrics\"],\n",
    "        \"policy_metrics\": train_eval_results[\"policy_metrics\"],\n",
    "        \"num_queries\": train_eval_results[\"num_queries\"],\n",
    "        \"positive_improvement_count\": train_eval_results[\"positive_improvement_count\"],\n",
    "        \"positive_improvement_ratio\": train_eval_results[\"positive_improvement_ratio\"],\n",
    "        \"parse_success_rate\": train_eval_results[\"parse_success_rate\"],\n",
    "        \"parse_success_count\": train_eval_results[\"parse_success_count\"]\n",
    "    },\n",
    "    \"eval_evaluation\": {\n",
    "        \"baseline_metrics\": eval_eval_results[\"baseline_metrics\"],\n",
    "        \"policy_metrics\": eval_eval_results[\"policy_metrics\"],\n",
    "        \"num_queries\": eval_eval_results[\"num_queries\"],\n",
    "        \"positive_improvement_count\": eval_eval_results[\"positive_improvement_count\"],\n",
    "        \"positive_improvement_ratio\": eval_eval_results[\"positive_improvement_ratio\"],\n",
    "        \"parse_success_rate\": eval_eval_results[\"parse_success_rate\"],\n",
    "        \"parse_success_count\": eval_eval_results[\"parse_success_count\"]\n",
    "    },\n",
    "    \"status\": \"completed\"\n",
    "}\n",
    "\n",
    "with open(METRICS_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"  ‚úÖ Metrics: {METRICS_FILE}\")\n",
    "print(\"\\n‚úÖ Minden artifact sikeresen mentve!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courtrankrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
