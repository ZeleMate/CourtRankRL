{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CourtRankRL GRPO Training - Runpod\n",
        "\n",
        "GRPO (Group Relative Policy Optimization) training a Qwen/Qwen3-4B-Instruct-2507 modellen.\n",
        "\n",
        "## Agents.md Specifikáció:\n",
        "- Qwen/Qwen3-4B-Instruct-2507 model QLoRA fine-tuninggal\n",
        "- Group size = slate length\n",
        "- NDCG@10 alapú reward shaping\n",
        "- Hungarian progress logging\n",
        "- Artifact export /workspace/artifacts/grpo_policy/ -ba\n",
        "\n",
        "## Előfeltételek:\n",
        "- A training slates JSONL exportálva legyen\n",
        "- HF token beállítva legyen\n",
        "- Megfelelő GPU memória (24GB+ ajánlott)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Install required packages\n",
        "os.system(\"pip install -q transformers peft trl torch bitsandbytes\")\n",
        "\n",
        "# Set paths\n",
        "ARTIFACTS_DIR = Path(\"/workspace/grpo_policy\")\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SLATE_FILE = Path(\"/workspace/training_slates.jsonl\")\n",
        "METRICS_FILE = ARTIFACTS_DIR / \"metrics.json\"\n",
        "\n",
        "print(\"Environment beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hungarian logging setup\n",
        "import logging\n",
        "\n",
        "class HungarianLogger:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(\"grpo_training\")\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        \n",
        "        handler = logging.StreamHandler()\n",
        "        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "        handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(handler)\n",
        "    \n",
        "    def info(self, msg):\n",
        "        self.logger.info(f\"[GRPO] {msg}\")\n",
        "    \n",
        "    def warning(self, msg):\n",
        "        self.logger.warning(f\"[GRPO FIGYELMEZTETÉS] {msg}\")\n",
        "    \n",
        "    def error(self, msg):\n",
        "        self.logger.error(f\"[GRPO HIBA] {msg}\")\n",
        "\n",
        "logger = HungarianLogger()\n",
        "logger.info(\"Hungarian logging beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (agents.md alapján)\n",
        "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "LORA_RANK = 64\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "GROUP_SIZE = 8  # Slate length\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_GRAD_NORM = 0.1\n",
        "WARMUP_RATIO = 0.1\n",
        "LOGGING_STEPS = 10\n",
        "SAVE_STEPS = 100\n",
        "EVAL_STEPS = 50\n",
        "MAX_STEPS = 1000\n",
        "\n",
        "REWARD_NDCG_K = 10\n",
        "REWARD_ENTROPY_BONUS = 0.01\n",
        "REWARD_CLAMP_NEGATIVE = True\n",
        "\n",
        "HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
        "\n",
        "logger.info(f\"Model: {MODEL_NAME}\")\n",
        "logger.info(f\"Group size: {GROUP_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading\n",
        "def load_training_data(slate_file: Path) -> List[Dict]:\n",
        "    \"\"\"Load training slates from JSONL file.\"\"\"\n",
        "    data = []\n",
        "    \n",
        "    if not slate_file.exists():\n",
        "        logger.error(f\"Slate fájl nem található: {slate_file}\")\n",
        "        return data\n",
        "    \n",
        "    with open(slate_file, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                slate = json.loads(line.strip())\n",
        "                data.append(slate)\n",
        "                \n",
        "                if line_num % 100 == 0:\n",
        "                    logger.info(f\"Betöltve: {line_num} slates\")\n",
        "                    \n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.warning(f\"Hibás JSON a {line_num}. sorban: {e}\")\n",
        "                continue\n",
        "    \n",
        "    logger.info(f\"Összesen betöltve: {len(data)} slates\")\n",
        "    return data\n",
        "\n",
        "training_data = load_training_data(SLATE_FILE)\n",
        "if not training_data:\n",
        "    logger.error(\"Nincs training adat - kilépés\")\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reward function (NDCG@10 based)\n",
        "def calculate_ndcg(relevance_scores: List[int], k: int = 10) -> float:\n",
        "    \"\"\"Calculate NDCG@k.\"\"\"\n",
        "    if not relevance_scores:\n",
        "        return 0.0\n",
        "    \n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(relevance_scores))):\n",
        "        rel = relevance_scores[i]\n",
        "        dcg += rel / (i + 1) ** 0.5\n",
        "    \n",
        "    # IDCG calculation\n",
        "    sorted_rel = sorted(relevance_scores, reverse=True)\n",
        "    idcg = 0.0\n",
        "    for i in range(min(k, len(sorted_rel))):\n",
        "        idcg += sorted_rel[i] / (i + 1) ** 0.5\n",
        "    \n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def reward_function(\n",
        "    completions: List[str], \n",
        "    slates: List[Dict], \n",
        "    **kwargs\n",
        ") -> List[float]:\n",
        "    \"\"\"Custom NDCG-based reward function.\"\"\"\n",
        "    rewards = []\n",
        "    \n",
        "    for completion, slate in zip(completions, slates):\n",
        "        try:\n",
        "            # Extract scores from model completion\n",
        "            lines = completion.strip().split('\\n')\n",
        "            scores = []\n",
        "            \n",
        "            for line in lines:\n",
        "                if line.strip() and any(char.isdigit() for char in line):\n",
        "                    # Extract numeric scores (simple parsing)\n",
        "                    parts = line.replace(',', '').split()\n",
        "                    for part in parts:\n",
        "                        try:\n",
        "                            score = float(part)\n",
        "                            if 0 <= score <= 10:  # Reasonable score range\n",
        "                                scores.append(score)\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "            \n",
        "            if len(scores) != len(slate['candidates']):\n",
        "                logger.warning(f\"Score szám mismatch: {len(scores)} vs {len(slate['candidates'])}\")\n",
        "                scores = [0.0] * len(slate['candidates'])\n",
        "            \n",
        "            # Calculate NDCG for predicted ranking\n",
        "            true_relevance = [cand['relevance'] for cand in slate['candidates']]\n",
        "            \n",
        "            # Get predicted ranking based on scores\n",
        "            predicted_ranking = sorted(\n",
        "                range(len(scores)), \n",
        "                key=lambda i: scores[i], \n",
        "                reverse=True\n",
        "            )\n",
        "            predicted_relevance = [true_relevance[i] for i in predicted_ranking]\n",
        "            \n",
        "            policy_ndcg = calculate_ndcg(predicted_relevance, REWARD_NDCG_K)\n",
        "            \n",
        "            # Baseline NDCG (random or original order)\n",
        "            baseline_ndcg = calculate_ndcg(true_relevance, REWARD_NDCG_K)\n",
        "            \n",
        "            # Reward = policy NDCG - baseline NDCG\n",
        "            reward = policy_ndcg - baseline_ndcg\n",
        "            \n",
        "            # Entropy bonus\n",
        "            if scores:\n",
        "                import numpy as np\n",
        "                scores_array = np.array(scores)\n",
        "                entropy = -np.sum(scores_array * np.log(scores_array + 1e-8))\n",
        "                reward += REWARD_ENTROPY_BONUS * entropy\n",
        "            \n",
        "            # Clamp negative rewards if configured\n",
        "            if REWARD_CLAMP_NEGATIVE and reward < 0:\n",
        "                reward = 0.0\n",
        "            \n",
        "            rewards.append(reward)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Reward calculation hiba: {e}\")\n",
        "            rewards.append(0.0)\n",
        "    \n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and tokenizer setup\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "logger.info(\"Model betöltése...\")\n",
        "\n",
        "# Quantization config for 4-bit loading\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    token=HF_TOKEN,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    token=HF_TOKEN,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "logger.info(\"Model és LoRA beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt template\n",
        "def create_prompt(slate: Dict) -> str:\n",
        "    \"\"\"Create Hungarian prompt for slate scoring.\"\"\"\n",
        "    query = slate['query_id']\n",
        "    candidates = slate['candidates']\n",
        "    \n",
        "    prompt = f\"\"\"A következő bírósági dokumentumokat kell rangsorolnod egy '{query}' keresési lekérdezéshez.\n",
        "\n",
        "Válaszolj minden dokumentumhoz egy 0-10 közötti relevancia pontszámmal (10 = nagyon releváns, 0 = nem releváns).\n",
        "\n",
        "Dokumentumok:\n",
        "\"\"\"\n",
        "    \n",
        "    for i, candidate in enumerate(candidates, 1):\n",
        "        # Use first chunk text as representative\n",
        "        text = candidate['chunks'][0]['text'][:500] + \"...\" if len(candidate['chunks'][0]['text']) > 500 else candidate['chunks'][0]['text']\n",
        "        \n",
        "        prompt += f\"\"\"\n",
        "{i}. Dokumentum (Bíróság: {candidate['chunks'][0]['metadata']['court']}, Év: {candidate['chunks'][0]['metadata']['year']})\n",
        "Szöveg: {text}\n",
        "\"\"\"\n",
        "    \n",
        "    prompt += \"\"\"\n",
        "Válaszolj minden dokumentumhoz egy sorban, a következő formátumban:\n",
        "1. [pontszám]\n",
        "2. [pontszám]\n",
        "...\n",
        "\n",
        "Pontszámok (0-10): \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# Test prompt\n",
        "if training_data:\n",
        "    test_prompt = create_prompt(training_data[0])\n",
        "    print(\"Prompt példa:\")\n",
        "    print(test_prompt[:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training dataset preparation\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SlateDataset(Dataset):\n",
        "    def __init__(self, data: List[Dict], tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        slate = self.data[idx]\n",
        "        prompt = create_prompt(slate)\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"slate\": slate\n",
        "        }\n",
        "\n",
        "dataset = SlateDataset(training_data, tokenizer)\n",
        "logger.info(f\"Dataset kész: {len(dataset)} példa\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Trainer setup\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=str(ARTIFACTS_DIR),\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=2,\n",
        "    group_size=GROUP_SIZE,\n",
        "    reward_funcs=reward_function,\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    config=grpo_config,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "logger.info(\"GRPO Trainer beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training execution\n",
        "logger.info(\"GRPO training indítása...\")\n",
        "logger.info(f\"Train dataset size: {len(dataset)}\")\n",
        "logger.info(f\"Estimated steps: {len(dataset) * NUM_TRAIN_EPOCHS // (1 * GRADIENT_ACCUMULATION_STEPS)}\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    logger.info(\"Training sikeresen befejezve\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training hiba: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and metrics\n",
        "logger.info(\"Model és metrikák mentése...\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "trainer.save_model(str(ARTIFACTS_DIR))\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(str(ARTIFACTS_DIR))\n",
        "\n",
        "# Save training metrics\n",
        "metrics = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"training_samples\": len(dataset),\n",
        "    \"epochs\": NUM_TRAIN_EPOCHS,\n",
        "    \"group_size\": GROUP_SIZE,\n",
        "    \"final_loss\": trainer.state.log_history[-1].get(\"loss\", 0.0) if trainer.state.log_history else 0.0,\n",
        "    \"total_steps\": len(trainer.state.log_history) if trainer.state.log_history else 0,\n",
        "}\n",
        "\n",
        "with open(METRICS_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "logger.info(f\"Artifacts mentve: {ARTIFACTS_DIR}\")\n",
        "logger.info(f\"Metrikák mentve: {METRICS_FILE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "logger.info(\"=== TRAINING ÖSSZEGZÉS ===\")\n",
        "logger.info(f\"Model: {MODEL_NAME}\")\n",
        "logger.info(f\"Training samples: {len(dataset)}\")\n",
        "logger.info(f\"Final loss: {metrics['final_loss']:.4f}\")\n",
        "logger.info(f\"Total steps: {metrics['total_steps']}\")\n",
        "logger.info(f\"Artifacts: {ARTIFACTS_DIR}\")\n",
        "logger.info(\"GRPO training sikeresen befejezve!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
