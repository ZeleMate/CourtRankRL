{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRPO Training Notebook - CourtRankRL\n",
        "\n",
        "Ez a notebook a CourtRankRL GRPO alapú reranking modell tanítását végzi el RunPod környezetben.\n",
        "\n",
        "## Agents.md specifikáció alapján:\n",
        "- Qwen/Qwen3-4B-Instruct-2507 model QLoRA adapterekkel\n",
        "- TRL GRPOTrainer használata\n",
        "- NDCG@10 reward calculation\n",
        "- Hungarian status messages\n",
        "\n",
        "## Előfeltételek:\n",
        "- slate JSONL file (baseline candidate slates)\n",
        "- HF token beállítva\n",
        "- Megfelelő GPU (A100/H100 ajánlott)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers peft trl datasets accelerate bitsandbytes\n",
        "!pip install jsonlines numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "print(\"GRPO Training Notebook inicializálva\")\n",
        "print(f\"PyTorch verzió: {torch.__version__}\")\n",
        "print(f\"GPU elérhető: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - agents.md alapján\n",
        "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "SLATE_SIZE = 20\n",
        "MAX_STEPS = 1000\n",
        "SAVE_STEPS = 100\n",
        "LEARNING_RATE = 1e-5\n",
        "LORA_RANK = 64\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Paths\n",
        "SLATE_FILE = \"/workspace/data/grpo_slates.jsonl\"\n",
        "OUTPUT_DIR = \"/workspace/artifacts/grpo_policy\"\n",
        "METRICS_FILE = os.path.join(OUTPUT_DIR, \"metrics.json\")\n",
        "\n",
        "print(\"Konfiguráció betöltve:\")\n",
        "print(f\"- Model: {MODEL_NAME}\")\n",
        "print(f\"- Slate size: {SLATE_SIZE}\")\n",
        "print(f\"- Max steps: {MAX_STEPS}\")\n",
        "print(f\"- Output dir: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_slates_data(slate_file: str) -> List[Dict]:\n",
        "    \"\"\"Load slate data from JSONL file.\"\"\"\n",
        "    print(f\"Slate adatok betöltése: {slate_file}\")\n",
        "\n",
        "    slates = []\n",
        "    with open(slate_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            slates.append(json.loads(line.strip()))\n",
        "\n",
        "    print(f\"Betöltött slates: {len(slates)}\")\n",
        "    return slates\n",
        "\n",
        "def calculate_ndcg(ranked_indices: List[int], true_relevance: List[float], k: int = 10) -> float:\n",
        "    \"\"\"Calculate NDCG@k as per agents.md specification.\"\"\"\n",
        "    if not true_relevance or not ranked_indices:\n",
        "        return 0.0\n",
        "\n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(ranked_indices))):\n",
        "        if i < len(true_relevance):\n",
        "            rel = true_relevance[ranked_indices[i]]\n",
        "            dcg += rel / np.log2(i + 2)\n",
        "\n",
        "    sorted_rel = sorted(true_relevance, reverse=True)\n",
        "    idcg = 0.0\n",
        "    for i in range(min(k, len(sorted_rel))):\n",
        "        idcg += sorted_rel[i] / np.log2(i + 2)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def calculate_entropy(ranking: List[int]) -> float:\n",
        "    \"\"\"Calculate entropy of ranking distribution.\"\"\"\n",
        "    if not ranking:\n",
        "        return 0.0\n",
        "\n",
        "    counts = {}\n",
        "    for idx in ranking:\n",
        "        counts[idx] = counts.get(idx, 0) + 1\n",
        "\n",
        "    probs = [count / len(ranking) for count in counts.values()]\n",
        "    entropy = -sum(p * np.log(p) for p in probs if p > 0)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def parse_model_ranking(completion: str) -> List[int]:\n",
        "    \"\"\"Parse model completion to extract ranking.\"\"\"\n",
        "    try:\n",
        "        # Extract indices from completion (example: \"1,3,2,4,0\" -> [1,3,2,4,0])\n",
        "        numbers = [int(x.strip()) for x in completion.split(\",\") if x.strip().isdigit()]\n",
        "        return numbers[:SLATE_SIZE]\n",
        "    except:\n",
        "        return list(range(SLATE_SIZE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load slate data\n",
        "slates_data = load_slates_data(SLATE_FILE)\n",
        "\n",
        "# Convert to dataset format for GRPO trainer\n",
        "def create_training_prompt(query_id: str, slate: List[Dict]) -> str:\n",
        "    \"\"\"Create training prompt for GRPO.\"\"\"\n",
        "    slate_items = []\n",
        "    for i, candidate in enumerate(slate):\n",
        "        slate_items.append({\n",
        "            \"index\": i,\n",
        "            \"doc_id\": candidate.get(\"doc_id\", \"\"),\n",
        "            \"text\": candidate.get(\"text\", \"\")[:500],\n",
        "            \"court\": candidate.get(\"court\", \"\"),\n",
        "            \"domain\": candidate.get(\"domain\", \"\"),\n",
        "            \"year\": candidate.get(\"year\", 0),\n",
        "            \"bm25_score\": candidate.get(\"bm25_score\", 0.0),\n",
        "            \"faiss_score\": candidate.get(\"faiss_score\", 0.0),\n",
        "            \"rrf_score\": candidate.get(\"rrf_score\", 0.0)\n",
        "        })\n",
        "\n",
        "    context = {\n",
        "        \"query\": query_id,\n",
        "        \"slate\": slate_items,\n",
        "        \"slate_size\": len(slate_items)\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"Rangsorold a következő bírósági dokumentumokat relevancia szerint. Válaszolj számokkal vesszővel elválasztva (pl. '1,3,2,4,0').\n",
        "\n",
        "Bírósági dokumentumok:\n",
        "{json.dumps(context, ensure_ascii=False, indent=2)}\n",
        "\n",
        "Rangsorolás:\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Create dataset\n",
        "training_examples = []\n",
        "for slate_data in slates_data:\n",
        "    prompt = create_training_prompt(slate_data[\"query_id\"], slate_data[\"slate\"])\n",
        "    training_examples.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"slate_data\": slate_data[\"slate\"]\n",
        "    })\n",
        "\n",
        "dataset = Dataset.from_list(training_examples)\n",
        "print(f\"Dataset létrehozva: {len(dataset)} példa\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model and tokenizer\n",
        "print(\"Modell inicializálása...\")\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"Modell QLoRA adapterekkel inicializálva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define reward function\n",
        "def reward_function(completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Custom reward function for GRPO trainer.\n",
        "    Agents.md: nDCG@10 difference with entropy bonus and variance normalization.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for completion, slate_data in zip(completions, kwargs.get(\"slate_data\", [])):\n",
        "        try:\n",
        "            # Parse model output to get rankings\n",
        "            predicted_order = parse_model_ranking(completion)\n",
        "\n",
        "            true_relevance = [item.get(\"relevance\", 0) for item in slate_data]\n",
        "            baseline_order = list(range(len(slate_data)))\n",
        "\n",
        "            baseline_ndcg = calculate_ndcg(baseline_order, true_relevance, k=10)\n",
        "            policy_ndcg = calculate_ndcg(predicted_order, true_relevance, k=10)\n",
        "\n",
        "            # NDCG difference as reward\n",
        "            reward = policy_ndcg - baseline_ndcg\n",
        "\n",
        "            # Clamp negative rewards\n",
        "            reward = max(reward, -1.0)\n",
        "\n",
        "            # Entropy bonus\n",
        "            entropy = calculate_entropy(predicted_order)\n",
        "            reward += 0.01 * entropy\n",
        "\n",
        "            rewards.append(reward)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Hiba a reward számításakor: {e}\")\n",
        "            rewards.append(0.0)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "print(\"Reward function definiálva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure GRPO trainer\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_generations=4,\n",
        "    max_steps=MAX_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=10,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=100,\n",
        "    max_completion_length=256,\n",
        "    group_size=SLATE_SIZE,\n",
        "    kl_penalty=\"none\",  # Disabled as per agents.md\n",
        ")\n",
        "\n",
        "print(\"GRPO konfiguráció kész\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train GRPO trainer\n",
        "print(\"GRPO trainer inicializálása...\")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=reward_function,\n",
        "    args=grpo_config,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "print(\"GRPO training indítása...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training befejezve\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save artifacts\n",
        "print(\"Artifactumok mentése...\")\n",
        "\n",
        "# Create output directory\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save training metrics\n",
        "metrics = {\n",
        "    \"training_samples\": len(dataset),\n",
        "    \"slate_size\": SLATE_SIZE,\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"max_steps\": MAX_STEPS,\n",
        "    \"status\": \"completed\"\n",
        "}\n",
        "\n",
        "with open(METRICS_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Artifactumok mentve: {OUTPUT_DIR}\")\n",
        "print(f\"Metrikák: {METRICS_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Complete!\n",
        "\n",
        "A GRPO training befejeződött. Az artifactumok a `/workspace/artifacts/grpo_policy/` könyvtárban találhatók:\n",
        "\n",
        "- `adapter_model.bin` - LoRA adapter weights\n",
        "- `tokenizer.json` - Tokenizer configuration\n",
        "- `metrics.json` - Training metrics\n",
        "\n",
        "Ezeket az artifactumokat le kell tölteni a helyi gépre a `data/models/grpo_policy/` könyvtárba.\n",
        "\n",
        "## Next Steps:\n",
        "1. Artifactumok letöltése\n",
        "2. Helyi tesztelés: `python -m src.search.grpo_reranker`\n",
        "3. Query pipeline integráció\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CourtRankRL GRPO Training - Runpod\n",
        "\n",
        "GRPO (Group Relative Policy Optimization) training a Qwen/Qwen3-4B-Instruct-2507 modellen.\n",
        "\n",
        "## Agents.md Specifikáció:\n",
        "- Qwen/Qwen3-4B-Instruct-2507 model QLoRA fine-tuninggal\n",
        "- Group size = slate length\n",
        "- NDCG@10 alapú reward shaping\n",
        "- Hungarian progress logging\n",
        "- Artifact export /workspace/artifacts/grpo_policy/ -ba\n",
        "\n",
        "## Előfeltételek:\n",
        "- A training slates JSONL exportálva legyen\n",
        "- HF token beállítva legyen\n",
        "- Megfelelő GPU memória (24GB+ ajánlott)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Install required packages\n",
        "os.system(\"pip install -q transformers peft trl torch bitsandbytes\")\n",
        "\n",
        "# Set paths\n",
        "ARTIFACTS_DIR = Path(\"/workspace/grpo_policy\")\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SLATE_FILE = Path(\"/workspace/training_slates.jsonl\")\n",
        "METRICS_FILE = ARTIFACTS_DIR / \"metrics.json\"\n",
        "\n",
        "print(\"Environment beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hungarian logging setup\n",
        "import logging\n",
        "\n",
        "class HungarianLogger:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(\"grpo_training\")\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        \n",
        "        handler = logging.StreamHandler()\n",
        "        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "        handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(handler)\n",
        "    \n",
        "    def info(self, msg):\n",
        "        self.logger.info(f\"[GRPO] {msg}\")\n",
        "    \n",
        "    def warning(self, msg):\n",
        "        self.logger.warning(f\"[GRPO FIGYELMEZTETÉS] {msg}\")\n",
        "    \n",
        "    def error(self, msg):\n",
        "        self.logger.error(f\"[GRPO HIBA] {msg}\")\n",
        "\n",
        "logger = HungarianLogger()\n",
        "logger.info(\"Hungarian logging beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (agents.md alapján)\n",
        "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "LORA_RANK = 64\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "GROUP_SIZE = 8  # Slate length\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_GRAD_NORM = 0.1\n",
        "WARMUP_RATIO = 0.1\n",
        "LOGGING_STEPS = 10\n",
        "SAVE_STEPS = 100\n",
        "EVAL_STEPS = 50\n",
        "MAX_STEPS = 1000\n",
        "\n",
        "REWARD_NDCG_K = 10\n",
        "REWARD_ENTROPY_BONUS = 0.01\n",
        "REWARD_CLAMP_NEGATIVE = True\n",
        "\n",
        "HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
        "\n",
        "logger.info(f\"Model: {MODEL_NAME}\")\n",
        "logger.info(f\"Group size: {GROUP_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading\n",
        "def load_training_data(slate_file: Path) -> List[Dict]:\n",
        "    \"\"\"Load training slates from JSONL file.\"\"\"\n",
        "    data = []\n",
        "    \n",
        "    if not slate_file.exists():\n",
        "        logger.error(f\"Slate fájl nem található: {slate_file}\")\n",
        "        return data\n",
        "    \n",
        "    with open(slate_file, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                slate = json.loads(line.strip())\n",
        "                data.append(slate)\n",
        "                \n",
        "                if line_num % 100 == 0:\n",
        "                    logger.info(f\"Betöltve: {line_num} slates\")\n",
        "                    \n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.warning(f\"Hibás JSON a {line_num}. sorban: {e}\")\n",
        "                continue\n",
        "    \n",
        "    logger.info(f\"Összesen betöltve: {len(data)} slates\")\n",
        "    return data\n",
        "\n",
        "training_data = load_training_data(SLATE_FILE)\n",
        "if not training_data:\n",
        "    logger.error(\"Nincs training adat - kilépés\")\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reward function (NDCG@10 based)\n",
        "def calculate_ndcg(relevance_scores: List[int], k: int = 10) -> float:\n",
        "    \"\"\"Calculate NDCG@k.\"\"\"\n",
        "    if not relevance_scores:\n",
        "        return 0.0\n",
        "    \n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(relevance_scores))):\n",
        "        rel = relevance_scores[i]\n",
        "        dcg += rel / (i + 1) ** 0.5\n",
        "    \n",
        "    # IDCG calculation\n",
        "    sorted_rel = sorted(relevance_scores, reverse=True)\n",
        "    idcg = 0.0\n",
        "    for i in range(min(k, len(sorted_rel))):\n",
        "        idcg += sorted_rel[i] / (i + 1) ** 0.5\n",
        "    \n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def reward_function(\n",
        "    completions: List[str], \n",
        "    slates: List[Dict], \n",
        "    **kwargs\n",
        ") -> List[float]:\n",
        "    \"\"\"Custom NDCG-based reward function.\"\"\"\n",
        "    rewards = []\n",
        "    \n",
        "    for completion, slate in zip(completions, slates):\n",
        "        try:\n",
        "            # Extract scores from model completion\n",
        "            lines = completion.strip().split('\\n')\n",
        "            scores = []\n",
        "            \n",
        "            for line in lines:\n",
        "                if line.strip() and any(char.isdigit() for char in line):\n",
        "                    # Extract numeric scores (simple parsing)\n",
        "                    parts = line.replace(',', '').split()\n",
        "                    for part in parts:\n",
        "                        try:\n",
        "                            score = float(part)\n",
        "                            if 0 <= score <= 10:  # Reasonable score range\n",
        "                                scores.append(score)\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "            \n",
        "            if len(scores) != len(slate['candidates']):\n",
        "                logger.warning(f\"Score szám mismatch: {len(scores)} vs {len(slate['candidates'])}\")\n",
        "                scores = [0.0] * len(slate['candidates'])\n",
        "            \n",
        "            # Calculate NDCG for predicted ranking\n",
        "            true_relevance = [cand['relevance'] for cand in slate['candidates']]\n",
        "            \n",
        "            # Get predicted ranking based on scores\n",
        "            predicted_ranking = sorted(\n",
        "                range(len(scores)), \n",
        "                key=lambda i: scores[i], \n",
        "                reverse=True\n",
        "            )\n",
        "            predicted_relevance = [true_relevance[i] for i in predicted_ranking]\n",
        "            \n",
        "            policy_ndcg = calculate_ndcg(predicted_relevance, REWARD_NDCG_K)\n",
        "            \n",
        "            # Baseline NDCG (random or original order)\n",
        "            baseline_ndcg = calculate_ndcg(true_relevance, REWARD_NDCG_K)\n",
        "            \n",
        "            # Reward = policy NDCG - baseline NDCG\n",
        "            reward = policy_ndcg - baseline_ndcg\n",
        "            \n",
        "            # Entropy bonus\n",
        "            if scores:\n",
        "                import numpy as np\n",
        "                scores_array = np.array(scores)\n",
        "                entropy = -np.sum(scores_array * np.log(scores_array + 1e-8))\n",
        "                reward += REWARD_ENTROPY_BONUS * entropy\n",
        "            \n",
        "            # Clamp negative rewards if configured\n",
        "            if REWARD_CLAMP_NEGATIVE and reward < 0:\n",
        "                reward = 0.0\n",
        "            \n",
        "            rewards.append(reward)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Reward calculation hiba: {e}\")\n",
        "            rewards.append(0.0)\n",
        "    \n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and tokenizer setup\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "logger.info(\"Model betöltése...\")\n",
        "\n",
        "# Quantization config for 4-bit loading\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    token=HF_TOKEN,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    token=HF_TOKEN,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "logger.info(\"Model és LoRA beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt template\n",
        "def create_prompt(slate: Dict) -> str:\n",
        "    \"\"\"Create Hungarian prompt for slate scoring.\"\"\"\n",
        "    query = slate['query_id']\n",
        "    candidates = slate['candidates']\n",
        "    \n",
        "    prompt = f\"\"\"A következő bírósági dokumentumokat kell rangsorolnod egy '{query}' keresési lekérdezéshez.\n",
        "\n",
        "Válaszolj minden dokumentumhoz egy 0-10 közötti relevancia pontszámmal (10 = nagyon releváns, 0 = nem releváns).\n",
        "\n",
        "Dokumentumok:\n",
        "\"\"\"\n",
        "    \n",
        "    for i, candidate in enumerate(candidates, 1):\n",
        "        # Use first chunk text as representative\n",
        "        text = candidate['chunks'][0]['text'][:500] + \"...\" if len(candidate['chunks'][0]['text']) > 500 else candidate['chunks'][0]['text']\n",
        "        \n",
        "        prompt += f\"\"\"\n",
        "{i}. Dokumentum (Bíróság: {candidate['chunks'][0]['metadata']['court']}, Év: {candidate['chunks'][0]['metadata']['year']})\n",
        "Szöveg: {text}\n",
        "\"\"\"\n",
        "    \n",
        "    prompt += \"\"\"\n",
        "Válaszolj minden dokumentumhoz egy sorban, a következő formátumban:\n",
        "1. [pontszám]\n",
        "2. [pontszám]\n",
        "...\n",
        "\n",
        "Pontszámok (0-10): \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# Test prompt\n",
        "if training_data:\n",
        "    test_prompt = create_prompt(training_data[0])\n",
        "    print(\"Prompt példa:\")\n",
        "    print(test_prompt[:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training dataset preparation\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SlateDataset(Dataset):\n",
        "    def __init__(self, data: List[Dict], tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        slate = self.data[idx]\n",
        "        prompt = create_prompt(slate)\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"slate\": slate\n",
        "        }\n",
        "\n",
        "dataset = SlateDataset(training_data, tokenizer)\n",
        "logger.info(f\"Dataset kész: {len(dataset)} példa\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Trainer setup\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=str(ARTIFACTS_DIR),\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=2,\n",
        "    group_size=GROUP_SIZE,\n",
        "    reward_funcs=reward_function,\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    config=grpo_config,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "logger.info(\"GRPO Trainer beállítva\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training execution\n",
        "logger.info(\"GRPO training indítása...\")\n",
        "logger.info(f\"Train dataset size: {len(dataset)}\")\n",
        "logger.info(f\"Estimated steps: {len(dataset) * NUM_TRAIN_EPOCHS // (1 * GRADIENT_ACCUMULATION_STEPS)}\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    logger.info(\"Training sikeresen befejezve\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training hiba: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and metrics\n",
        "logger.info(\"Model és metrikák mentése...\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "trainer.save_model(str(ARTIFACTS_DIR))\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(str(ARTIFACTS_DIR))\n",
        "\n",
        "# Save training metrics\n",
        "metrics = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"training_samples\": len(dataset),\n",
        "    \"epochs\": NUM_TRAIN_EPOCHS,\n",
        "    \"group_size\": GROUP_SIZE,\n",
        "    \"final_loss\": trainer.state.log_history[-1].get(\"loss\", 0.0) if trainer.state.log_history else 0.0,\n",
        "    \"total_steps\": len(trainer.state.log_history) if trainer.state.log_history else 0,\n",
        "}\n",
        "\n",
        "with open(METRICS_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "logger.info(f\"Artifacts mentve: {ARTIFACTS_DIR}\")\n",
        "logger.info(f\"Metrikák mentve: {METRICS_FILE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "logger.info(\"=== TRAINING ÖSSZEGZÉS ===\")\n",
        "logger.info(f\"Model: {MODEL_NAME}\")\n",
        "logger.info(f\"Training samples: {len(dataset)}\")\n",
        "logger.info(f\"Final loss: {metrics['final_loss']:.4f}\")\n",
        "logger.info(f\"Total steps: {metrics['total_steps']}\")\n",
        "logger.info(f\"Artifacts: {ARTIFACTS_DIR}\")\n",
        "logger.info(\"GRPO training sikeresen befejezve!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
