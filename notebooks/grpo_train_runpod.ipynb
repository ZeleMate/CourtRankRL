{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CourtRankRL GRPO Training - Chunk-Based, A100 Memory Efficient\n",
    "\n",
    "## Agents.md Specifik√°ci√≥ (Chunk-Based)\n",
    "\n",
    "Ez a notebook a CourtRankRL GRPO alap√∫ reranking modell tan√≠t√°s√°t v√©gzi el **A100 GPU-n** (80GB VRAM, memory-constrained).\n",
    "\n",
    "### F≈ëbb jellemz≈ëk (Chunk-Based megold√°s):\n",
    "- **Model**: Qwen/Qwen3-4B-Instruct-2507 (4-bit) + QLoRA (rank=32, alpha=64)\n",
    "- **Training**: TRL GRPOTrainer GRPO algoritmussal\n",
    "  - Loss: \"dapo\" (eliminates length bias)\n",
    "  - Reward scaling: \"batch\" (robust - PPO Lite)\n",
    "  - Importance sampling: \"sequence\" (stable - GSPO)\n",
    "- **Dataset**: 98 query (teljes), 20 chunk/slate, **TELJES chunk sz√∂veg** (~500-800 char)\n",
    "- **Slate strat√©gia**: Chunk-level retrieval (nem doc aggreg√°ci√≥!) ‚Üí legrelev√°nsabb chunk-ok\n",
    "- **Baseline**: Slate sorrendje = fusion ranking [0,1,2,...] (BM25+FAISS fusion szerint)\n",
    "- **Hardware**: Batch size 2, grad accumulation 6, 6 generations/prompt (MEMORY EFFICIENT)\n",
    "- **Training time**: ~25-35 perc (600 steps, vLLM-mel, cs√∂kkentett gen miatt)\n",
    "\n",
    "### Mi√©rt chunk-based?\n",
    "- ‚úÖ **Relev√°ns kontextus**: BM25+FAISS m√°r kiv√°lasztotta a legrelev√°nsabb chunk-okat\n",
    "- ‚úÖ **Teljes sz√∂veg**: A model l√°tja, MI√âRT relev√°ns egy dokumentum\n",
    "- ‚úÖ **Jobb tanul√°s**: A model megtanulja √©rt√©kelni a val√≥di tartalmat, nem csak metaadatokat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K√∂rnyezet setup √©s csomagok telep√≠t√©se\n",
    "# KRITIKUS: Kompatibilis verzi√≥k telep√≠t√©se (Unsloth + TRL + dependencies)\n",
    "\n",
    "# 1. Core dependencies EL≈êSZ√ñR (stabil verzi√≥k)\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install -q transformers datasets huggingface_hub\n",
    "%pip install -q numpy scipy scikit-learn\n",
    "\n",
    "# 2. Accelerate - FIX√ÅLT VERZI√ì az is_fsdp2 kompatibilit√°s miatt\n",
    "# CRITICAL: accelerate>=0.30.0 sz√ºks√©ges az is_fsdp2 attrib√∫tumhoz, de kompatibilisnek kell lennie Unsloth-tal\n",
    "%pip install -q \"accelerate>=0.30.0\" --upgrade\n",
    "\n",
    "# 3. LoRA √©s kvantiz√°ci√≥\n",
    "%pip install -q peft bitsandbytes\n",
    "\n",
    "# 4. TRL (legfrissebb stabil verzi√≥ - GRPO t√°mogat√°ssal)\n",
    "%pip install -q --upgrade trl\n",
    "\n",
    "# 5. Unsloth telep√≠t√©se UTOLJ√ÅRA (hogy a TRL-hez igazodjon)\n",
    "%pip install -q --upgrade unsloth\n",
    "\n",
    "# 6. vLLM (opcion√°lis, de aj√°nlott fast_inference-hez)\n",
    "%pip install -q vllm\n",
    "\n",
    "# 7. Egy√©b f√ºgg≈ës√©gek\n",
    "%pip install -q --upgrade typing_extensions ranx\n",
    "\n",
    "print(\"‚úÖ Csomagok telep√≠tve (kompatibilis verzi√≥k)\")\n",
    "print(\"üì¶ Telep√≠tett verzi√≥k ellen≈ërz√©se:\")\n",
    "import unsloth, torch, transformers, trl, accelerate\n",
    "print(f\"  - PyTorch: {torch.__version__}\")\n",
    "print(f\"  - Transformers: {transformers.__version__}\")\n",
    "print(f\"  - Accelerate: {accelerate.__version__} (CRITICAL: >=0.30.0 az is_fsdp2 t√°mogat√°shoz)\")\n",
    "print(f\"  - TRL: {trl.__version__}\")\n",
    "print(f\"  - Unsloth: {unsloth.__version__}\")\n",
    "\n",
    "# FONTOS FIGYELMEZTET√âS a verzi√≥r√≥l\n",
    "if tuple(map(int, accelerate.__version__.split('.')[:2])) < (0, 30):\n",
    "    print(f\"‚ö†Ô∏è  FIGYELEM: Accelerate verzi√≥ ({accelerate.__version__}) < 0.30.0\")\n",
    "    print(f\"   A monkey patch fog futni a training sor√°n az is_fsdp2 kompatibilit√°s√©rt\")\n",
    "    print(f\"   Aj√°nlott: pip install accelerate>=0.30.0 --upgrade && RESTART KERNEL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importok\n",
    "import os, warnings\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"      # hivatalos flag a dynamo kikapcsol√°s√°ra\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # √°rtalmatlan, de seg√≠t fallbackben\n",
    "# (opcion√°lis, ha Unsloth figyeli)\n",
    "os.environ[\"UNSLOTH_COMPILE\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*An output with one or more elements was resized.*\")\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl.trainer.grpo_trainer import GRPOTrainer\n",
    "from trl.trainer.grpo_config import GRPOConfig\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ranx import Qrels, Run, evaluate\n",
    "\n",
    "print(\"‚úÖ Importok bet√∂ltve (Unsloth + TRL + sklearn + scipy + ranx)\")\n",
    "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
    "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL FIX: Accelerate is_fsdp2 kompatibilit√°si patch\n",
    "# Ha a jelenlegi Accelerate verzi√≥ nem t√°mogatja az is_fsdp2 attrib√∫tumot, patchelj√ºk\n",
    "try:\n",
    "    from accelerate import Accelerator\n",
    "    from accelerate.state import AcceleratorState\n",
    "    \n",
    "    # Ellen≈ërizz√ºk, hogy l√©tezik-e az is_fsdp2 property\n",
    "    test_state = AcceleratorState()\n",
    "    _ = test_state.is_fsdp2\n",
    "    print(\"‚úÖ Accelerate verzi√≥ t√°mogatja az is_fsdp2-t\")\n",
    "except AttributeError:\n",
    "    print(\"‚ö†Ô∏è Accelerate verzi√≥ NEM t√°mogatja az is_fsdp2-t, monkey patch alkalmaz√°sa...\")\n",
    "    \n",
    "    # Monkey patch: adjuk hozz√° az is_fsdp2 property-t az AcceleratorState-hez\n",
    "    def is_fsdp2_property(self):\n",
    "        \"\"\"Visszakompatibilit√°si patch: is_fsdp2 mindig False r√©gebbi Accelerate verzi√≥kban\"\"\"\n",
    "        return False\n",
    "    \n",
    "    # Property hozz√°ad√°sa az oszt√°lyhoz\n",
    "    AcceleratorState.is_fsdp2 = property(is_fsdp2_property)\n",
    "    \n",
    "    # Ellen≈ërz√©s\n",
    "    test_state = AcceleratorState()\n",
    "    assert hasattr(test_state, 'is_fsdp2'), \"Patch sikertelen!\"\n",
    "    assert test_state.is_fsdp2 == False, \"Patch helytelen √©rt√©ket ad!\"\n",
    "    print(\"‚úÖ Monkey patch sikeresen alkalmazva - is_fsdp2 = False\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå V√°ratlan hiba az is_fsdp2 ellen≈ërz√©se sor√°n: {e}\")\n",
    "    print(\"‚ö†Ô∏è Folytat√°s patch n√©lk√ºl - ha training hiba lesz, telep√≠tsd √∫jra: pip install accelerate>=0.30.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace bejelentkez√©s\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ HuggingFace bejelentkez√©s sikeres\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nincs HUGGINGFACE_TOKEN, a modell let√∂lt√©se korl√°tozott lehet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigur√°ci√≥ - MEMORY EFFICIENT (A100 80GB) + CURRICULUM LEARNING\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Instruct-2507\"\n",
    "SLATE_SIZE = 30  # Enhanced: 20 ‚Üí 30 (more documents per slate, better quality)\n",
    "\n",
    "# Curriculum Learning konfigur√°ci√≥\n",
    "CURRICULUM_ENABLED = True  # Enged√©lyezve - LEGNAGYOBB JAVUL√ÅSI POTENCI√ÅL\n",
    "CURRICULUM_STRATEGY = \"weighted_sampling\"  # \"filtered_dataset\" vagy \"weighted_sampling\"\n",
    "# weighted_sampling: k√∂nnyebb p√©ld√°kat gyakrabban v√°lasztjuk ki\n",
    "# Ez biztos√≠tja, hogy a modell fokozatosan megtanulja j√≥l rangsorolni\n",
    "GROUP_SIZE = 8\n",
    "LORA_RANK = 32  # 64‚Üí32: cs√∂kkentett rank a mem√≥ria miatt\n",
    "LORA_ALPHA = 64  # 128‚Üí64: alpha k√∂veti a rank-ot\n",
    "LORA_DROPOUT = 0.05\n",
    "MAX_SEQ_LENGTH = 16384  # 8192‚Üí12288: n√∂velt √©rt√©k a hosszabb promptokhoz (9600 token + marg√≥)\n",
    "GPU_MEMORY_UTILIZATION = 0.5  # 0.88‚Üí0.5: DRASTIKUSAN cs√∂kkentve a rendelkez√©sre √°ll√≥ szabad mem√≥ria miatt\n",
    "USE_GRADIENT_CHECKPOINTING = \"unsloth\"\n",
    "\n",
    "LEARNING_RATE = 1e-4  # Enhanced: 5e-5 ‚Üí 1e-4 (larger steps, better convergence)\n",
    "MAX_STEPS = 1000  # Enhanced: 600 ‚Üí 1000 (more epochs, but with early stopping if needed)\n",
    "SAVE_STEPS = 200  # Save more frequently for checkpoint recovery\n",
    "EVAL_STEPS = 50\n",
    "LOGGING_STEPS = 10\n",
    "WARMUP_STEPS = 100  # Enhanced: 50 ‚Üí 100 (gentler warmup)\n",
    "GRADIENT_ACCUMULATION_STEPS = 6  # 3‚Üí6: nagyobb accumulation, kisebb batch\n",
    "NUM_GENERATIONS = 6  # 14‚Üí6: cs√∂kkentett gener√°ci√≥k\n",
    "PER_DEVICE_BATCH_SIZE = 2  # 4‚Üí2: FEL√âRE cs√∂kkentve\n",
    "# KRITIKUS: generation_batch_size-nak oszthat√≥nak KELL lennie:\n",
    "#   1. NUM_GENERATIONS-szel (6)\n",
    "#   2. PER_DEVICE_BATCH_SIZE-zal (2)\n",
    "# Legkisebb k√∂z√∂s t√∂bbsz√∂r√∂s (LCM): 6\n",
    "GENERATION_BATCH_SIZE = 6  # 6√∑6=1, 6√∑2=3 ‚úì\n",
    "OPTIMIZER_NAME = \"paged_adamw_8bit\"\n",
    "LR_SCHEDULER_TYPE = \"linear\"  # Enhanced: cosine ‚Üí linear (more stable final convergence)\n",
    "\n",
    "NDCG_K = 10\n",
    "ENTROPY_BONUS = 0.01\n",
    "# Reward clipping range (updated in reward function)\n",
    "REWARD_CLIP_MIN = -1.0  # Will be updated in enhanced reward function\n",
    "REWARD_CLIP_MAX = 2.0   # Enhanced: wider range for positive improvements\n",
    "TRAIN_SPLIT = 0.8\n",
    "SEED = 42\n",
    "\n",
    "BASE_PATH = Path(os.getenv(\"WORKSPACE_PATH\", \"/workspace\"))\n",
    "\n",
    "# HuggingFace token defini√°l√°sa (HF_TOKEN = hf_token)\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "HF_TOKEN = hf_token\n",
    "\n",
    "BASELINE_METRICS_PATH = BASE_PATH / \"baseline_query_metrics.csv\"\n",
    "SLATE_FILE = BASE_PATH / \"training_slates.jsonl\"\n",
    "OUTPUT_DIR = BASE_PATH / \"artifacts\" / \"grpo_policy\"\n",
    "METRICS_FILE = OUTPUT_DIR / \"metrics.json\"\n",
    "\n",
    "print(\"üìã A100 80GB - Memory Efficient Konfigur√°ci√≥:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA Rank: {LORA_RANK} (cs√∂kkentve mem√≥ria miatt)\")\n",
    "print(f\"  GPU Memory Utilization: {GPU_MEMORY_UTILIZATION} (0.5 = 50%)\")\n",
    "print(f\"  Batch: {PER_DEVICE_BATCH_SIZE} √ó {GRADIENT_ACCUMULATION_STEPS} = {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} (effective)\")\n",
    "print(f\"  Generations: {NUM_GENERATIONS}, Generation batch: {GENERATION_BATCH_SIZE}\")\n",
    "print(f\"  Steps per generation: {GENERATION_BATCH_SIZE // PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"  Steps: {MAX_STEPS}\")\n",
    "print(f\"  ‚ö†Ô∏è  Mem√≥ria optimaliz√°lt be√°ll√≠t√°sok akt√≠vak!\")\n",
    "\n",
    "if not SLATE_FILE.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Slate f√°jl nem tal√°lhat√≥: {SLATE_FILE}\")\n",
    "\n",
    "if not BASELINE_METRICS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Baseline metrik√°k f√°jl nem tal√°lhat√≥: {BASELINE_METRICS_PATH}\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU mem√≥ria tiszt√≠t√°sa\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# CUDA cache tiszt√≠t√°sa\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ CUDA cache t√∂r√∂lve\")\n",
    "    \n",
    "    # Mem√≥ria st√°tusz\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    free = (total - allocated)\n",
    "    \n",
    "    print(f\"\\nüìä GPU Mem√≥ria St√°tusz (A100 80GB):\")\n",
    "    print(f\"  Total: {total:.2f} GB\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Free: {free:.2f} GB\")\n",
    "    print(f\"  Free %: {(free/total)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seg√©df√ºggv√©nyek\n",
    "def calculate_ndcg(ranked_indices: List[int], true_relevance: List[float], k: int = 10) -> float:\n",
    "    if not true_relevance or not ranked_indices or max(true_relevance) == 0:\n",
    "        return 0.0\n",
    "    y_true = np.array(true_relevance)\n",
    "    max_score = len(ranked_indices)\n",
    "    y_score = np.zeros_like(y_true, dtype=float)\n",
    "    for i, idx in enumerate(ranked_indices[:k]):\n",
    "        if idx < len(y_true):\n",
    "            y_score[idx] = max_score - i\n",
    "    if np.sum(y_score) == 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(ndcg_score(y_true.reshape(1, -1), y_score.reshape(1, -1), k=k))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def parse_model_ranking(completion: str, slate_size: int = SLATE_SIZE) -> List[int]:\n",
    "    \"\"\"\n",
    "    Megenged≈ëbb parsing - kev√©sb√© szigor√∫ form√°tum k√∂vetelm√©ny.\n",
    "    Ha nincs el√©g valid index, kieg√©sz√≠tj√ºk a hi√°nyz√≥kat a baseline order szerint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pr√≥b√°lunk sz√°mokat kinyerni k√ºl√∂nb√∂z≈ë m√≥don\n",
    "        numbers = []\n",
    "        \n",
    "        # 1. Vessz≈ëvel elv√°lasztott sz√°mok\n",
    "        parts = completion.split(\",\")\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            # K√©rj√ºk el minden sz√°mot (ak√°r t√∂bb sz√°m egy cell√°ban)\n",
    "            for word in part.split():\n",
    "                word = word.strip().rstrip(\".,;\")\n",
    "                if word.isdigit():\n",
    "                    num = int(word)\n",
    "                    if 0 <= num < slate_size:\n",
    "                        numbers.append(num)\n",
    "        \n",
    "        # 2. Ha nincs el√©g, pr√≥b√°lunk sz√≥k√∂zzel elv√°lasztott form√°tumot\n",
    "        if len(numbers) < slate_size // 3:\n",
    "            for word in completion.split():\n",
    "                word = word.strip().rstrip(\".,;\")\n",
    "                if word.isdigit():\n",
    "                    num = int(word)\n",
    "                    if 0 <= num < slate_size and num not in numbers:\n",
    "                        numbers.append(num)\n",
    "        \n",
    "        # 3. Valid sz√°mok gy≈±jt√©se\n",
    "        valid_numbers = []\n",
    "        seen = set()\n",
    "        for n in numbers:\n",
    "            if n not in seen:\n",
    "                valid_numbers.append(n)\n",
    "                seen.add(n)\n",
    "        \n",
    "        # 4. Ha van el√©g valid sz√°m (legal√°bb 1/3-a a slate-nek), haszn√°ljuk\n",
    "        if len(valid_numbers) >= max(1, slate_size // 3):\n",
    "            # Kieg√©sz√≠tj√ºk a hi√°nyz√≥ indexeket a baseline order szerint\n",
    "            missing_indices = [i for i in range(slate_size) if i not in valid_numbers]\n",
    "            # A hi√°nyz√≥kat a v√©g√©re tesz√ºk (nem zavarjuk meg a modell rankingj√©t)\n",
    "            result = valid_numbers + missing_indices\n",
    "            return result[:slate_size]\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: baseline order (nem random shuffle - √≠gy jobb mint teljesen random)\n",
    "    return list(range(slate_size))\n",
    "\n",
    "def create_training_prompt(query_id: str, slate: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced prompt generation for GRPO training with explicit relevance labels and few-shot examples.\n",
    "    The prompt uses explicit instructions in ENGLISH to help the model learn proper relevance-based ranking.\n",
    "    Only the data content (queries, document texts, metadata) remains in HUNGARIAN as it comes from the dataset.\n",
    "    \n",
    "    Improvements:\n",
    "    1. Explicit relevance labels (0/1/2) for each document\n",
    "    2. Few-shot example ranking output\n",
    "    3. More detailed relevance criteria\n",
    "    4. Clearer output format specification\n",
    "    \"\"\"\n",
    "    prompt = f'''# Legal Document Relevance Assessment and Ranking\n",
    "\n",
    "## TASK\n",
    "You are helping to rank Hungarian court decision documents by their relevance to a search query.\n",
    "\n",
    "## SEARCH QUERY\n",
    "\"{query_id}\"\n",
    "\n",
    "## WHAT TO CONSIDER\n",
    "\n",
    "When ranking documents, think about:\n",
    "- How well the document topic matches the query\n",
    "- Whether the content directly addresses what the query is asking\n",
    "- The legal domain and court type (may or may not be relevant)\n",
    "- The relevance labels (0=not relevant, 1=relevant, 2=highly relevant) - these can guide you\n",
    "- BM25 and FAISS scores show some similarity signals, but content is most important\n",
    "\n",
    "## DOCUMENT EVALUATION\n",
    "\n",
    "You need to evaluate the following {len(slate)} document excerpts:\n",
    "\n",
    "'''\n",
    "    \n",
    "    separator_line = \"\\u2500\" * 63\n",
    "    \n",
    "    # Enhanced document presentation with EXPLICIT RELEVANCE LABELS\n",
    "    for idx, doc in enumerate(slate):\n",
    "        chunk_text = doc.get('text', '')[:800]\n",
    "        bm25 = doc.get('bm25_score', 0)\n",
    "        faiss = doc.get('faiss_score', 0)\n",
    "        relevance = doc.get('relevance', 0)\n",
    "        court = doc.get('court', 'N/A')\n",
    "        domain = doc.get('domain', 'N/A')\n",
    "        year = doc.get('year', 'N/A')\n",
    "        \n",
    "        # Relevance label mapping\n",
    "        rel_label = \"NOT RELEVANT\" if relevance == 0 else (\"RELEVANT\" if relevance == 1 else \"HIGHLY RELEVANT\")\n",
    "        \n",
    "        prompt += f'''{separator_line}\n",
    "DOCUMENT [{idx}]\n",
    "\n",
    "Relevance: {rel_label} (Grade {relevance}/2)\n",
    "ID: {doc.get('doc_id', 'N/A')} | Court: {court} | Domain: {domain} | Year: {year}\n",
    "Scores: BM25={bm25:.2f}, FAISS={faiss:.3f}\n",
    "\n",
    "Content:\n",
    "{chunk_text}\n",
    "\n",
    "'''\n",
    "    \n",
    "    # Strict ranking instructions to force tangible outputs\n",
    "    prompt += f'''{separator_line}\n",
    "\n",
    "## RANKING INSTRUCTIONS\n",
    "\n",
    "You need to rank the {len(slate)} documents by their relevance to the search query.\n",
    "\n",
    "### GUIDELINES (focus on relevance):\n",
    "- Consider the search query and how well each document matches it\n",
    "- Documents with higher RELEVANCE grades (2 > 1 > 0) are generally more relevant, but also analyze the content\n",
    "- Think about topic match, content quality, and legal domain relevance\n",
    "- BM25 and FAISS scores can help, but focus primarily on content relevance\n",
    "\n",
    "### OUTPUT FORMAT (STRICT):\n",
    "- Respond with EXACTLY one line in this format ‚Üí RANKING: <comma-separated indices> #END\n",
    "- Use document indices 0 to {len(slate)-1}, each exactly once, separated by commas and without extra spaces\n",
    "- Do NOT include headings, bullet points, Markdown, or explanations before or after the line\n",
    "- If you are uncertain, still provide your best full ranking covering every index\n",
    "- End the line with \"#END\" to confirm completion\n",
    "\n",
    "Example:\n",
    "RANKING: 3,0,5,1,4,2,8,6,7,9 #END\n",
    "\n",
    "RANKING:\n",
    "'''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"‚úÖ Seg√©df√ºggv√©nyek defini√°lva\")\n",
    "print(\"  üìù Enhanced learning-to-rank prompt template:\")\n",
    "print(\"     - English instructions with Hungarian data\")\n",
    "print(\"     - Explicit ranking criteria\")\n",
    "print(\"     - Step-by-step guidance\")\n",
    "print(\"     - Clear do's and don'ts\")\n",
    "print(\"     - Structured document presentation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slate adatok bet√∂lt√©se\n",
    "print(f\"üìÇ Slate adatok bet√∂lt√©se: {SLATE_FILE}\")\n",
    "df_slates = pd.read_json(SLATE_FILE, lines=True, encoding='utf-8')\n",
    "slates_data = df_slates.to_dict('records')\n",
    "print(f\"‚úÖ Bet√∂ltve: {len(slates_data)} slate\")\n",
    "\n",
    "sample = slates_data[0]\n",
    "print(f\"\\nüìã Minta slate strukt√∫ra:\")\n",
    "print(f\"  Query ID: {sample['query_id'][:50]}...\")\n",
    "print(f\"  Slate elemek: {len(sample['slate'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt megtekint√©se\n",
    "test_prompt = create_training_prompt(slates_data[0][\"query_id\"], slates_data[0][\"slate\"])\n",
    "print(\"üìù R√âSZLETES GRPO PROMPT - TELJES P√âLDA\")\n",
    "print(\"=\"*80)\n",
    "print(test_prompt)\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Prompt statisztik√°k:\")\n",
    "print(f\"  Teljes hossz: {len(test_prompt)} karakter\")\n",
    "print(f\"  Becs√ºlt tokenek: ~{len(test_prompt)//4} token\")\n",
    "print(f\"  Dokumentumok sz√°ma: {len(slates_data[0]['slate'])}\")\n",
    "print(f\"  Slate ID: {slates_data[0]['query_id'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Multi-Metrika Reward Setup\n",
    "\n",
    "A standard nDCG@10 optimaliz√°ci√≥ helyett **konvex kombin√°ci√≥t** alkalmazunk:\n",
    "- **nDCG@10** (50%): Rangsor min≈ës√©g\n",
    "- **MRR@5** (35%): Els≈ë relev√°ns tal√°lat poz√≠ci√≥ja (√ºzleti priorit√°s!)\n",
    "- **Recall@20** (15%): Completeness\n",
    "\n",
    "Ez biztos√≠tja, hogy a model a **val√≥di √ºzleti √©rt√©ket** maximaliz√°lja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Metrika Reward Helper Functions (inline defini√°lva - nincs k√ºls≈ë import)\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def compute_mrr(relevances: List[int], k: Optional[int] = None) -> float:\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank sz√°m√≠t√°sa.\n",
    "    \n",
    "    MRR@k = 1 / rank_first_relevant (ha van relev√°ns a top-k-ban)\n",
    "           = 0 (ha nincs relev√°ns a top-k-ban)\n",
    "    \"\"\"\n",
    "    if k is not None:\n",
    "        relevances = relevances[:k]\n",
    "    \n",
    "    for i, rel in enumerate(relevances, start=1):\n",
    "        if rel > 0:\n",
    "            return 1.0 / i\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def compute_recall(relevances: List[int], k: int, total_relevant: Optional[int] = None) -> float:\n",
    "    \"\"\"\n",
    "    Recall@k sz√°m√≠t√°sa.\n",
    "    \n",
    "    Recall@k = |{relevant docs in top-k}| / |{all relevant docs}|\n",
    "    \"\"\"\n",
    "    if not relevances:\n",
    "        return 0.0\n",
    "    \n",
    "    # Relevantes doc-ok sz√°ma top-k-ban\n",
    "    relevant_in_topk = sum(1 for rel in relevances[:k] if rel > 0)\n",
    "    \n",
    "    # √ñsszes relev√°ns (ha nincs megadva, akkor az eg√©sz list√°b√≥l sz√°moljuk)\n",
    "    if total_relevant is None:\n",
    "        total_relevant = sum(1 for rel in relevances if rel > 0)\n",
    "    \n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return relevant_in_topk / total_relevant\n",
    "\n",
    "\n",
    "def compute_ndcg_metric(relevances: List[int], k: int) -> float:\n",
    "    \"\"\"\n",
    "    nDCG@k sz√°m√≠t√°sa sklearn haszn√°lat√°val.\n",
    "    \n",
    "    Args:\n",
    "        relevances: Relevancia √©rt√©kek ranking sorrendben [rel_1, rel_2, ...]\n",
    "        k: Top-k truncation\n",
    "    \n",
    "    Returns:\n",
    "        nDCG@k √©rt√©k [0, 1]\n",
    "    \"\"\"\n",
    "    if not relevances or k <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # sklearn ndcg_score: [true_relevances], [predicted_scores]\n",
    "    # predicted_scores: magasabb score = jobb ranking (ford√≠tott sorrend)\n",
    "    relevances_truncated = relevances[:k]\n",
    "    scores = list(range(len(relevances_truncated), 0, -1))  # [n, n-1, ..., 1]\n",
    "    \n",
    "    try:\n",
    "        ndcg = ndcg_score(\n",
    "            y_true=[relevances_truncated],\n",
    "            y_score=[scores],\n",
    "            k=k\n",
    "        )\n",
    "        return float(ndcg)\n",
    "    except Exception:\n",
    "        # Edge case: nincs relev√°ns dokumentum\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def compute_multi_metric_reward(\n",
    "    predicted_indices: List[int],\n",
    "    slate: List[Dict],\n",
    "    baseline_metrics: Dict[str, float],\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "    clip_range: Tuple[float, float] = (-1.0, 1.0),\n",
    "    use_sigmoid: bool = True,\n",
    "    mrr_tiebreak_bonus: float = 0.02,\n",
    "    diversity_penalty_weight: float = 0.0\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Multi-metrika reward sz√°m√≠t√°sa sigmoid-alap√∫ stabiliz√°ci√≥val.\n",
    "    \n",
    "    Jav√≠t√°sok:\n",
    "    - Sigmoid transzform√°ci√≥ a delta-kra ‚Üí stabilabb gradiens\n",
    "    - MRR tie-break bonus: ha nDCG/Recall stagn√°l de MRR javul\n",
    "    - Diverzit√°s b√ºntet√©s: ugyanazon court/domain t√∫ls√∫ly a top-10-ben\n",
    "    \n",
    "    Args:\n",
    "        predicted_indices: Model √°ltal predikt√°lt ranking indexek\n",
    "        slate: Slate adatok [{\"relevance\": int, \"court\": str, \"domain\": str, ...}, ...]\n",
    "        baseline_metrics: Baseline metrik√°k {\"ndcg@10\": float, \"mrr@5\": float, ...}\n",
    "        weights: Metrika s√∫lyok {\"ndcg10\": 0.6, \"mrr5\": 0.3, \"recall20\": 0.1}\n",
    "        clip_range: Reward clipping tartom√°ny (min, max)\n",
    "        use_sigmoid: Ha True, sigmoid(delta) transzform√°ci√≥\n",
    "        mrr_tiebreak_bonus: Bonus ha csak MRR javul (+Œµ)\n",
    "        diversity_penalty_weight: Diverzit√°s b√ºntet√©s s√∫lya (0 = kikapcsolva)\n",
    "    \n",
    "    Returns:\n",
    "        (total_reward, components_dict)\n",
    "    \"\"\"\n",
    "    # Default weights (updated: 0.6/0.3/0.1)\n",
    "    if weights is None:\n",
    "        weights = {\"ndcg10\": 0.60, \"mrr5\": 0.30, \"recall20\": 0.10}\n",
    "    \n",
    "    # Relevancia array a predicted ranking szerint\n",
    "    relevances = [slate[i][\"relevance\"] for i in predicted_indices]\n",
    "    \n",
    "    # Policy metrik√°k\n",
    "    policy_ndcg10 = compute_ndcg_metric(relevances, k=10)\n",
    "    policy_mrr5 = compute_mrr(relevances, k=5)\n",
    "    \n",
    "    # Recall@20: total_relevant a teljes slate-b≈ël (fix√°lt candidate pool!)\n",
    "    total_relevant = sum(1 for doc in slate if doc.get(\"relevance\", 0) > 0)\n",
    "    policy_recall20 = compute_recall(relevances, k=20, total_relevant=total_relevant)\n",
    "    \n",
    "    # Baseline metrik√°k\n",
    "    baseline_ndcg10 = baseline_metrics.get(\"ndcg@10\", 0.0)\n",
    "    baseline_mrr5 = baseline_metrics.get(\"mrr@5\", 0.0)\n",
    "    baseline_recall20 = baseline_metrics.get(\"recall@20\", 0.0)\n",
    "    \n",
    "    # Delta sz√°m√≠t√°s\n",
    "    delta_ndcg10 = policy_ndcg10 - baseline_ndcg10\n",
    "    delta_mrr5 = policy_mrr5 - baseline_mrr5\n",
    "    delta_recall20 = policy_recall20 - baseline_recall20\n",
    "    \n",
    "    # Sigmoid transzform√°ci√≥ (stabilabb reward signal)\n",
    "    if use_sigmoid:\n",
    "        # sigmoid(x) = 1 / (1 + exp(-k*x)), k=5 ‚Üí √©rz√©kenys√©g\n",
    "        def sigmoid_transform(delta, k=5.0):\n",
    "            return 2.0 / (1.0 + np.exp(-k * delta)) - 1.0  # range: [-1, 1]\n",
    "        \n",
    "        component_ndcg = weights[\"ndcg10\"] * sigmoid_transform(delta_ndcg10)\n",
    "        component_mrr = weights[\"mrr5\"] * sigmoid_transform(delta_mrr5)\n",
    "        component_recall = weights[\"recall20\"] * sigmoid_transform(delta_recall20)\n",
    "    else:\n",
    "        # Line√°ris (original)\n",
    "        component_ndcg = weights[\"ndcg10\"] * delta_ndcg10\n",
    "        component_mrr = weights[\"mrr5\"] * delta_mrr5\n",
    "        component_recall = weights[\"recall20\"] * delta_recall20\n",
    "    \n",
    "    # Total reward\n",
    "    reward_raw = component_ndcg + component_mrr + component_recall\n",
    "    \n",
    "    # ====== TIE-BREAK BONUS: MRR javul√°s ======\n",
    "    # Ha nDCG/Recall nem v√°ltozik jelent≈ësen, de MRR javul ‚Üí felhaszn√°l√≥i √©lm√©ny++\n",
    "    if abs(delta_ndcg10) < 0.01 and abs(delta_recall20) < 0.01 and delta_mrr5 > 0.05:\n",
    "        reward_raw += mrr_tiebreak_bonus\n",
    "    \n",
    "    # ====== DIVERZIT√ÅS B√úNTET√âS ======\n",
    "    # Ha a top-10 t√∫l homog√©n (ugyanaz a court/domain domin√°l) ‚Üí -Œµ\n",
    "    if diversity_penalty_weight > 0:\n",
    "        top10_indices = predicted_indices[:10]\n",
    "        top10_courts = [slate[i].get(\"court\", \"N/A\") for i in top10_indices if i < len(slate)]\n",
    "        top10_domains = [slate[i].get(\"domain\", \"N/A\") for i in top10_indices if i < len(slate)]\n",
    "        \n",
    "        # Shannon entropy (magas = diverzit√°s, alacsony = homog√©n)\n",
    "        def shannon_entropy(items):\n",
    "            counts = Counter(items)\n",
    "            total = len(items)\n",
    "            if total == 0:\n",
    "                return 0.0\n",
    "            probs = [c/total for c in counts.values()]\n",
    "            return -sum(p * np.log(p + 1e-9) for p in probs)\n",
    "        \n",
    "        court_entropy = shannon_entropy(top10_courts)\n",
    "        domain_entropy = shannon_entropy(top10_domains)\n",
    "        \n",
    "        # Normaliz√°l√°s: max entropy = log(10) ‚âà 2.3\n",
    "        max_entropy = np.log(10)\n",
    "        diversity_score = (court_entropy + domain_entropy) / (2 * max_entropy)\n",
    "        \n",
    "        # B√ºntet√©s ha alacsony diverzit√°s (< 0.5 ‚Üí homog√©n)\n",
    "        if diversity_score < 0.5:\n",
    "            penalty = diversity_penalty_weight * (0.5 - diversity_score)\n",
    "            reward_raw -= penalty\n",
    "    \n",
    "    # Clipping\n",
    "    reward_clipped = np.clip(reward_raw, clip_range[0], clip_range[1])\n",
    "    \n",
    "    # Komponensek dictionary (r√©szletes tracking)\n",
    "    components = {\n",
    "        \"reward_total\": float(reward_clipped),\n",
    "        \"reward_raw\": float(reward_raw),\n",
    "        \"component_ndcg10\": float(component_ndcg),\n",
    "        \"component_mrr5\": float(component_mrr),\n",
    "        \"component_recall20\": float(component_recall),\n",
    "        \"delta_ndcg10\": float(delta_ndcg10),\n",
    "        \"delta_mrr5\": float(delta_mrr5),\n",
    "        \"delta_recall20\": float(delta_recall20),\n",
    "        \"policy_ndcg10\": float(policy_ndcg10),\n",
    "        \"policy_mrr5\": float(policy_mrr5),\n",
    "        \"policy_recall20\": float(policy_recall20),\n",
    "        \"baseline_ndcg10\": float(baseline_ndcg10),\n",
    "        \"baseline_mrr5\": float(baseline_mrr5),\n",
    "        \"baseline_recall20\": float(baseline_recall20),\n",
    "    }\n",
    "    \n",
    "    return float(reward_clipped), components\n",
    "\n",
    "print(\"‚úÖ Multi-Metrika Reward Helper Functions defini√°lva (inline)\")\n",
    "print(f\"   üéØ Multi-metric reward: sigmoid-based, 0.6/0.3/0.1 weights\")\n",
    "print(f\"   üì¶ Nincs k√ºls≈ë import - minden inline defini√°lva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrik√°k bet√∂lt√©se (pre-computed from baseline_evaluation.ipynb)\n",
    "BASELINE_METRICS_PATH = BASE_PATH / \"baseline_query_metrics.csv\"\n",
    "\n",
    "if not BASELINE_METRICS_PATH.exists():\n",
    "    print(f\"‚ö†Ô∏è  FIGYELEM: Baseline metrics f√°jl nem tal√°lhat√≥: {BASELINE_METRICS_PATH}\")\n",
    "    print(f\"   El≈ëbb futtasd le a baseline_evaluation.ipynb notebookot!\")\n",
    "    print(f\"   Fallback: Baseline metrik√°k inicializ√°l√°sa 0-ra (NEM AJ√ÅNLOTT)\")\n",
    "    baseline_metrics_dict = {}\n",
    "else:\n",
    "    baseline_df = pd.read_csv(BASELINE_METRICS_PATH, encoding='utf-8')\n",
    "    baseline_metrics_dict = {\n",
    "        row[\"query_id\"]: row.to_dict()\n",
    "        for _, row in baseline_df.iterrows()\n",
    "    }\n",
    "    print(f\"‚úÖ Baseline metrik√°k bet√∂ltve: {len(baseline_metrics_dict)} query\")\n",
    "    \n",
    "    # Statisztik√°k\n",
    "    if baseline_metrics_dict:\n",
    "        sample_query = list(baseline_metrics_dict.keys())[0]\n",
    "        sample_metrics = baseline_metrics_dict[sample_query]\n",
    "        print(f\"\\nüìä El√©rhet≈ë metrik√°k (p√©lda query):\")\n",
    "        for key in sorted(sample_metrics.keys()):\n",
    "            if key != \"query_id\" and isinstance(sample_metrics[key], (int, float)):\n",
    "                print(f\"  ‚Ä¢ {key}: {sample_metrics[key]:.4f}\")\n",
    "        \n",
    "        # Aggreg√°lt baseline teljes√≠tm√©ny\n",
    "        avg_ndcg10 = np.mean([m.get(\"ndcg@10\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "        avg_mrr5 = np.mean([m.get(\"mrr@5\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "        avg_recall20 = np.mean([m.get(\"recall@20\", 0.0) for m in baseline_metrics_dict.values()])\n",
    "        \n",
    "        print(f\"\\nüéØ Baseline teljes√≠tm√©ny (√°tlag):\")\n",
    "        print(f\"  ‚Ä¢ nDCG@10:   {avg_ndcg10:.4f}\")\n",
    "        print(f\"  ‚Ä¢ MRR@5:     {avg_mrr5:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Recall@20: {avg_recall20:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CURRICULUM LEARNING - Legnagyobb jav√≠t√°si potenci√°l ======\n",
    "# Strat√©gia: El≈ësz√∂r k√∂nny≈± p√©ld√°kkal tan√≠tunk (magas baseline nDCG), azt√°n fokozatosan nehezebbekre\n",
    "# Ez seg√≠t a modellnek el≈ësz√∂r megtanulni a j√≥ rangsorol√°st, azt√°n finomhangolni\n",
    "\n",
    "print(f\"\\nüìö Dataset el≈ëk√©sz√≠t√©se CURRICULUM LEARNING-nel...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Baseline nDCG kisz√°m√≠t√°sa minden slate-hez\n",
    "print(\"üéØ 1. Baseline nDCG kisz√°m√≠t√°sa slate-enk√©nt...\")\n",
    "slate_baseline_ndcgs = []\n",
    "\n",
    "for slate_data in slates_data:\n",
    "    slate = slate_data.get(\"slate\", [])\n",
    "    if not slate:\n",
    "        continue\n",
    "    \n",
    "    relevances = [doc.get('relevance', 0) for doc in slate]\n",
    "    baseline_indices = list(range(len(slate)))\n",
    "    baseline_ndcg = calculate_ndcg(baseline_indices, relevances, k=NDCG_K)\n",
    "    \n",
    "    slate_baseline_ndcgs.append({\n",
    "        \"query_id\": slate_data[\"query_id\"],\n",
    "        \"baseline_ndcg\": baseline_ndcg,\n",
    "        \"num_relevant\": sum(1 for r in relevances if r >= 1),\n",
    "        \"num_high_relevant\": sum(1 for r in relevances if r == 2),\n",
    "    })\n",
    "\n",
    "# 2. Curriculum szintek defini√°l√°sa baseline nDCG alapj√°n\n",
    "ndcgs = [s[\"baseline_ndcg\"] for s in slate_baseline_ndcgs]\n",
    "\n",
    "print(f\"\\nüìä Baseline nDCG statisztik√°k:\")\n",
    "print(f\"  √Åtlag: {np.mean(ndcgs):.4f}\")\n",
    "print(f\"  Median: {np.median(ndcgs):.4f}\")\n",
    "print(f\"  Min: {np.min(ndcgs):.4f}, Max: {np.max(ndcgs):.4f}\")\n",
    "print(f\"  Sz√≥r√°s: {np.std(ndcgs):.4f}\")\n",
    "\n",
    "# KRITIKUS: Ellen≈ërizz√ºk hogy van-e el√©g sz√≥r√°s a curriculum besorol√°shoz\n",
    "ndcg_range = np.max(ndcgs) - np.min(ndcgs)\n",
    "VARIANCE_THRESHOLD = 0.05  # Ha a tartom√°ny kisebb mint ez, fallback strat√©gia\n",
    "\n",
    "if ndcg_range < VARIANCE_THRESHOLD:\n",
    "    print(f\"\\n‚ö†Ô∏è  FIGYELEM: nDCG eloszl√°s √∂sszeomlott! (range={ndcg_range:.4f} < {VARIANCE_THRESHOLD})\")\n",
    "    print(f\"   Val√≥sz√≠n≈± ok: slate-ek relevancia szerint rendezve (label leakage!)\")\n",
    "    print(f\"   ‚Üí FALLBACK STRAT√âGIA: baseline_query_metrics.csv haszn√°lata\")\n",
    "    \n",
    "    # Fallback: haszn√°ljuk a baseline metrics CSV adatait\n",
    "    if baseline_metrics_dict:\n",
    "        print(f\"\\n   ‚úÖ Baseline metrics el√©rhet≈ë, kombin√°lt neh√©zs√©gi score haszn√°lata\")\n",
    "        slate_baseline_ndcgs_fallback = []\n",
    "        \n",
    "        for s in slate_baseline_ndcgs:\n",
    "            query_id = s[\"query_id\"]\n",
    "            metrics = baseline_metrics_dict.get(query_id, {})\n",
    "            \n",
    "            # Kombin√°lt neh√©zs√©gi score: t√∂bb metrika √°tlaga\n",
    "            combined_score = (\n",
    "                metrics.get(\"ndcg@10\", 0.0) * 0.5 +\n",
    "                metrics.get(\"mrr@5\", 0.0) * 0.3 +\n",
    "                metrics.get(\"recall@20\", 0.0) * 0.2\n",
    "            )\n",
    "            \n",
    "            slate_baseline_ndcgs_fallback.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"baseline_ndcg\": combined_score,  # Kombin√°lt score\n",
    "                \"num_relevant\": s[\"num_relevant\"],\n",
    "                \"num_high_relevant\": s[\"num_high_relevant\"],\n",
    "            })\n",
    "        \n",
    "        slate_baseline_ndcgs = slate_baseline_ndcgs_fallback\n",
    "        ndcgs = [s[\"baseline_ndcg\"] for s in slate_baseline_ndcgs]\n",
    "        \n",
    "        print(f\"   üìä Fallback score statisztik√°k:\")\n",
    "        print(f\"      √Åtlag: {np.mean(ndcgs):.4f}\")\n",
    "        print(f\"      Sz√≥r√°s: {np.std(ndcgs):.4f}\")\n",
    "        print(f\"      Range: {np.max(ndcgs) - np.min(ndcgs):.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Baseline metrics nem el√©rhet≈ë!\")\n",
    "        print(f\"   ‚Üí EGYSZER≈∞S√çTETT FALLBACK: relevance count alap√∫ neh√©zs√©g\")\n",
    "        \n",
    "        for s in slate_baseline_ndcgs:\n",
    "            # Heurisztika: mennyi relev√°ns dokumentum van?\n",
    "            num_rel = s[\"num_relevant\"]\n",
    "            num_high = s[\"num_high_relevant\"]\n",
    "            # Normaliz√°lt score: t√∂bb relev√°ns = k√∂nnyebb\n",
    "            s[\"baseline_ndcg\"] = (num_high * 2 + num_rel) / 20.0  # max ~20\n",
    "        \n",
    "        ndcgs = [s[\"baseline_ndcg\"] for s in slate_baseline_ndcgs]\n",
    "        print(f\"   üìä Heurisztikus score statisztik√°k:\")\n",
    "        print(f\"      √Åtlag: {np.mean(ndcgs):.4f}\")\n",
    "        print(f\"      Range: {np.max(ndcgs) - np.min(ndcgs):.4f}\")\n",
    "\n",
    "# Percentilis sz√°m√≠t√°s\n",
    "ndcg_percentiles = {\n",
    "    \"easy\": np.percentile(ndcgs, 75),      # Top 25% - legjobb baseline (nDCG > ~0.5-0.6)\n",
    "    \"medium\": np.percentile(ndcgs, 50),    # Top 50% - k√∂zepes baseline (nDCG > ~0.3-0.4)\n",
    "    \"hard\": np.percentile(ndcgs, 25),      # Top 75% - neh√©z baseline (nDCG > ~0.1-0.2)\n",
    "    \"very_hard\": 0.0,                      # Minden - a legnehezebbek is\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà Curriculum szintek (percentile):\")\n",
    "print(f\"  Easy (> {ndcg_percentiles['easy']:.3f}): Top 25% - legjobb baseline slate-ek\")\n",
    "print(f\"  Medium (> {ndcg_percentiles['medium']:.3f}): Top 50% - k√∂zepes baseline\")\n",
    "print(f\"  Hard (> {ndcg_percentiles['hard']:.3f}): Top 75% - nehezebb slate-ek\")\n",
    "print(f\"  Very Hard: Minden slate\")\n",
    "\n",
    "# 3. Slate-ek kategoriz√°l√°sa\n",
    "slate_difficulty = {}\n",
    "for s in slate_baseline_ndcgs:\n",
    "    ndcg = s[\"baseline_ndcg\"]\n",
    "    if ndcg >= ndcg_percentiles[\"easy\"]:\n",
    "        difficulty = \"easy\"\n",
    "    elif ndcg >= ndcg_percentiles[\"medium\"]:\n",
    "        difficulty = \"medium\"\n",
    "    elif ndcg >= ndcg_percentiles[\"hard\"]:\n",
    "        difficulty = \"hard\"\n",
    "    else:\n",
    "        difficulty = \"very_hard\"\n",
    "    slate_difficulty[s[\"query_id\"]] = difficulty\n",
    "\n",
    "# 4. Dataset √©p√≠t√©se difficulty tagging-gel\n",
    "training_examples = []\n",
    "slate_lookup = {}\n",
    "\n",
    "for slate_data in slates_data:\n",
    "    query_id = slate_data[\"query_id\"]\n",
    "    if query_id not in slate_difficulty:\n",
    "        continue\n",
    "    \n",
    "    prompt = create_training_prompt(query_id, slate_data[\"slate\"])\n",
    "    difficulty = slate_difficulty[query_id]\n",
    "    baseline_ndcg = next(s[\"baseline_ndcg\"] for s in slate_baseline_ndcgs if s[\"query_id\"] == query_id)\n",
    "    \n",
    "    training_examples.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"difficulty\": difficulty,\n",
    "        \"baseline_ndcg\": baseline_ndcg,\n",
    "    })\n",
    "    slate_lookup[query_id] = slate_data[\"slate\"]\n",
    "\n",
    "# 5. Difficulty eloszl√°s\n",
    "difficulty_counts = {}\n",
    "for ex in training_examples:\n",
    "    d = ex[\"difficulty\"]\n",
    "    difficulty_counts[d] = difficulty_counts.get(d, 0) + 1\n",
    "\n",
    "print(f\"\\nüìä Difficulty eloszl√°s:\")\n",
    "for diff in [\"easy\", \"medium\", \"hard\", \"very_hard\"]:\n",
    "    count = difficulty_counts.get(diff, 0)\n",
    "    pct = 100 * count / len(training_examples) if training_examples else 0\n",
    "    avg_ndcg = np.mean([ex[\"baseline_ndcg\"] for ex in training_examples if ex[\"difficulty\"] == diff]) if count > 0 else 0\n",
    "    print(f\"  {diff:12s}: {count:3d} query ({pct:5.1f}%) - avg baseline nDCG: {avg_ndcg:.4f}\")\n",
    "\n",
    "# 6. Curriculum learning konfigur√°ci√≥ (haszn√°lva a cella 4-ben defini√°lt v√°ltoz√≥kat)\n",
    "# A CURRICULUM_ENABLED √©s CURRICULUM_STRATEGY m√°r defini√°lva van a cella 4-ben\n",
    "\n",
    "print(f\"\\n‚úÖ Curriculum Learning konfigur√°ci√≥:\")\n",
    "print(f\"  Enabled: {CURRICULUM_ENABLED}\")\n",
    "print(f\"  Strategy: {CURRICULUM_STRATEGY}\")\n",
    "if CURRICULUM_STRATEGY == \"filtered_dataset\":\n",
    "    easy_count = len([ex for ex in training_examples if ex['difficulty'] == 'easy'])\n",
    "    medium_count = len([ex for ex in training_examples if ex['difficulty'] == 'medium'])\n",
    "    print(f\"  Training dataset: easy ({easy_count}) + medium ({medium_count}) = {easy_count + medium_count} query\")\n",
    "    print(f\"  Ez biztos√≠tja, hogy csak j√≥ baseline nDCG-val rendelkez≈ë p√©ld√°kkal tan√≠tunk\")\n",
    "    print(f\"  ‚Üí Jobb tanul√°si signal ‚Üí Jobb v√©gs≈ë teljes√≠tm√©ny ‚Üí K√∂zelebb az 1.0 nDCG-hoz\")\n",
    "\n",
    "# 7. Train/eval split (stratified by difficulty ha lehet)\n",
    "from collections import defaultdict\n",
    "\n",
    "# Eval set: random 20%, de pr√≥b√°ljuk difficulty-t is meg≈ërizni\n",
    "full_dataset = Dataset.from_list(training_examples)\n",
    "indices = np.arange(len(full_dataset))\n",
    "train_indices, eval_indices = train_test_split(indices, test_size=1.0 - TRAIN_SPLIT, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_dataset = full_dataset.select(train_indices)\n",
    "eval_dataset = full_dataset.select(eval_indices)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset l√©trehozva:\")\n",
    "print(f\"  Training: {len(train_dataset)} query (80%)\")\n",
    "print(f\"  Evaluation: {len(eval_dataset)} query (20%)\")\n",
    "print(f\"  Slate lookup: {len(slate_lookup)} entry\")\n",
    "\n",
    "# 8. Training examples difficulty statisztik√°k\n",
    "train_difficulty_dist = defaultdict(int)\n",
    "for idx in train_indices:\n",
    "    train_difficulty_dist[training_examples[idx][\"difficulty\"]] += 1\n",
    "\n",
    "print(f\"\\nüìä Training set difficulty eloszl√°s:\")\n",
    "for diff in [\"easy\", \"medium\", \"hard\", \"very_hard\"]:\n",
    "    count = train_difficulty_dist.get(diff, 0)\n",
    "    print(f\"  {diff:12s}: {count:3d} query\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° CURRICULUM LEARNING haszn√°lata:\")\n",
    "print(\"   A training fokozatosan √©p√ºl fel: el≈ësz√∂r easy query-kkel,\")\n",
    "print(\"   azt√°n fokozatosan hozz√°adjuk a nehezebbeket.\")\n",
    "print(\"   Ez seg√≠t a modellnek jobban tanulni a rangsorol√°st.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval split export√°l√°sa √∂sszehasonl√≠t√≥ √©rt√©kel√©shez\n",
    "# CRITICAL: Az eval split query_id-k export√°l√°sa determinisztikusan reproduk√°lhat√≥ form√°tumban\n",
    "# Ez biztos√≠tja, hogy a baseline √©s GRPO modell ugyanazon eval halmazon legyen ki√©rt√©kelve\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì§ Eval Split Export√°l√°sa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Query ID-k kinyer√©se az eval_dataset-b≈ël\n",
    "eval_query_ids = []\n",
    "\n",
    "for example in eval_dataset:\n",
    "    prompt = example[\"prompt\"]\n",
    "    # Ugyanaz a regex parsing logika, mint a reward function-ben\n",
    "    match = re.search(r'(?:SEARCH QUERY|Query)[:\\s\"]*([^\"\\n]+)', prompt)\n",
    "    query_id = None\n",
    "    if match:\n",
    "        query_id = match.group(1)\n",
    "    else:\n",
    "        match2 = re.search(r'##\\s*SEARCH\\s*QUERY\\s*\\n\\s*\"([^\"]+)\"', prompt)\n",
    "        if match2:\n",
    "            query_id = match2.group(1)\n",
    "    \n",
    "    if query_id and query_id not in eval_query_ids:\n",
    "        eval_query_ids.append(query_id)\n",
    "\n",
    "# Rendez√©s a determinisztikus kimenet√©rt\n",
    "eval_query_ids.sort()\n",
    "\n",
    "print(f\"\\nüìä Eval split statisztik√°k:\")\n",
    "print(f\"  Eval query-k sz√°ma: {len(eval_query_ids)}\")\n",
    "print(f\"  Train query-k sz√°ma: {len(train_dataset)}\")\n",
    "print(f\"  Split ar√°ny: {len(eval_query_ids) / (len(eval_query_ids) + len(train_dataset)):.1%}\")\n",
    "\n",
    "# Eval split export JSON form√°tumban\n",
    "eval_split_data = {\n",
    "    \"query_ids\": eval_query_ids,\n",
    "    \"num_queries\": len(eval_query_ids),\n",
    "    \"split_params\": {\n",
    "        \"train_split\": TRAIN_SPLIT,\n",
    "        \"seed\": SEED,\n",
    "        \"shuffle\": True\n",
    "    },\n",
    "    \"created_at\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "eval_split_path = OUTPUT_DIR / \"eval_split.json\"\n",
    "with open(eval_split_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_split_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Eval split export√°lva: {eval_split_path}\")\n",
    "print(f\"   Ez a f√°jl haszn√°lva lesz a baseline √©s GRPO √∂sszehasonl√≠t√≥ notebookban\")\n",
    "print(f\"   ugyanazon eval halmazon val√≥ ki√©rt√©kel√©shez.\")\n",
    "\n",
    "# Opcion√°lis: CSV export is (egyszer≈±bb bet√∂lt√©shez)\n",
    "eval_split_csv = OUTPUT_DIR / \"eval_split.csv\"\n",
    "eval_df = pd.DataFrame({\"query_id\": eval_query_ids})\n",
    "eval_df.to_csv(eval_split_csv, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Eval split CSV: {eval_split_csv}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bet√∂lt√©se - Memory Efficient\n",
    "print(f\"üîÑ Model bet√∂lt√©se Unsloth-tal: {MODEL_NAME}\")\n",
    "print(f\"  ‚öôÔ∏è  GPU Memory Utilization: {GPU_MEMORY_UTILIZATION}\")\n",
    "print(f\"  ‚öôÔ∏è  Max LoRA Rank: {LORA_RANK}\")\n",
    "\n",
    "# Mem√≥ria tiszt√≠t√°s model bet√∂lt√©s el≈ëtt\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    fast_inference=True,  # CRITICAL: Enable vLLM for GRPO rollouts (2-3x faster inference)\n",
    "    max_lora_rank=LORA_RANK,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    max_model_len=MAX_SEQ_LENGTH,  # CRITICAL: vLLM max_model_len must match max_seq_length for long prompts\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    use_rslora=True,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# Tokenizer padding fix (tensor shape stability)\n",
    "# CRITICAL: Csak padding_side √°ll√≠t√°sa, NE √°ll√≠tsuk √°t a pad_token-et, mert a tokenizer √∫jra√°ll√≠t√°sa\n",
    "# tenzorm√©ret elt√©r√©st okozhat a GRPO trainer vLLM bels≈ë logik√°j√°val\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Note: pad_token be√°ll√≠t√°sa kihagyva - a GRPO trainer √©s vLLM automatikusan kezeli\n",
    "# Az explicit pad_token √°t√°ll√≠t√°s √ºtk√∂zhet a modell config-tal √©s tenzor m√©ret probl√©m√°kat okozhat\n",
    "\n",
    "print(\"‚úÖ Model √©s tokenizer bet√∂ltve (Unsloth + vLLM + RSLoRA)\")\n",
    "print(f\"  ‚úì fast_inference=True: vLLM enged√©lyezve GRPO rollouts-hoz (2-3x gyorsabb)\")\n",
    "print(f\"  ‚úì Padding side: {tokenizer.padding_side}\")\n",
    "print(f\"  ‚úì Pad token ID: {tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id else 'N/A (auto-managed)'}\")\n",
    "print(f\"  ‚úì Note: GRPO trainer automatikusan kezeli a tokenizer be√°ll√≠t√°sokat vLLM-mel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU mem√≥ria monitoring model bet√∂lt√©s ut√°n\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    free = total - allocated\n",
    "    \n",
    "    print(f\"\\nüìä GPU Mem√≥ria Model Bet√∂lt√©s Ut√°n:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Free: {free:.2f} GB\")\n",
    "    print(f\"  Free %: {(free/total)*100:.1f}%\")\n",
    "    \n",
    "    if free < 10:\n",
    "        print(f\"  ‚ö†Ô∏è  FIGYELEM: Kev√©s szabad mem√≥ria ({free:.2f} GB)!\")\n",
    "        print(f\"  üí° Opci√≥: Tov√°bbi cs√∂kkent√©s lehet sz√ºks√©ges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== MULTI-METRIKA REWARD FUNCTION (Sigmoid-Based) ======\n",
    "# Konvex kombin√°ci√≥ja h√°rom f≈ë metrik√°nak sigmoid transzform√°ci√≥val:\n",
    "# - nDCG@10 (60%): Rangsor min≈ës√©g (n√∂velt s√∫ly)\n",
    "# - MRR@5 (30%): Els≈ë relev√°ns tal√°lat (gyakorlati haszn√°lhat√≥s√°g)\n",
    "# - Recall@20 (10%): Completeness\n",
    "#\n",
    "# Jav√≠t√°sok:\n",
    "# - Sigmoid(delta) ‚Üí stabilabb gradiens, nem satur√°l√≥dik\n",
    "# - MRR tie-break bonus: ha nDCG/Recall stagn√°l, de MRR javul ‚Üí +0.02\n",
    "# - Diverzit√°s b√ºntet√©s: homog√©n top-10 (court/domain) ‚Üí -Œµ\n",
    "\n",
    "# Reward konfigur√°ci√≥ (config.py alapj√°n)\n",
    "REWARD_WEIGHTS = {\n",
    "    \"ndcg10\": 0.60,    # 50% ‚Üí 60%\n",
    "    \"mrr5\": 0.30,      # 35% ‚Üí 30%\n",
    "    \"recall20\": 0.10,  # 15% ‚Üí 10%\n",
    "}\n",
    "REWARD_CLIP_RANGE = (-1.0, 1.0)\n",
    "REWARD_USE_SIGMOID = True\n",
    "REWARD_MRR_TIEBREAK_BONUS = 0.02\n",
    "REWARD_DIVERSITY_PENALTY = 0.05  # Diverzit√°s b√ºntet√©s s√∫lya\n",
    "\n",
    "# Komponens tracking glob√°lis v√°ltoz√≥ (logging c√©lokra)\n",
    "reward_components_log = []\n",
    "\n",
    "def multi_metric_reward_function(completions, prompts, **kwargs):\n",
    "    \"\"\"\n",
    "    Multi-metrika GRPO reward f√ºggv√©ny sigmoid transzform√°ci√≥val.\n",
    "    \n",
    "    A reward h√°rom metrika s√∫lyozott kombin√°ci√≥ja:\n",
    "    1. nDCG@10 (60%): Rangsor min≈ës√©g\n",
    "    2. MRR@5 (30%): Els≈ë relev√°ns tal√°lat poz√≠ci√≥ja (UX priorit√°s!)\n",
    "    3. Recall@20 (10%): Relev√°ns tal√°latok lefedetts√©ge\n",
    "    \n",
    "    Speci√°lis funkci√≥k:\n",
    "    - Sigmoid(delta): stabil gradiens, nem satur√°l√≥dik\n",
    "    - MRR tie-break: +bonus ha csak MRR javul (felhaszn√°l√≥i √©lm√©ny++)\n",
    "    - Diverzit√°s check: homog√©n top-10 ‚Üí penalty\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: Reward √©rt√©kek minden completion-h√∂z\n",
    "    \"\"\"\n",
    "    global reward_components_log\n",
    "    rewards = []\n",
    "    batch_components = []\n",
    "    \n",
    "    for completion, prompt in zip(completions, prompts):\n",
    "        try:\n",
    "            # 1. Query ID extraction\n",
    "            match = re.search(r'(?:SEARCH QUERY|Query)[:\\s\"]*([^\"\\n]+)', prompt)\n",
    "            query_id = None\n",
    "            if match:\n",
    "                query_id = match.group(1)\n",
    "            else:\n",
    "                match2 = re.search(r'##\\s*SEARCH\\s*QUERY\\s*\\n\\s*\"([^\"]+)\"', prompt)\n",
    "                if match2:\n",
    "                    query_id = match2.group(1)\n",
    "            \n",
    "            if not query_id or query_id not in slate_lookup:\n",
    "                # Parsing hiba - enyhe b√ºntet√©s\n",
    "                rewards.append(-0.1)\n",
    "                batch_components.append({\"error\": \"query_id_not_found\"})\n",
    "                continue\n",
    "            \n",
    "            # 2. Slate lookup\n",
    "            slate = slate_lookup[query_id]\n",
    "            \n",
    "            # 3. Parse model output\n",
    "            predicted_indices = parse_model_ranking(completion, len(slate))\n",
    "            \n",
    "            # 4. Baseline metrik√°k lookup\n",
    "            if query_id not in baseline_metrics_dict:\n",
    "                # Nincs baseline adat - fallback to zero baseline (nem ide√°lis, de biztons√°gos)\n",
    "                baseline_metrics = {\n",
    "                    \"ndcg@10\": 0.0,\n",
    "                    \"mrr@5\": 0.0,\n",
    "                    \"mrr@10\": 0.0,  # Kompatibilit√°s\n",
    "                    \"recall@20\": 0.0,\n",
    "                }\n",
    "            else:\n",
    "                baseline_metrics = baseline_metrics_dict[query_id]\n",
    "\n",
    "        \n",
    "            \n",
    "            # 5. Multi-metric reward sz√°m√≠t√°s (helper function from src.reward_metrics)\n",
    "            reward, components = compute_multi_metric_reward(\n",
    "                predicted_indices=predicted_indices,\n",
    "                slate=slate,\n",
    "                baseline_metrics=baseline_metrics,\n",
    "                weights=REWARD_WEIGHTS,\n",
    "                clip_range=REWARD_CLIP_RANGE,\n",
    "                use_sigmoid=REWARD_USE_SIGMOID,\n",
    "                mrr_tiebreak_bonus=REWARD_MRR_TIEBREAK_BONUS,\n",
    "                diversity_penalty_weight=REWARD_DIVERSITY_PENALTY,\n",
    "            )\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            batch_components.append(components)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Exception eset√©n enyhe b√ºntet√©s\n",
    "            rewards.append(-0.1)\n",
    "            batch_components.append({\"error\": str(e)[:100]})\n",
    "    \n",
    "    # Komponensek ment√©se logginghoz\n",
    "    reward_components_log.extend(batch_components)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"‚úÖ Multi-Metrika GRPO Reward function defini√°lva (Sigmoid-Based v2.0)\")\n",
    "print(f\"   üìä Reward s√∫lyok:\")\n",
    "print(f\"      ‚Ä¢ nDCG@10:   {REWARD_WEIGHTS['ndcg10']:.0%} (rangsor min≈ës√©g)\")\n",
    "print(f\"      ‚Ä¢ MRR@5:     {REWARD_WEIGHTS['mrr5']:.0%} (els≈ë relev√°ns tal√°lat)\")\n",
    "print(f\"      ‚Ä¢ Recall@20: {REWARD_WEIGHTS['recall20']:.0%} (completeness)\")\n",
    "print(f\"   üéØ Sigmoid transzform√°ci√≥: {REWARD_USE_SIGMOID}\")\n",
    "print(f\"   üéÅ MRR tie-break bonus: +{REWARD_MRR_TIEBREAK_BONUS}\")\n",
    "print(f\"   üåà Diverzit√°s b√ºntet√©s: {REWARD_DIVERSITY_PENALTY}\")\n",
    "print(f\"   üìè Reward clipping: {REWARD_CLIP_RANGE}\")\n",
    "print(f\"   üìà Komponens tracking: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== DRY RUN VALIDATION - Korai ellen≈ërz√©s teljes tr√©ning el≈ëtt ======\n",
    "# C√©l: 2-3 perc alatt valid√°ljuk a rendszert (prompt, parsing, reward function, model output)\n",
    "# Ez megakad√°lyozza, hogy 25-35 percig futson a tr√©ning hib√°s konfigur√°ci√≥val\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üî¨ DRY RUN VALIDATION - Korai ellen≈ërz√©s\")\n",
    "print(\"=\"*80)\n",
    "print(\"Ez a l√©p√©s 2-3 perc alatt teszteli a rendszert teljes tr√©ning el≈ëtt.\\n\")\n",
    "\n",
    "# Konfigur√°ci√≥\n",
    "DRY_RUN_SAMPLE_SIZE = 10  # 10 query tesztel√©se\n",
    "DRY_RUN_MAX_NEW_TOKENS = 50  # R√∂vid gener√°l√°sok (gyors teszt)\n",
    "DRY_RUN_PARSING_SUCCESS_THRESHOLD = 0.8  # 80% parsing success sz√ºks√©ges\n",
    "DRY_RUN_MIN_REWARD_MEAN = -0.1  # Minimum reward mean (nagyon laza, csak valid√°l√°s)\n",
    "\n",
    "# 1. Mini-batch tesztel√©s: sample query-k kiv√°laszt√°sa\n",
    "print(\"üìä 1. Mini-batch tesztel√©s el≈ëk√©sz√≠t√©se...\")\n",
    "test_indices = np.random.choice(len(slates_data), size=min(DRY_RUN_SAMPLE_SIZE, len(slates_data)), replace=False)\n",
    "test_slates = [slates_data[i] for i in test_indices]\n",
    "test_prompts = [create_training_prompt(slate[\"query_id\"], slate[\"slate\"]) for slate in test_slates]\n",
    "\n",
    "print(f\"   ‚úÖ {len(test_slates)} query kiv√°lasztva tesztel√©shez\")\n",
    "print(f\"   üìã Query ID p√©ld√°k:\")\n",
    "for i, slate in enumerate(test_slates[:3]):\n",
    "    print(f\"      {i+1}. {slate['query_id'][:60]}...\")\n",
    "\n",
    "# 2. Prompt parsing valid√°ci√≥\n",
    "print(f\"\\nüìù 2. Prompt parsing valid√°ci√≥...\")\n",
    "parsing_successes = 0\n",
    "parsing_failures = []\n",
    "\n",
    "for i, (prompt, slate_data) in enumerate(zip(test_prompts, test_slates)):\n",
    "    # Tesztelj√ºk a query_id extraction-t (ugyanaz mint a reward function-ben)\n",
    "    match = re.search(r'(?:SEARCH QUERY|Query)[:\\s\"]*([^\"\\n]+)', prompt)\n",
    "    query_id = None\n",
    "    if match:\n",
    "        query_id = match.group(1)\n",
    "    else:\n",
    "        match2 = re.search(r'##\\s*SEARCH\\s*QUERY\\s*\\n\\s*\"([^\"]+)\"', prompt)\n",
    "        if match2:\n",
    "            query_id = match2.group(1)\n",
    "    \n",
    "    if query_id and query_id in slate_lookup:\n",
    "        parsing_successes += 1\n",
    "    else:\n",
    "        parsing_failures.append({\n",
    "            \"index\": i,\n",
    "            \"query_id_expected\": slate_data[\"query_id\"],\n",
    "            \"query_id_extracted\": query_id,\n",
    "        })\n",
    "\n",
    "parsing_success_rate = parsing_successes / len(test_prompts) if test_prompts else 0.0\n",
    "print(f\"   ‚úÖ Parsing success rate: {parsing_success_rate:.2%} ({parsing_successes}/{len(test_prompts)})\")\n",
    "\n",
    "if parsing_failures:\n",
    "    print(f\"   ‚ö†Ô∏è  {len(parsing_failures)} parsing hiba:\")\n",
    "    for failure in parsing_failures[:3]:\n",
    "        print(f\"      Query {failure['index']}: expected '{failure['query_id_expected'][:50]}...', got '{failure['query_id_extracted']}'\")\n",
    "\n",
    "# 3. Slate strukt√∫ra valid√°ci√≥\n",
    "print(f\"\\nüì¶ 3. Slate strukt√∫ra valid√°ci√≥...\")\n",
    "slate_issues = []\n",
    "relevance_distributions = []\n",
    "\n",
    "for i, slate_data in enumerate(test_slates):\n",
    "    slate = slate_data.get(\"slate\", [])\n",
    "    if not slate:\n",
    "        slate_issues.append({\"index\": i, \"issue\": \"empty_slate\"})\n",
    "        continue\n",
    "    \n",
    "    relevances = [doc.get('relevance', 0) for doc in slate]\n",
    "    relevant_count = sum(1 for r in relevances if r >= 1)\n",
    "    high_rel_count = sum(1 for r in relevances if r == 2)\n",
    "    \n",
    "    relevance_distributions.append({\n",
    "        \"total\": len(relevances),\n",
    "        \"relevant\": relevant_count,\n",
    "        \"high_relevant\": high_rel_count,\n",
    "        \"baseline_ndcg\": calculate_ndcg(list(range(len(relevances))), relevances, k=10),\n",
    "    })\n",
    "    \n",
    "    if relevant_count == 0:\n",
    "        slate_issues.append({\"index\": i, \"issue\": \"no_relevant_docs\", \"query\": slate_data[\"query_id\"][:50]})\n",
    "\n",
    "if slate_issues:\n",
    "    print(f\"   ‚ö†Ô∏è  {len(slate_issues)} slate probl√©ma tal√°lhat√≥:\")\n",
    "    for issue in slate_issues[:3]:\n",
    "        print(f\"      Query {issue['index']}: {issue['issue']}\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Minden slate valid\")\n",
    "\n",
    "avg_stats = {\n",
    "    \"total_docs\": np.mean([d[\"total\"] for d in relevance_distributions]),\n",
    "    \"relevant_docs\": np.mean([d[\"relevant\"] for d in relevance_distributions]),\n",
    "    \"high_relevant_docs\": np.mean([d[\"high_relevant\"] for d in relevance_distributions]),\n",
    "    \"baseline_ndcg\": np.mean([d[\"baseline_ndcg\"] for d in relevance_distributions]),\n",
    "}\n",
    "print(f\"   üìä √Åtlagos slate statisztik√°k:\")\n",
    "print(f\"      Dokumentumok/slate: {avg_stats['total_docs']:.1f}\")\n",
    "print(f\"      Relev√°ns dokumentumok: {avg_stats['relevant_docs']:.1f}\")\n",
    "print(f\"      Magas relevancia (rel=2): {avg_stats['high_relevant_docs']:.1f}\")\n",
    "print(f\"      Baseline nDCG@10: {avg_stats['baseline_ndcg']:.4f}\")\n",
    "\n",
    "# 4. Model inference teszt (ha modell m√°r bet√∂ltve)\n",
    "print(f\"\\nü§ñ 4. Model inference teszt...\")\n",
    "try:\n",
    "    model.eval()\n",
    "    test_completions = []\n",
    "    \n",
    "    # Egyetlen prompt tesztel√©se (gyors)\n",
    "    test_prompt = test_prompts[0]\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=DRY_RUN_MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    completion = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    test_slate = test_slates[0]\n",
    "    parsed_indices = parse_model_ranking(completion, len(test_slate[\"slate\"]))\n",
    "    \n",
    "    is_valid = len(parsed_indices) == len(test_slate[\"slate\"]) and len(set(parsed_indices)) == len(parsed_indices)\n",
    "    \n",
    "    print(f\"   ‚úÖ Model inference sikeres\")\n",
    "    print(f\"      Completion hossz: {len(completion)} karakter\")\n",
    "    print(f\"      Parsed indices: {len(parsed_indices)}/{len(test_slate['slate'])}\")\n",
    "    print(f\"      Valid parsing: {is_valid}\")\n",
    "    print(f\"      Sample output: {completion[:100]}...\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Model inference hiba (ez ok√©, ha modell m√©g nincs bet√∂ltve): {e}\")\n",
    "\n",
    "# 5. Reward function tesztel√©s\n",
    "print(f\"\\nüéØ 5. Reward function tesztel√©s...\")\n",
    "test_rewards = []\n",
    "test_reward_details = []\n",
    "\n",
    "for prompt, slate_data in zip(test_prompts[:5], test_slates[:5]):  # Csak 5 query teljes teszt\n",
    "    try:\n",
    "        # Szimul√°lt completion (random ranking a teszthez)\n",
    "        test_completion = \",\".join(str(i) for i in np.random.permutation(len(slate_data[\"slate\"])))\n",
    "        \n",
    "        # Reward sz√°m√≠t√°s (ugyanaz a logika mint a reward_function-ben)\n",
    "        query_id = slate_data[\"query_id\"]\n",
    "        slate = slate_lookup[query_id]\n",
    "        relevance = [doc.get('relevance', 0) for doc in slate]\n",
    "        baseline = list(range(len(slate)))\n",
    "        predicted = parse_model_ranking(test_completion, len(slate))\n",
    "        \n",
    "        ndcg_baseline = calculate_ndcg(baseline, relevance, k=NDCG_K)\n",
    "        ndcg_policy = calculate_ndcg(predicted, relevance, k=NDCG_K)\n",
    "        reward = ndcg_policy - ndcg_baseline\n",
    "        \n",
    "        test_rewards.append(reward)\n",
    "        test_reward_details.append({\n",
    "            \"ndcg_baseline\": ndcg_baseline,\n",
    "            \"ndcg_policy\": ndcg_policy,\n",
    "            \"reward\": reward,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        test_rewards.append(-0.5)\n",
    "        test_reward_details.append({\"error\": str(e)})\n",
    "\n",
    "if test_rewards:\n",
    "    reward_mean = np.mean(test_rewards)\n",
    "    reward_std = np.std(test_rewards)\n",
    "    print(f\"   ‚úÖ Reward function m≈±k√∂dik\")\n",
    "    print(f\"      Reward mean: {reward_mean:.4f}\")\n",
    "    print(f\"      Reward std: {reward_std:.4f}\")\n",
    "    print(f\"      Reward range: [{min(test_rewards):.4f}, {max(test_rewards):.4f}]\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Reward function tesztel√©s nem siker√ºlt\")\n",
    "\n",
    "# 6. √ñsszefoglal√≥ √©s d√∂nt√©s\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"üìã DRY RUN VALIDATION √ñSSZEFOGLAL√ì\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validation_passed = True\n",
    "warnings = []\n",
    "\n",
    "if parsing_success_rate < DRY_RUN_PARSING_SUCCESS_THRESHOLD:\n",
    "    validation_passed = False\n",
    "    warnings.append(f\"‚ùå Parsing success rate t√∫l alacsony: {parsing_success_rate:.2%} < {DRY_RUN_PARSING_SUCCESS_THRESHOLD:.0%}\")\n",
    "\n",
    "if len(slate_issues) > len(test_slates) * 0.3:  # T√∂bb mint 30% slate probl√©ma\n",
    "    validation_passed = False\n",
    "    warnings.append(f\"‚ùå T√∫l sok slate probl√©ma: {len(slate_issues)}/{len(test_slates)}\")\n",
    "\n",
    "if test_rewards and np.mean(test_rewards) < DRY_RUN_MIN_REWARD_MEAN:\n",
    "    warnings.append(f\"‚ö†Ô∏è  Alacsony reward mean: {np.mean(test_rewards):.4f} (nem blokkolja)\")\n",
    "\n",
    "if avg_stats['baseline_ndcg'] < 0.1:\n",
    "    warnings.append(f\"‚ö†Ô∏è  Alacsony baseline nDCG: {avg_stats['baseline_ndcg']:.4f} (lehet, hogy rossz slate min≈ës√©g)\")\n",
    "\n",
    "print(\"\\n‚úÖ Pozit√≠v mutat√≥k:\")\n",
    "print(f\"   ‚úì Parsing success: {parsing_success_rate:.2%}\")\n",
    "print(f\"   ‚úì √Åtlagos baseline nDCG@10: {avg_stats['baseline_ndcg']:.4f}\")\n",
    "print(f\"   ‚úì √Åtlagos relev√°ns dokumentumok/slate: {avg_stats['relevant_docs']:.1f}\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\n‚ö†Ô∏è  Figyelmeztet√©sek:\")\n",
    "    for warning in warnings:\n",
    "        print(f\"   {warning}\")\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"\\n‚úÖ DRY RUN VALIDATION PASSED - Training folytathat√≥!\")\n",
    "    print(\"   A rendszer validnak t≈±nik, folytathatod a teljes tr√©ninget.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå DRY RUN VALIDATION FAILED - Training NEM AJ√ÅNLOTT!\")\n",
    "    print(\"   K√©rlek jav√≠tsd a hib√°kat a tr√©ning ind√≠t√°sa el≈ëtt.\")\n",
    "    print(\"   Ellen≈ërizd:\")\n",
    "    print(\"   - Prompt gener√°l√°s √©s query_id extraction\")\n",
    "    print(\"   - Slate strukt√∫ra √©s relevancia eloszl√°s\")\n",
    "    print(\"   - Reward function m≈±k√∂d√©s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Trainer konfigur√°ci√≥ - MEMORY EFFICIENT + CURRICULUM LEARNING\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    generation_batch_size=GENERATION_BATCH_SIZE,  # Explicit: LCM(6, 2) = 6\n",
    "    optim=OPTIMIZER_NAME,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    loss_type=\"dr_grpo\",\n",
    "    scale_rewards=\"batch\",\n",
    "    use_vllm=True,\n",
    "    vllm_importance_sampling_correction=False,  # vLLM token logprobs nem egyeznek az alap modell√©vel\n",
    "    importance_sampling_level=\"token\",\n",
    "    mask_truncated_completions=True,\n",
    "    epsilon=0.1,  # Enhanced: 0.2 ‚Üí 0.1 (smaller policy updates, more stable)\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False, \"use_unsloth\": True},\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    dataloader_num_workers=1,  # 2‚Üí1: cs√∂kkentett worker\n",
    "    seed=SEED,\n",
    "    max_completion_length=768,  # CRITICAL: explicit completion length (default 256 t√∫l kev√©s 30 dokumentumhoz)\n",
    ")\n",
    "\n",
    "grpo_config.unsloth_num_chunks = 1\n",
    "\n",
    "print(\"\\n‚úÖ GRPO Trainer konfigur√°ci√≥ k√©sz (Memory Efficient + Curriculum Learning)\")\n",
    "print(f\"  ‚úì generation_batch_size={GENERATION_BATCH_SIZE}\")\n",
    "print(f\"    - oszthat√≥ num_generations={NUM_GENERATIONS}-szel: {GENERATION_BATCH_SIZE}√∑{NUM_GENERATIONS}={GENERATION_BATCH_SIZE//NUM_GENERATIONS}\")\n",
    "print(f\"    - oszthat√≥ per_device_batch={PER_DEVICE_BATCH_SIZE}-gyel: {GENERATION_BATCH_SIZE}√∑{PER_DEVICE_BATCH_SIZE}={GENERATION_BATCH_SIZE//PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"  ‚úì Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚úì Mem√≥ria optimaliz√°ci√≥: 50% GPU util, 1 worker, rank 32\")\n",
    "if CURRICULUM_ENABLED:\n",
    "    print(f\"  ‚úì Curriculum Learning: {CURRICULUM_STRATEGY} strat√©gia aktiv√°lva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Multi-Metrika Reward Validation\n",
    "\n",
    "Most tesztelj√ºk a friss√≠tett multi-metrika reward f√ºggv√©nyt:\n",
    "- Sigmoid transzform√°ci√≥ m≈±k√∂d√©se\n",
    "- Komponensek (nDCG/MRR/Recall) sz√°m√≠t√°sa\n",
    "- MRR tie-break bonus\n",
    "- Diverzit√°s check\n",
    "- Baseline metrics bet√∂lt√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== MULTI-METRIKA REWARD VALIDATION ======\n",
    "# Gyors teszt: multi-metric reward function m≈±k√∂dik-e helyesen\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üß™ MULTI-METRIKA REWARD VALID√ÅCI√ì\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Teszt konfigur√°ci√≥\n",
    "NUM_TEST_QUERIES = 5\n",
    "test_results = []\n",
    "\n",
    "print(f\"\\nüìä {NUM_TEST_QUERIES} query tesztel√©se multi-metrika reward-dal...\\n\")\n",
    "\n",
    "for i, slate_data in enumerate(slates_data[:NUM_TEST_QUERIES]):\n",
    "    query_id = slate_data[\"query_id\"]\n",
    "    slate = slate_data[\"slate\"]\n",
    "    \n",
    "    print(f\"üîç Test {i+1}/{NUM_TEST_QUERIES}: {query_id[:60]}...\")\n",
    "    \n",
    "    # 1. Baseline metrik√°k check\n",
    "    if query_id in baseline_metrics_dict:\n",
    "        baseline = baseline_metrics_dict[query_id]\n",
    "        print(f\"   ‚úÖ Baseline metrik√°k: nDCG@10={baseline.get('ndcg@10', 0):.4f}, MRR@5={baseline.get('mrr@5', 0):.4f}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Nincs baseline metrika erre a query-re (fallback to zero)\")\n",
    "        baseline = {\"ndcg@10\": 0.0, \"mrr@5\": 0.0, \"recall@20\": 0.0}\n",
    "    \n",
    "    # 2. Szimul√°lt policy ranking (random permutation)\n",
    "    predicted_indices = list(np.random.permutation(len(slate)))\n",
    "    \n",
    "    # 3. Multi-metric reward sz√°m√≠t√°s\n",
    "    try:\n",
    "        reward, components = compute_multi_metric_reward(\n",
    "            predicted_indices=predicted_indices,\n",
    "            slate=slate,\n",
    "            baseline_metrics=baseline,\n",
    "            weights=REWARD_WEIGHTS,\n",
    "            clip_range=REWARD_CLIP_RANGE,\n",
    "            use_sigmoid=REWARD_USE_SIGMOID,\n",
    "            mrr_tiebreak_bonus=REWARD_MRR_TIEBREAK_BONUS,\n",
    "            diversity_penalty_weight=REWARD_DIVERSITY_PENALTY,\n",
    "        )\n",
    "        \n",
    "        # 4. Eredm√©nyek\n",
    "        print(f\"   üìà Policy metrik√°k:\")\n",
    "        print(f\"      nDCG@10:   {components['policy_ndcg10']:.4f} (baseline: {components['baseline_ndcg10']:.4f}, Œî={components['delta_ndcg10']:+.4f})\")\n",
    "        print(f\"      MRR@5:     {components['policy_mrr5']:.4f} (baseline: {components['baseline_mrr5']:.4f}, Œî={components['delta_mrr5']:+.4f})\")\n",
    "        print(f\"      Recall@20: {components['policy_recall20']:.4f} (baseline: {components['baseline_recall20']:.4f}, Œî={components['delta_recall20']:+.4f})\")\n",
    "        \n",
    "        print(f\"   üéØ Reward komponensek:\")\n",
    "        print(f\"      nDCG comp:  {components['component_ndcg10']:+.4f} (60% s√∫ly)\")\n",
    "        print(f\"      MRR comp:   {components['component_mrr5']:+.4f} (30% s√∫ly)\")\n",
    "        print(f\"      Recall comp:{components['component_recall20']:+.4f} (10% s√∫ly)\")\n",
    "        \n",
    "        print(f\"   üí∞ Total reward: {components['reward_total']:.4f} (raw: {components['reward_raw']:.4f})\")\n",
    "        \n",
    "        test_results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"reward\": reward,\n",
    "            \"components\": components,\n",
    "            \"success\": True,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Hiba: {e}\")\n",
    "        test_results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False,\n",
    "        })\n",
    "    \n",
    "    print()\n",
    "\n",
    "# √ñsszefoglal√≥\n",
    "success_count = sum(1 for r in test_results if r.get(\"success\", False))\n",
    "success_rate = success_count / len(test_results) if test_results else 0.0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä VALID√ÅCI√ì √ñSSZEFOGLAL√ì\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Sikeres reward sz√°m√≠t√°sok: {success_count}/{len(test_results)} ({success_rate:.0%})\")\n",
    "\n",
    "if success_count > 0:\n",
    "    successful_rewards = [r[\"reward\"] for r in test_results if r.get(\"success\")]\n",
    "    print(f\"\\nüìà Reward statisztik√°k:\")\n",
    "    print(f\"   Mean:   {np.mean(successful_rewards):.4f}\")\n",
    "    print(f\"   Std:    {np.std(successful_rewards):.4f}\")\n",
    "    print(f\"   Min:    {np.min(successful_rewards):.4f}\")\n",
    "    print(f\"   Max:    {np.max(successful_rewards):.4f}\")\n",
    "    print(f\"   Median: {np.median(successful_rewards):.4f}\")\n",
    "    \n",
    "    # Komponensek √°tlaga\n",
    "    if test_results[0].get(\"components\"):\n",
    "        avg_delta_ndcg = np.mean([r[\"components\"][\"delta_ndcg10\"] for r in test_results if r.get(\"success\")])\n",
    "        avg_delta_mrr = np.mean([r[\"components\"][\"delta_mrr5\"] for r in test_results if r.get(\"success\")])\n",
    "        avg_delta_recall = np.mean([r[\"components\"][\"delta_recall20\"] for r in test_results if r.get(\"success\")])\n",
    "        \n",
    "        print(f\"\\nüìä √Åtlagos delta metrik√°k:\")\n",
    "        print(f\"   Œî nDCG@10:   {avg_delta_ndcg:+.4f}\")\n",
    "        print(f\"   Œî MRR@5:     {avg_delta_mrr:+.4f}\")\n",
    "        print(f\"   Œî Recall@20: {avg_delta_recall:+.4f}\")\n",
    "\n",
    "if success_rate >= 0.8:\n",
    "    print(f\"\\n‚úÖ MULTI-METRIKA REWARD VALIDATION PASSED!\")\n",
    "    print(f\"   A reward function m≈±k√∂dik, folytathat√≥ a training.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå MULTI-METRIKA REWARD VALIDATION FAILED!\")\n",
    "    print(f\"   Ellen≈ërizd a baseline metrics bet√∂lt√©s√©t √©s a reward function implement√°ci√≥t.\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CURRICULUM LEARNING - Simplified Approach ======\n",
    "# Mivel a TRL trainer nem t√°mogatja a dataset dinamikus v√°ltoztat√°s√°t k√∂zben,\n",
    "# k√©t megold√°s van:\n",
    "# 1. Weighted sampling: k√∂nnyebb p√©ld√°kat gyakrabban v√°lasztjuk (implement√°lva)\n",
    "# 2. Filtered dataset: csak az easy/medium p√©ld√°kat haszn√°ljuk (simpler, de hat√°sos)\n",
    "\n",
    "CURRICULUM_STRATEGY = \"weighted_sampling\"  # vagy \"filtered_dataset\"\n",
    "\n",
    "if CURRICULUM_ENABLED and CURRICULUM_STRATEGY == \"filtered_dataset\":\n",
    "    # Egyszer≈± strat√©gia: csak easy + medium p√©ld√°kat haszn√°lunk (legjobb baseline nDCG)\n",
    "    # Ez biztos√≠tja, hogy a modell el≈ësz√∂r megtanulja a j√≥l rangsorolni, amikor van mit rangsorolni\n",
    "    print(\"\\nüìä Curriculum Learning: Filtered Dataset strat√©gia\")\n",
    "    print(\"   Csak easy + medium difficulty p√©ld√°kat haszn√°lunk a training-ben\")\n",
    "    \n",
    "    filtered_train_examples = [ex for ex in training_examples \n",
    "                              if ex[\"difficulty\"] in [\"easy\", \"medium\"]]\n",
    "    filtered_train_indices = [i for i in train_indices \n",
    "                             if training_examples[i][\"difficulty\"] in [\"easy\", \"medium\"]]\n",
    "    \n",
    "    train_dataset = full_dataset.select(filtered_train_indices)\n",
    "    \n",
    "    print(f\"   Eredeti training examples: {len(training_examples)}\")\n",
    "    print(f\"   Filtered training examples: {len(filtered_train_examples)} ({100*len(filtered_train_examples)/len(training_examples):.1f}%)\")\n",
    "    print(f\"   √Åtlagos baseline nDCG (filtered): {np.mean([ex['baseline_ndcg'] for ex in filtered_train_examples]):.4f}\")\n",
    "    print(f\"   vs. Eredeti √°tlag: {np.mean([ex['baseline_ndcg'] for ex in training_examples]):.4f}\")\n",
    "\n",
    "# Trainer inicializ√°l√°sa\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[multi_metric_reward_function],  # UPDATED: Multi-metric reward\n",
    "    args=grpo_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ GRPO Trainer inicializ√°lva\")\n",
    "print(f\"  ‚úì Reward function: Multi-Metric (nDCG@10 + MRR@5 + Recall@20)\")\n",
    "if CURRICULUM_ENABLED:\n",
    "    print(f\"  ‚úì Curriculum Learning: {CURRICULUM_STRATEGY}\")\n",
    "    print(f\"  ‚úì Training queries: {len(train_dataset)}\")\n",
    "else:\n",
    "    print(f\"  Training queries: {len(train_dataset)}\")\n",
    "print(f\"  Eval queries: {len(eval_dataset)}\")\n",
    "print(f\"  GPU mem√≥ria: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ind√≠t√°sa\n",
    "print(\"\\nüöÄ GRPO TRAINING IND√çT√ÅSA\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if CURRICULUM_ENABLED:\n",
    "    print(f\"üìö Curriculum Learning akt√≠v: {CURRICULUM_STRATEGY}\")\n",
    "    if CURRICULUM_STRATEGY == \"filtered_dataset\":\n",
    "        print(\"   Training k√∂nnyebb p√©ld√°kkal (easy + medium) - jobb tanul√°si signal\")\n",
    "    print()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Training sikeresen befejezve!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training hiba: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Nincs el√©rhet≈ë trainer.state.log_history √©s a metrics.json f√°jl sem tal√°lhat√≥.\n"
     ]
    }
   ],
   "source": [
    "# Robusztus GRPO tr√©ning vizualiz√°ci√≥ (hibat≈±r≈ë)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Megpr√≥b√°ljuk a TRL trainer log_history-j√°t haszn√°lni; ha nincs, es√ºnk vissza a metrics.json-re\n",
    "log_history = None\n",
    "try:\n",
    "    _ = trainer  # ellen≈ërz√©s, hogy l√©tezik-e\n",
    "    log_history = getattr(trainer.state, \"log_history\", None)\n",
    "except NameError:\n",
    "    log_history = None\n",
    "\n",
    "# Ha nincs log_history, pr√≥b√°ljuk a metrics.json-b√≥l kirajzolni legal√°bb a jutalom trendet\n",
    "if not log_history:\n",
    "    metrics_path = METRICS_FILE if 'METRICS_FILE' in globals() else Path(\"data/models/grpo_policy/metrics.json\")\n",
    "    if Path(metrics_path).exists():\n",
    "        with open(metrics_path, 'r', encoding='utf-8') as f:\n",
    "            metrics = json.load(f)\n",
    "        rewards_trend = metrics.get(\"training_rewards\", {}).get(\"trend\", [])\n",
    "        if rewards_trend:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(rewards_trend, color=\"royalblue\")\n",
    "            plt.title(\"Jutalom trend (metrics.json)\")\n",
    "            plt.xlabel(\"L√©p√©s\")\n",
    "            plt.ylabel(\"Jutalom\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Nincs el√©rhet≈ë trainer.state.log_history √©s a metrics.json nem tartalmaz 'training_rewards.trend'-et.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nincs el√©rhet≈ë trainer.state.log_history √©s a metrics.json f√°jl sem tal√°lhat√≥.\")\n",
    "else:\n",
    "    # Innent≈ël a log_history alapj√°n dolgozunk\n",
    "    logs = pd.DataFrame(log_history)\n",
    "    if logs.empty:\n",
    "        print(\"‚ö†Ô∏è A logok DataFrame √ºres.\")\n",
    "    else:\n",
    "        # Csak numerikus oszlopok megtart√°sa\n",
    "        numeric_cols = [c for c in logs.columns if np.issubdtype(pd.Series(logs[c]).dropna().dtype, np.number)]\n",
    "        if not numeric_cols:\n",
    "            print(\"‚ö†Ô∏è A logokban nincs numerikus oszlop a megjelen√≠t√©shez.\")\n",
    "        else:\n",
    "            logs = logs[numeric_cols].copy()\n",
    "\n",
    "            def pick_col(df, candidates):\n",
    "                for c in candidates:\n",
    "                    if c in df.columns:\n",
    "                        return c\n",
    "                return None\n",
    "\n",
    "            # Reward (√°tlag) oszlop kiv√°laszt√°sa t√∂bb lehets√©ges kulccsal\n",
    "            reward_candidates = [\n",
    "                \"rewards/reward_function/mean\",  # TRL GRPO gyakori\n",
    "                \"rewards/mean\",\n",
    "                \"reward/mean\",\n",
    "                \"rewards/reward\",\n",
    "                \"reward\",\n",
    "            ]\n",
    "            reward_col = pick_col(logs, reward_candidates)\n",
    "\n",
    "            # Reward sz√≥r√°s oszlop (opcion√°lis)\n",
    "            reward_std_candidates = [\n",
    "                \"rewards/reward_function/std\",\n",
    "                \"rewards/std\",\n",
    "                \"reward/std\",\n",
    "            ]\n",
    "            reward_std_col = pick_col(logs, reward_std_candidates)\n",
    "\n",
    "            # Loss √©s KL oszlopok (opcion√°lis)\n",
    "            loss_col = pick_col(logs, [\"loss\", \"train/loss\"])  \n",
    "            kl_col = pick_col(logs, [\"kl\", \"train/kl\", \"kl_divergence\"]) \n",
    "\n",
    "            # √Åbr√°k l√©trehoz√°sa dinamikusan az el√©rhet≈ë metrik√°k alapj√°n\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(13, 8))\n",
    "            axes = axes.flatten()\n",
    "            ax_idx = 0\n",
    "\n",
    "            # 1) Reward trend (sim√≠tott) + opcion√°lis sz√≥r√°s s√°v\n",
    "            if reward_col is not None:\n",
    "                reward_smooth = logs[reward_col].rolling(window=10, min_periods=1).mean()\n",
    "                axes[ax_idx].plot(reward_smooth, label=f\"Jutalom (√°tlag, {reward_col})\", color=\"royalblue\")\n",
    "                if reward_std_col is not None:\n",
    "                    lo = reward_smooth - logs[reward_std_col]\n",
    "                    hi = reward_smooth + logs[reward_std_col]\n",
    "                    axes[ax_idx].fill_between(logs.index, lo, hi, alpha=0.2, color=\"royalblue\", label=\"¬± sz√≥r√°s\")\n",
    "                axes[ax_idx].set_title(\"Jutalom trend\")\n",
    "                axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "                axes[ax_idx].set_ylabel(\"Jutalom\")\n",
    "                axes[ax_idx].legend()\n",
    "                ax_idx += 1\n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è Nem tal√°lhat√≥ reward √°tlag oszlop a logokban. Kihagyom a jutalom g√∂rb√©t.\")\n",
    "\n",
    "            # 2) Loss g√∂rbe\n",
    "            if loss_col is not None and ax_idx < len(axes):\n",
    "                axes[ax_idx].plot(logs[loss_col], label=\"Vesztes√©g (loss)\", color=\"firebrick\")\n",
    "                axes[ax_idx].set_title(\"Tan√≠t√°si vesztes√©g\")\n",
    "                axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "                axes[ax_idx].set_ylabel(\"Loss\")\n",
    "                axes[ax_idx].legend()\n",
    "                ax_idx += 1\n",
    "\n",
    "            # 3) KL g√∂rbe\n",
    "            if kl_col is not None and ax_idx < len(axes):\n",
    "                axes[ax_idx].plot(logs[kl_col], label=\"KL divergencia\", color=\"darkorange\")\n",
    "                axes[ax_idx].set_title(\"KL divergencia alakul√°sa\")\n",
    "                axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "                axes[ax_idx].set_ylabel(\"KL\")\n",
    "                axes[ax_idx].legend()\n",
    "                ax_idx += 1\n",
    "\n",
    "            # 4) Reward sz√≥r√°s k√ºl√∂n (ha van √©s maradt hely)\n",
    "            if reward_std_col is not None and ax_idx < len(axes):\n",
    "                axes[ax_idx].plot(logs[reward_std_col], label=\"Jutalom sz√≥r√°s\", color=\"seagreen\")\n",
    "                axes[ax_idx].set_title(\"Jutalom sz√≥r√°s\")\n",
    "                axes[ax_idx].set_xlabel(\"L√©p√©s\")\n",
    "                axes[ax_idx].set_ylabel(\"Sz√≥r√°s\")\n",
    "                axes[ax_idx].legend()\n",
    "                ax_idx += 1\n",
    "\n",
    "            # Ha kevesebb metrika √°ll rendelkez√©sre, a marad√©k tengelyeket rejts√ºk el\n",
    "            for i in range(ax_idx, len(axes)):\n",
    "                axes[i].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Prompt Quality Check - Sample Completions\n",
    "# Futtasd TRAINING EL≈êTT, hogy l√°sd, milyen outputokat gener√°l a modell\n",
    "\n",
    "print(\"üî¨ PROMPT QUALITY CHECK - Sample Completions Gener√°l√°sa\\n\")\n",
    "print(\"Ez a cella teszteli, hogy a modell milyen outputokat gener√°l az √∫j prompttal.\")\n",
    "print(\"FONTOS: Ez optional, csak debug c√©lra.\\n\")\n",
    "\n",
    "# Egy random query kiv√°laszt√°sa\n",
    "import random\n",
    "random.seed(42)\n",
    "test_slate_idx = random.randint(0, len(slates_data)-1)\n",
    "test_slate_data = slates_data[test_slate_idx]\n",
    "test_prompt_text = create_training_prompt(test_slate_data[\"query_id\"], test_slate_data[\"slate\"])\n",
    "\n",
    "# Gener√°lunk n√©h√°ny completion-t\n",
    "print(f\"Query: {test_slate_data['query_id'][:80]}...\")\n",
    "print(f\"Slate size: {len(test_slate_data['slate'])}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    # Model inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(test_prompt_text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    print(\"\\nüé≤ 3 Sample Completion Gener√°l√°sa...\\n\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=768,  # N√∂velve 100-r√≥l 768-re, hogy elegend≈ë legyen 30 dokumentum rangsorol√°s√°hoz\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,  # Stop sequence hozz√°ad√°sa\n",
    "            )\n",
    "        \n",
    "        completion = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        parsed = parse_model_ranking(completion, len(test_slate_data[\"slate\"]))\n",
    "        \n",
    "        print(f\"Completion {i+1}:\")\n",
    "        print(f\"  Raw output: {completion[:150]}\")\n",
    "        print(f\"  Parsed indices: {parsed[:10]}\")  # First 10 indices\n",
    "        print(f\"  Valid? {len(parsed) == len(test_slate_data['slate']) and len(set(parsed)) == len(parsed)}\")\n",
    "        print()\n",
    "    \n",
    "    # Relevance info\n",
    "    relevances = [doc.get('relevance', 0) for doc in test_slate_data[\"slate\"]]\n",
    "    print(f\"üìä Ground truth relevance distribution:\")\n",
    "    print(f\"  Relevant docs (rel>=1): {sum(1 for r in relevances if r >= 1)}/{len(relevances)}\")\n",
    "    print(f\"  Highly relevant (rel=2): {sum(1 for r in relevances if r == 2)}/{len(relevances)}\")\n",
    "    print(f\"  Baseline order nDCG@10: {calculate_ndcg(list(range(len(relevances))), relevances, k=10):.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\\n‚úÖ Prompt quality check complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during quality check: {e}\")\n",
    "    print(\"Ez nem probl√©ma, a training ett≈ël m√©g futhat.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifactumok ment√©se\n",
    "print(\"\\nüíæ Artifactumok ment√©se...\")\n",
    "model.save_pretrained_merged(str(OUTPUT_DIR), tokenizer, save_method=\"lora\")\n",
    "print(f\"  ‚úÖ LoRA adapter: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Modell √ñsszehasonl√≠t√°s\n",
    "\n",
    "Az √©rt√©kel√©s √©s a baseline vs GRPO √∂sszehasonl√≠t√°s a **`notebooks/model_comparison.ipynb`** notebookban t√∂rt√©nik.\n",
    "\n",
    "Ez biztos√≠tja, hogy a baseline √©s GRPO modell **ugyanazon eval halmazon** legyen ki√©rt√©kelve, √≠gy a metrik√°k √∂sszevethet≈ëk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Az √©rt√©kel√©s a notebooks/model_comparison.ipynb notebookban t√∂rt√©nik.\n",
    "# Ez biztos√≠tja, hogy a baseline √©s GRPO modell ugyanazon eval halmazon legyen ki√©rt√©kelve.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courtrankrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
