{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chunk-data-analysis-header",
   "metadata": {},
   "source": [
    "# Chunk Adatok Ki√©rt√©kel√©se - CourtRankRL Projekt\n",
    "\n",
    "Ez a notebook a chunks.jsonl f√°jlban tal√°lhat√≥ chunk adatokat elemzi. Az agents.md specifik√°ci√≥ alapj√°n k√©sz√≠tett ki√©rt√©kel√©si szempontokat vizsg√°lja a jelenlegi adatokkal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-config",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Projekt konfigur√°ci√≥ bet√∂lt√©se\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m project_root = Path(\u001b[34;43m__file__\u001b[39;49m).parent.parent.parent\n\u001b[32m     19\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfigs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "# Plot st√≠lus be√°ll√≠t√°sa\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Projekt konfigur√°ci√≥ bet√∂lt√©se\n",
    "import sys\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "from configs import config\n",
    "\n",
    "print(\"CourtRankRL - Chunk Data Analysis\")\n",
    "print(f\"Chunks: {config.CHUNKS_JSONL}\")\n",
    "print(f\"Processed docs: {config.PROCESSED_DOCS_LIST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-chunk-data",
   "metadata": {},
   "source": [
    "## 1. Chunk Adatok Bet√∂lt√©se\n",
    "\n",
    "A chunks.jsonl f√°jl bet√∂lt√©se mintav√©telez√©ssel a teljes√≠tm√©ny √©rdek√©ben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-chunks-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunkok bet√∂lt√©se mintav√©telez√©ssel\n",
    "chunks_file = config.CHUNKS_JSONL\n",
    "sample_size = min(50000, 50000)  # Maximum 50k chunk elemz√©sre\n",
    "\n",
    "df = None\n",
    "print(f\"Chunkok bet√∂lt√©se: {chunks_file}\")\n",
    "print(f\"Mintav√©tel: {sample_size} chunk\")\n",
    "\n",
    "if chunks_file.exists():\n",
    "    try:\n",
    "        chunks_list = []\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                try:\n",
    "                    chunk = json.loads(line.strip())\n",
    "                    chunks_list.append(chunk)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        if chunks_list:\n",
    "            df = pd.DataFrame(chunks_list)\n",
    "            print(f\"‚úÖ Bet√∂lt√∂tt chunkok sz√°ma: {len(df)}\")\n",
    "            print(f\"Oszlopok: {df.columns.tolist()}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Nem tal√°lhat√≥ak chunk adatok\")\n",
    "            df = None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hiba a chunkok bet√∂lt√©se sor√°n: {e}\")\n",
    "        df = None\n",
    "else:\n",
    "    print(f\"‚ùå Chunks f√°jl nem tal√°lhat√≥: {chunks_file}\")\n",
    "    print(\"Futtassa a build pipeline-t el≈ësz√∂r: uv run courtrankrl build\")\n",
    "    df = None\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(f\"\\nAdatok bet√∂ltve: {df.shape[0]} chunk, {df.shape[1]} oszlop\")\n",
    "    print(f\"Els≈ë chunk sz√∂vege (r√©szlet):\")\n",
    "    first_text = df['text'].iloc[0] if 'text' in df.columns else 'N/A'\n",
    "    print(f\"{str(first_text)[:200]}...\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Nincs adat az elemz√©shez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-info-analysis",
   "metadata": {},
   "source": [
    "## 2. Alapvet≈ë Inform√°ci√≥k √©s Statisztik√°k\n",
    "\n",
    "A chunk adatok alapvet≈ë statisztik√°inak elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-basic-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(\"üìä Chunk adatok alapvet≈ë inform√°ci√≥i:\")\n",
    "    \n",
    "    # Adatt√≠pusok\n",
    "    print(f\"Adatok alakja: {df.shape}\")\n",
    "    print(f\"Oszlopok: {df.columns.tolist()}\")\n",
    "    df.info()\n",
    "    \n",
    "    # Mintav√©tel az els≈ë n√©h√°ny chunkb√≥l\n",
    "    print(\"\\nEls≈ë 3 chunk adatai:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        chunk = df.iloc[i]\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"  doc_id: {chunk.get('doc_id', 'N/A')}\")\n",
    "        print(f\"  chunk_id: {chunk.get('chunk_id', 'N/A')}\")\n",
    "        print(f\"  birosag: {chunk.get('birosag', 'N/A')}\")\n",
    "        print(f\"  JogTerulet: {chunk.get('JogTerulet', 'N/A')}\")\n",
    "        if 'text' in chunk:\n",
    "            text_preview = str(chunk['text'])[:100] + \"...\" if len(str(chunk['text'])) > 100 else str(chunk['text'])\n",
    "            print(f\"  text: {text_preview}\")\n",
    "    \n",
    "    # Le√≠r√≥ statisztik√°k\n",
    "    print(\"\\nüìà Le√≠r√≥ statisztik√°k:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        display(df[numeric_cols].describe())\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        display(df[categorical_cols].describe())\n",
    "else:\n",
    "    print(\"‚ùå Nincs adat az alapvet≈ë inform√°ci√≥k elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-values-analysis",
   "metadata": {},
   "source": [
    "## 3. Hi√°nyz√≥ √ârt√©kek Elemz√©se\n",
    "\n",
    "A hi√°nyz√≥ √©rt√©kek azonos√≠t√°sa chunk szinten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-missing-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(\"üîç Chunk adatok hi√°nyz√≥ √©rt√©kei:\")\n",
    "    \n",
    "    # Hi√°nyz√≥ √©rt√©kek statisztik√°i\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({'Darabsz√°m': missing_values, 'Sz√°zal√©k': missing_percent})\n",
    "    missing_df = missing_df[missing_df['Darabsz√°m'] > 0].sort_values(by='Sz√°zal√©k', ascending=False)\n",
    "    \n",
    "    print(\"Hi√°nyz√≥ √©rt√©kek oszloponk√©nt:\")\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Hi√°nyz√≥ √©rt√©kek vizualiz√°ci√≥ja\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Hi√°nyz√≥ √©rt√©kek eloszl√°sa a chunk adatokban')\n",
    "    plt.show()\n",
    "    \n",
    "    # Kritikus hi√°nyz√≥ √©rt√©kek\n",
    "    critical_missing = missing_df[missing_df['Sz√°zal√©k'] > 50]  # 50% feletti hi√°ny\n",
    "    if not critical_missing.empty:\n",
    "        print(f\"‚ö†Ô∏è Kritikus hi√°nyz√≥ √©rt√©kek (>50%): {len(critical_missing)} oszlop\")\n",
    "        display(critical_missing)\n",
    "    else:\n",
    "        print(\"‚úÖ Nincsenek kritikus hi√°nyz√≥ √©rt√©kek\")\n",
    "else:\n",
    "    print(\"‚ùå Nincs adat a hi√°nyz√≥ √©rt√©kek elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-length-analysis",
   "metadata": {},
   "source": [
    "## 4. Sz√∂veghossz Elemz√©se\n",
    "\n",
    "A chunkok sz√∂vegeinek hossza (karakterek √©s szavak sz√°ma szerint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-text-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns:\n",
    "    print(\"üìè Sz√∂veghossz elemz√©se:\")\n",
    "    \n",
    "    # Karakterek √©s szavak sz√°ma\n",
    "    df['karakter_szam'] = df['text'].astype(str).apply(len)\n",
    "    df['szo_szam'] = df['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "    \n",
    "    print(\"Sz√∂veghossz statisztik√°k:\")\n",
    "    display(df[['karakter_szam', 'szo_szam']].describe())\n",
    "    \n",
    "    # Embedding modell k√∂vetelm√©nyeinek ellen≈ërz√©se\n",
    "    max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "    long_chunks = df[df['karakter_szam'] > max_length]\n",
    "    print(f\"\\nChunkok sz√°ma, amelyek hosszabbak {max_length} karaktern√©l: {len(long_chunks)} ({100*len(long_chunks)/len(df):.1f}%)\")\n",
    "    \n",
    "    if len(long_chunks) > 0:\n",
    "        print(f\"Leghosszabb chunk: {df['karakter_szam'].max()} karakter\")\n",
    "        print(f\"Legr√∂videbb hossz√∫ chunk: {long_chunks['karakter_szam'].min()} karakter\")\n",
    "        \n",
    "        # Leghosszabb chunk r√©szlete\n",
    "        longest_chunk = df.loc[df['karakter_szam'].idxmax()]\n",
    "        print(f\"\\nLeghosszabb chunk r√©szlete:\")\n",
    "        print(f\"doc_id: {longest_chunk.get('doc_id', 'N/A')}\")\n",
    "        print(f\"chunk_id: {longest_chunk.get('chunk_id', 'N/A')}\")\n",
    "        print(f\"text: {str(longest_chunk['text'])[:200]}...\")\n",
    "    \n",
    "    # Eloszl√°sok vizualiz√°ci√≥ja\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Karakterek sz√°m√°nak eloszl√°sa\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(x=df['karakter_szam'], bins=50, kde=True)\n",
    "    plt.title('Chunk karakterhossz eloszl√°sa')\n",
    "    plt.xlabel('Karakterek sz√°ma')\n",
    "    plt.ylabel('Chunkok sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Szavak sz√°m√°nak eloszl√°sa\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(x=df['szo_szam'], bins=50, kde=True)\n",
    "    plt.title('Chunk sz√≥sz√°m eloszl√°sa')\n",
    "    plt.xlabel('Szavak sz√°ma')\n",
    "    plt.ylabel('Chunkok sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplotok\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(y=df['karakter_szam'])\n",
    "    plt.title('Chunk karakterhossz boxplot')\n",
    "    plt.ylabel('Karakterek sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df['szo_szam'])\n",
    "    plt.title('Chunk sz√≥sz√°m boxplot')\n",
    "    plt.ylabel('Szavak sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Nincs 'text' oszlop a sz√∂veghossz elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorical-analysis",
   "metadata": {},
   "source": [
    "## 5. Kategorikus V√°ltoz√≥k Elemz√©se\n",
    "\n",
    "A chunkok metadatainak kategorikus v√°ltoz√≥i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-categorical-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_categories(df, column_name, top_n=20):\n",
    "    \"\"\"Seg√©df√ºggv√©ny a leggyakoribb kateg√≥ri√°k megjelen√≠t√©s√©re.\"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"‚ùå '{column_name}' oszlop nem tal√°lhat√≥\")\n",
    "        return\n",
    "    \n",
    "    counts = df[column_name].value_counts()\n",
    "    print(f\"\\n'{column_name}' egyedi √©rt√©keinek sz√°ma: {counts.nunique()}\")\n",
    "    print(f\"Leggyakoribb {top_n} √©rt√©k:\")\n",
    "    display(counts.head(top_n))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    counts.head(top_n).plot(kind='bar')\n",
    "    plt.title(f'Chunkok megoszl√°sa - {column_name} (Top {top_n})')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Chunkok sz√°ma')\n",
    "    plt.xticks(rotation=75, ha='right')\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"üìä Kategorikus v√°ltoz√≥k elemz√©se:\")\n",
    "    \n",
    "    # B√≠r√≥s√°g elemz√©se\n",
    "    if 'birosag' in df.columns:\n",
    "        plot_top_categories(df, 'birosag', top_n=30)\n",
    "    \n",
    "    # Jogter√ºlet elemz√©se\n",
    "    if 'JogTerulet' in df.columns:\n",
    "        plot_top_categories(df, 'JogTerulet', top_n=20)\n",
    "    \n",
    "    # Egy√©b fontos kategorikus v√°ltoz√≥k\n",
    "    for col in ['MeghozoBirosag', 'Kollegium', 'Azonosito']:\n",
    "        if col in df.columns:\n",
    "            plot_top_categories(df, col, top_n=15)\n",
    "else:\n",
    "    print(\"‚ùå Nincs adat a kategorikus v√°ltoz√≥k elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word-analysis",
   "metadata": {},
   "source": [
    "## 6. Gyakori Szavak Elemz√©se\n",
    "\n",
    "A chunkokban leggyakrabban el≈ëfordul√≥ szavak elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-word-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns:\n",
    "    print(\"üî§ Gyakori szavak elemz√©se:\")\n",
    "    \n",
    "    # Mivel nagy lehet az adathalmaz, vegy√ºnk egy mint√°t\n",
    "    sample_size = min(len(df), 10000)\n",
    "    sample_df = df.sample(n=sample_size, random_state=42)\n",
    "    print(f\"Sz√≥elemz√©s mintanagys√°ga: {sample_size} chunk\")\n",
    "    \n",
    "    # Egyszer≈± tokeniz√°l√°s\n",
    "    def simple_tokenize(text):\n",
    "        return [word.lower() for word in text.split() if word.strip()]\n",
    "    \n",
    "    # Szavak gyakoris√°g√°nak sz√°m√≠t√°sa\n",
    "    word_counts = Counter()\n",
    "    total_words = 0\n",
    "    \n",
    "    for text in sample_df['text'].astype(str):\n",
    "        tokens = simple_tokenize(text)\n",
    "        word_counts.update(tokens)\n",
    "        total_words += len(tokens)\n",
    "    \n",
    "    # Top 30 sz√≥ kiv√°laszt√°sa\n",
    "    top_words = word_counts.most_common(30)\n",
    "    word_freq = pd.DataFrame(top_words, columns=['szo', 'gyakorisag'])\n",
    "    \n",
    "    print(f\"Top 30 leggyakoribb sz√≥ (√∂sszes sz√≥: {total_words:,}):\")\n",
    "    display(word_freq)\n",
    "    \n",
    "    # Vizualiz√°ci√≥\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='gyakorisag', y='szo', data=word_freq)\n",
    "    plt.title('Top 30 leggyakoribb sz√≥ a chunkokban')\n",
    "    plt.xlabel('Gyakoris√°g')\n",
    "    plt.ylabel('Sz√≥')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # T√∂ltel√©kszavak ar√°nya\n",
    "    stop_words = ['√©s', 'a', 'az', 'de', 'hogy', 'is', 'nem', 'van', 'lesz', 'volt', 'mint']\n",
    "    stop_word_count = sum(word_counts.get(word, 0) for word in stop_words)\n",
    "    stop_word_ratio = stop_word_count / total_words if total_words > 0 else 0\n",
    "    print(f\"\\nT√∂ltel√©kszavak ar√°nya: {stop_word_ratio:.2%}\")\n",
    "    print(f\"T√∂ltel√©kszavak √∂sszesen: {stop_word_count:,}\")\n",
    "    \n",
    "    # Sz√≥hossz eloszl√°s\n",
    "    word_lengths = [len(word) for word in word_counts.keys()]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(word_lengths, bins=range(1, max(word_lengths) + 2), alpha=0.7)\n",
    "    plt.title('Sz√≥hossz eloszl√°sa a chunkokban')\n",
    "    plt.xlabel('Sz√≥ hossza (karakter)')\n",
    "    plt.ylabel('Szavak sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Nincs 'text' oszlop a gyakori szavak elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunk-structure-analysis",
   "metadata": {},
   "source": [
    "## 7. Chunk Szerkezet Elemz√©se\n",
    "\n",
    "A chunkok bels≈ë szerkezet√©nek √©s tartalm√°nak elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-chunk-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns:\n",
    "    print(\"üèóÔ∏è Chunk szerkezet elemz√©se:\")\n",
    "    \n",
    "    # Els≈ë chunk teljes tartalm√°nak megjelen√≠t√©se\n",
    "    first_chunk_text = df['text'].iloc[0] if len(df) > 0 else ''\n",
    "    print(\"\\nEls≈ë chunk teljes sz√∂vege:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(first_chunk_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sz√∂veg strukt√∫ra elemz√©s\n",
    "    sample_df = df.sample(min(1000, len(df)), random_state=42)\n",
    "    \n",
    "    # Mondatok √©s bekezd√©sek elemz√©se\n",
    "    sample_df['mondat_szam'] = sample_df['text'].astype(str).apply(\n",
    "        lambda x: len([s for s in x.split('.') if s.strip()]) if '.' in x else 1\n",
    "    )\n",
    "    sample_df['bekezdes_szam'] = sample_df['text'].astype(str).apply(\n",
    "        lambda x: len([p for p in x.split('\\n') if p.strip()]) if '\\n' in x else 1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSz√∂veg strukt√∫ra statisztik√°k (minta: {len(sample_df)} chunk):\")\n",
    "    display(sample_df[['mondat_szam', 'bekezdes_szam']].describe())\n",
    "    \n",
    "    # Strukt√∫ra vizualiz√°ci√≥\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(x=sample_df['mondat_szam'], bins=30, kde=True)\n",
    "    plt.title('Mondatok sz√°ma chunkonk√©nt')\n",
    "    plt.xlabel('Mondatok sz√°ma')\n",
    "    plt.ylabel('Chunkok sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(x=sample_df['bekezdes_szam'], bins=30, kde=True)\n",
    "    plt.title('Bekezd√©sek sz√°ma chunkonk√©nt')\n",
    "    plt.xlabel('Bekezd√©sek sz√°ma')\n",
    "    plt.ylabel('Chunkok sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sz√∂veg komplexit√°s\n",
    "    sample_df['atlag_mondat_hossz'] = sample_df.apply(\n",
    "        lambda row: row['karakter_szam'] / max(row['mondat_szam'], 1), axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSz√∂veg komplexit√°s:\")\n",
    "    display(sample_df['atlag_mondat_hossz'].describe())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sample_df['atlag_mondat_hossz'], bins=50, alpha=0.7)\n",
    "    plt.title('√Åtlagos mondathossz eloszl√°sa')\n",
    "    plt.xlabel('Karakterek mondatonk√©nt')\n",
    "    plt.ylabel('Chunkok sz√°ma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Nincs 'text' oszlop a chunk szerkezet elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-correlation",
   "metadata": {},
   "source": [
    "## 8. Metadatok √©s Sz√∂veg Kapcsolata\n",
    "\n",
    "A chunk metadatok √©s a sz√∂veges tartalom k√∂z√∂tti kapcsolatok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-metadata-text-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns and 'karakter_szam' in df.columns:\n",
    "    print(\"üîó Metadatok √©s sz√∂veg kapcsolata:\")\n",
    "    \n",
    "    # Sz√∂veghossz √©s kategorikus v√°ltoz√≥k kapcsolata\n",
    "    categorical_cols = ['birosag', 'JogTerulet', 'MeghozoBirosag']\n",
    "    available_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    \n",
    "    for col in available_cols[:3]:  # Maximum 3 kategorikus v√°ltoz√≥\n",
    "        if df[col].nunique() <= 20:  # Csak ha nem t√∫l sok egyedi √©rt√©k\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            top_categories = df[col].value_counts().nlargest(10).index\n",
    "            df_filtered = df[df[col].isin(top_categories)]\n",
    "            \n",
    "            sns.boxplot(data=df_filtered, x=col, y='karakter_szam')\n",
    "            plt.title(f'Sz√∂veghossz eloszl√°sa - {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Karakterek sz√°ma')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    \n",
    "    # Korrel√°ci√≥ numerikus v√°ltoz√≥k k√∂z√∂tt\n",
    "    numeric_cols = ['karakter_szam', 'szo_szam', 'HatarozatEve']\n",
    "    available_numeric = [col for col in numeric_cols if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    if len(available_numeric) > 1:\n",
    "        correlation_matrix = df[available_numeric].corr()\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Numerikus v√°ltoz√≥k korrel√°ci√≥ja')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nKorrel√°ci√≥s m√°trix:\")\n",
    "        display(correlation_matrix)\n",
    "else:\n",
    "    print(\"‚ùå Nincs el√©g adat a metadatok √©s sz√∂veg kapcsolat elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-check",
   "metadata": {},
   "source": [
    "## 9. Chunk Min≈ës√©g Ellen≈ërz√©se\n",
    "\n",
    "A chunking min≈ës√©g√©nek √©s konzisztenci√°j√°nak ellen≈ërz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-chunk-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(\"‚úÖ Chunk min≈ës√©g ellen≈ërz√©se:\")\n",
    "    \n",
    "    # Alapvet≈ë ellen≈ërz√©sek\n",
    "    quality_issues = {}\n",
    "    \n",
    "    # 1. √úres vagy t√∫l r√∂vid chunkok\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        empty_chunks = df[df['karakter_szam'] < 50].shape[0]\n",
    "        quality_issues['T√∫l r√∂vid chunkok (<50 karakter)'] = empty_chunks\n",
    "    \n",
    "    # 2. T√∫l hossz√∫ chunkok\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "        long_chunks = df[df['karakter_szam'] > max_length].shape[0]\n",
    "        quality_issues[f'T√∫l hossz√∫ chunkok (>{max_length} karakter)'] = long_chunks\n",
    "    \n",
    "    # 3. Hi√°nyz√≥ metadatok\n",
    "    required_cols = ['doc_id', 'chunk_id']\n",
    "    for col in required_cols:\n",
    "        if col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                quality_issues[f'Hi√°nyz√≥ {col}'] = missing\n",
    "    \n",
    "    # 4. Duplik√°lt chunk_id-k\n",
    "    if 'chunk_id' in df.columns:\n",
    "        duplicates = df['chunk_id'].duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            quality_issues['Duplik√°lt chunk_id-k'] = duplicates\n",
    "    \n",
    "    # 5. Hi√°nyz√≥ sz√∂veg\n",
    "    if 'text' in df.columns:\n",
    "        empty_text = df[df['text'].astype(str).str.len() < 10].shape[0]\n",
    "        if empty_text > 0:\n",
    "            quality_issues['T√∫l r√∂vid/√ºres sz√∂veg'] = empty_text\n",
    "    \n",
    "    # Eredm√©nyek megjelen√≠t√©se\n",
    "    if quality_issues:\n",
    "        print(\"\\n‚ö†Ô∏è Chunk min≈ës√©gi probl√©m√°k:\")\n",
    "        for issue, count in quality_issues.items():\n",
    "            print(f\"  {issue}: {count} db ({100*count/len(df):.1f}%)\")\n",
    "        \n",
    "        # Probl√©m√°s chunkok r√©szletei\n",
    "        print(\"\\nProbl√©m√°s chunkok r√©szletei:\")\n",
    "        problematic_mask = pd.Series(False, index=df.index)\n",
    "        \n",
    "        if 'karakter_szam' in df.columns:\n",
    "            problematic_mask |= (df['karakter_szam'] < 50)\n",
    "            problematic_mask |= (df['karakter_szam'] > max_length)\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            problematic_mask |= (df['text'].astype(str).str.len() < 10)\n",
    "        \n",
    "        if problematic_mask.any():\n",
    "            problematic_df = df[problematic_mask].head(5)\n",
    "            for i, (_, chunk) in enumerate(problematic_df.iterrows()):\n",
    "                print(f\"\\nProbl√©m√°s chunk {i+1}:\")\n",
    "                print(f\"  chunk_id: {chunk.get('chunk_id', 'N/A')}\")\n",
    "                print(f\"  doc_id: {chunk.get('doc_id', 'N/A')}\")\n",
    "                if 'karakter_szam' in chunk:\n",
    "                    print(f\"  karakter_szam: {chunk['karakter_szam']}\")\n",
    "                if 'text' in chunk:\n",
    "                    text_preview = str(chunk['text'])[:150] + \"...\" if len(str(chunk['text'])) > 150 else str(chunk['text'])\n",
    "                    print(f\"  text: {text_preview}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Nincsenek jelent≈ës min≈ës√©gi probl√©m√°k\")\n",
    "        \n",
    "    # √ñsszefoglal√≥ statisztik√°k\n",
    "    print(\"\\nüìä Chunk min≈ës√©g √∂sszefoglal√≥:\")\n",
    "    print(f\"  √ñsszes chunk: {len(df)}\")\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        print(f\"  √Åtlagos karakterhossz: {df['karakter_szam'].mean():.0f}\")\n",
    "        print(f\"  Medi√°n karakterhossz: {df['karakter_szam'].median():.0f}\")\n",
    "    if 'szo_szam' in df.columns:\n",
    "        print(f\"  √Åtlagos sz√≥sz√°m: {df['szo_szam'].mean():.0f}\")\n",
    "        print(f\"  Medi√°n sz√≥sz√°m: {df['szo_szam'].median():.0f}\")\n",
    "    \n",
    "    # Chunk eloszl√°s b√≠r√≥s√°g szerint\n",
    "    if 'birosag' in df.columns:\n",
    "        chunks_per_court = df.groupby('birosag').size().sort_values(ascending=False)\n",
    "        print(f\"\\nChunkok eloszl√°sa b√≠r√≥s√°g szerint (Top 5):\")\n",
    "        for court, count in chunks_per_court.head().items():\n",
    "            print(f\"  {court}: {count} chunk\")\n",
    "else:\n",
    "    print(\"‚ùå Nincs adat a chunk min≈ës√©g ellen≈ërz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 10. K√∂vetkeztet√©sek\n",
    "\n",
    "A chunk adatok ki√©rt√©kel√©s√©nek √∂sszefoglal√°sa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-conclusions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CHUNK ADATOK ELEMZ√âS √ñSSZEFOGLAL√ì ===\")\n",
    "print(\"\\n‚úÖ Sikeresen elemezve:\")\n",
    "if df is not None:\n",
    "    print(f\"   üìÑ Chunkok: {len(df)} db\")\n",
    "    print(f\"   üìä Oszlopok: {len(df.columns)} db\")\n",
    "    \n",
    "    # Alapvet≈ë statisztik√°k\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        print(f\"   üìè √Åtlagos karakterhossz: {df['karakter_szam'].mean():.0f}\")\n",
    "        print(f\"   üìè Medi√°n karakterhossz: {df['karakter_szam'].median():.0f}\")\n",
    "    \n",
    "    if 'szo_szam' in df.columns:\n",
    "        print(f\"   üî§ √Åtlagos sz√≥sz√°m: {df['szo_szam'].mean():.0f}\")\n",
    "        print(f\"   üî§ Medi√°n sz√≥sz√°m: {df['szo_szam'].median():.0f}\")\n",
    "    \n",
    "    # Hi√°nyz√≥ √©rt√©kek\n",
    "    missing_total = df.isnull().sum().sum()\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_ratio = missing_total / total_cells * 100\n",
    "    print(f\"   ‚ùå Hi√°nyz√≥ √©rt√©kek: {missing_total}/{total_cells} ({missing_ratio:.2f}%)\")\n",
    "    \n",
    "    # Unikalit√°s\n",
    "    if 'chunk_id' in df.columns:\n",
    "        unique_chunks = df['chunk_id'].nunique()\n",
    "        print(f\"   üÜî Egyedi chunk_id: {unique_chunks}/{len(df)} ({100 * unique_chunks / len(df):.2f}%)\")\n",
    "\n",
    "print(\"\\nüìã Agents.md specifik√°ci√≥ ellen≈ërz√©s:\")\n",
    "if df is not None:\n",
    "    # Alapvet≈ë metadatok megl√©te\n",
    "    required_cols = ['doc_id', 'chunk_id', 'text']\n",
    "    missing_required = [col for col in required_cols if col not in df.columns]\n",
    "    if not missing_required:\n",
    "        print(\"   ‚úÖ Alapvet≈ë metadatok jelen vannak\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Hi√°nyz√≥ alapvet≈ë metadatok: {missing_required}\")\n",
    "    \n",
    "    # Sz√∂veghossz ellen≈ërz√©s\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "        long_chunks = df[df['karakter_szam'] > max_length].shape[0]\n",
    "        if long_chunks == 0:\n",
    "            print(\"   ‚úÖ Nincsenek t√∫l hossz√∫ chunkok\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è T√∫l hossz√∫ chunkok: {long_chunks}/{len(df)} ({100*long_chunks/len(df):.1f}%)\")\n",
    "    \n",
    "    # Jogter√ºletek\n",
    "    if 'JogTerulet' in df.columns:\n",
    "        valid_domains = df['JogTerulet'].notna().sum()\n",
    "        print(f\"   ‚öñÔ∏è √ârv√©nyes jogter√ºletek: {valid_domains}/{len(df)} ({100 * valid_domains / len(df):.2f}%)\")\n",
    "\n",
    "print(\"\\nüí° Aj√°nl√°sok:\")\n",
    "if df is not None:\n",
    "    # Chunking probl√©m√°k\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "        short_chunks = df[df['karakter_szam'] < 50].shape[0]\n",
    "        long_chunks = df[df['karakter_szam'] > max_length].shape[0]\n",
    "        \n",
    "        if short_chunks > len(df) * 0.1:  # 10% feletti ar√°ny\n",
    "            print(f\"   üîß T√∫l sok r√∂vid chunk: {short_chunks} db - chunking param√©terek finomhangol√°sa sz√ºks√©ges\")\n",
    "        \n",
    "        if long_chunks > len(df) * 0.1:  # 10% feletti ar√°ny\n",
    "            print(f\"   ‚úÇÔ∏è T√∫l sok hossz√∫ chunk: {long_chunks} db - chunking param√©terek finomhangol√°sa sz√ºks√©ges\")\n",
    "    \n",
    "    # Hi√°nyz√≥ metadatok\n",
    "    missing_cols = df.columns[df.isnull().sum() > len(df) * 0.5]  # 50% feletti hi√°ny\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"   üìù Kritikus hi√°nyz√≥ metadatok: {list(missing_cols)} - jav√≠t√°s sz√ºks√©ges\")\n",
    "    \n",
    "    # Duplik√°lt chunk_id-k\n",
    "    if 'chunk_id' in df.columns:\n",
    "        duplicates = df['chunk_id'].duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"   üÜî Duplik√°lt chunk_id-k jav√≠t√°sa: {duplicates} db\")\n",
    "\n",
    "print(\"\\nüéØ Chunk adatok elemz√©se k√©sz - a retrieval rendszer haszn√°latra k√©sz!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
