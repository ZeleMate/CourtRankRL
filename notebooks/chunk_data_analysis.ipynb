{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chunk-data-analysis-header",
   "metadata": {},
   "source": [
    "# Chunk Adatok Kiértékelése - CourtRankRL Projekt\n",
    "\n",
    "Ez a notebook a chunks.jsonl fájlban található chunk adatokat elemzi. Az agents.md specifikáció alapján készített kiértékelési szempontokat vizsgálja a jelenlegi adatokkal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-config",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Projekt konfiguráció betöltése\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m project_root = Path(\u001b[34;43m__file__\u001b[39;49m).parent.parent.parent\n\u001b[32m     19\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfigs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "# Plot stílus beállítása\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Projekt konfiguráció betöltése\n",
    "import sys\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "from configs import config\n",
    "\n",
    "print(\"CourtRankRL - Chunk Data Analysis\")\n",
    "print(f\"Chunks: {config.CHUNKS_JSONL}\")\n",
    "print(f\"Processed docs: {config.PROCESSED_DOCS_LIST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-chunk-data",
   "metadata": {},
   "source": [
    "## 1. Chunk Adatok Betöltése\n",
    "\n",
    "A chunks.jsonl fájl betöltése mintavételezéssel a teljesítmény érdekében."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-chunks-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunkok betöltése mintavételezéssel\n",
    "chunks_file = config.CHUNKS_JSONL\n",
    "sample_size = min(50000, 50000)  # Maximum 50k chunk elemzésre\n",
    "\n",
    "df = None\n",
    "print(f\"Chunkok betöltése: {chunks_file}\")\n",
    "print(f\"Mintavétel: {sample_size} chunk\")\n",
    "\n",
    "if chunks_file.exists():\n",
    "    try:\n",
    "        chunks_list = []\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                try:\n",
    "                    chunk = json.loads(line.strip())\n",
    "                    chunks_list.append(chunk)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        if chunks_list:\n",
    "            df = pd.DataFrame(chunks_list)\n",
    "            print(f\"✅ Betöltött chunkok száma: {len(df)}\")\n",
    "            print(f\"Oszlopok: {df.columns.tolist()}\")\n",
    "        else:\n",
    "            print(\"⚠️ Nem találhatóak chunk adatok\")\n",
    "            df = None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Hiba a chunkok betöltése során: {e}\")\n",
    "        df = None\n",
    "else:\n",
    "    print(f\"❌ Chunks fájl nem található: {chunks_file}\")\n",
    "    print(\"Futtassa a build pipeline-t először: uv run courtrankrl build\")\n",
    "    df = None\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(f\"\\nAdatok betöltve: {df.shape[0]} chunk, {df.shape[1]} oszlop\")\n",
    "    print(f\"Első chunk szövege (részlet):\")\n",
    "    first_text = df['text'].iloc[0] if 'text' in df.columns else 'N/A'\n",
    "    print(f\"{str(first_text)[:200]}...\")\n",
    "else:\n",
    "    print(\"\\n❌ Nincs adat az elemzéshez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-info-analysis",
   "metadata": {},
   "source": [
    "## 2. Alapvető Információk és Statisztikák\n",
    "\n",
    "A chunk adatok alapvető statisztikáinak elemzése."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-basic-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(\"📊 Chunk adatok alapvető információi:\")\n",
    "    \n",
    "    # Adattípusok\n",
    "    print(f\"Adatok alakja: {df.shape}\")\n",
    "    print(f\"Oszlopok: {df.columns.tolist()}\")\n",
    "    df.info()\n",
    "    \n",
    "    # Mintavétel az első néhány chunkból\n",
    "    print(\"\\nElső 3 chunk adatai:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        chunk = df.iloc[i]\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"  doc_id: {chunk.get('doc_id', 'N/A')}\")\n",
    "        print(f\"  chunk_id: {chunk.get('chunk_id', 'N/A')}\")\n",
    "        print(f\"  birosag: {chunk.get('birosag', 'N/A')}\")\n",
    "        print(f\"  JogTerulet: {chunk.get('JogTerulet', 'N/A')}\")\n",
    "        if 'text' in chunk:\n",
    "            text_preview = str(chunk['text'])[:100] + \"...\" if len(str(chunk['text'])) > 100 else str(chunk['text'])\n",
    "            print(f\"  text: {text_preview}\")\n",
    "    \n",
    "    # Leíró statisztikák\n",
    "    print(\"\\n📈 Leíró statisztikák:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        display(df[numeric_cols].describe())\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        display(df[categorical_cols].describe())\n",
    "else:\n",
    "    print(\"❌ Nincs adat az alapvető információk elemzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-values-analysis",
   "metadata": {},
   "source": [
    "## 3. Hiányzó Értékek Elemzése\n",
    "\n",
    "A hiányzó értékek azonosítása chunk szinten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-missing-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(\"🔍 Chunk adatok hiányzó értékei:\")\n",
    "    \n",
    "    # Hiányzó értékek statisztikái\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({'Darabszám': missing_values, 'Százalék': missing_percent})\n",
    "    missing_df = missing_df[missing_df['Darabszám'] > 0].sort_values(by='Százalék', ascending=False)\n",
    "    \n",
    "    print(\"Hiányzó értékek oszloponként:\")\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Hiányzó értékek vizualizációja\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Hiányzó értékek eloszlása a chunk adatokban')\n",
    "    plt.show()\n",
    "    \n",
    "    # Kritikus hiányzó értékek\n",
    "    critical_missing = missing_df[missing_df['Százalék'] > 50]  # 50% feletti hiány\n",
    "    if not critical_missing.empty:\n",
    "        print(f\"⚠️ Kritikus hiányzó értékek (>50%): {len(critical_missing)} oszlop\")\n",
    "        display(critical_missing)\n",
    "    else:\n",
    "        print(\"✅ Nincsenek kritikus hiányzó értékek\")\n",
    "else:\n",
    "    print(\"❌ Nincs adat a hiányzó értékek elemzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-length-analysis",
   "metadata": {},
   "source": [
    "## 4. Szöveghossz Elemzése\n",
    "\n",
    "A chunkok szövegeinek hossza (karakterek és szavak száma szerint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-text-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns:\n",
    "    print(\"📏 Szöveghossz elemzése:\")\n",
    "    \n",
    "    # Karakterek és szavak száma\n",
    "    df['karakter_szam'] = df['text'].astype(str).apply(len)\n",
    "    df['szo_szam'] = df['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "    \n",
    "    print(\"Szöveghossz statisztikák:\")\n",
    "    display(df[['karakter_szam', 'szo_szam']].describe())\n",
    "    \n",
    "    # Embedding modell követelményeinek ellenőrzése\n",
    "    max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "    long_chunks = df[df['karakter_szam'] > max_length]\n",
    "    print(f\"\\nChunkok száma, amelyek hosszabbak {max_length} karakternél: {len(long_chunks)} ({100*len(long_chunks)/len(df):.1f}%)\")\n",
    "    \n",
    "    if len(long_chunks) > 0:\n",
    "        print(f\"Leghosszabb chunk: {df['karakter_szam'].max()} karakter\")\n",
    "        print(f\"Legrövidebb hosszú chunk: {long_chunks['karakter_szam'].min()} karakter\")\n",
    "        \n",
    "        # Leghosszabb chunk részlete\n",
    "        longest_chunk = df.loc[df['karakter_szam'].idxmax()]\n",
    "        print(f\"\\nLeghosszabb chunk részlete:\")\n",
    "        print(f\"doc_id: {longest_chunk.get('doc_id', 'N/A')}\")\n",
    "        print(f\"chunk_id: {longest_chunk.get('chunk_id', 'N/A')}\")\n",
    "        print(f\"text: {str(longest_chunk['text'])[:200]}...\")\n",
    "    \n",
    "    # Eloszlások vizualizációja\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Karakterek számának eloszlása\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(x=df['karakter_szam'], bins=50, kde=True)\n",
    "    plt.title('Chunk karakterhossz eloszlása')\n",
    "    plt.xlabel('Karakterek száma')\n",
    "    plt.ylabel('Chunkok száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Szavak számának eloszlása\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(x=df['szo_szam'], bins=50, kde=True)\n",
    "    plt.title('Chunk szószám eloszlása')\n",
    "    plt.xlabel('Szavak száma')\n",
    "    plt.ylabel('Chunkok száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplotok\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(y=df['karakter_szam'])\n",
    "    plt.title('Chunk karakterhossz boxplot')\n",
    "    plt.ylabel('Karakterek száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df['szo_szam'])\n",
    "    plt.title('Chunk szószám boxplot')\n",
    "    plt.ylabel('Szavak száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ Nincs 'text' oszlop a szöveghossz elemzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorical-analysis",
   "metadata": {},
   "source": [
    "## 5. Kategorikus Változók Elemzése\n",
    "\n",
    "A chunkok metadatainak kategorikus változói."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-categorical-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_categories(df, column_name, top_n=20):\n",
    "    \"\"\"Segédfüggvény a leggyakoribb kategóriák megjelenítésére.\"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"❌ '{column_name}' oszlop nem található\")\n",
    "        return\n",
    "    \n",
    "    counts = df[column_name].value_counts()\n",
    "    print(f\"\\n'{column_name}' egyedi értékeinek száma: {counts.nunique()}\")\n",
    "    print(f\"Leggyakoribb {top_n} érték:\")\n",
    "    display(counts.head(top_n))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    counts.head(top_n).plot(kind='bar')\n",
    "    plt.title(f'Chunkok megoszlása - {column_name} (Top {top_n})')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Chunkok száma')\n",
    "    plt.xticks(rotation=75, ha='right')\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"📊 Kategorikus változók elemzése:\")\n",
    "    \n",
    "    # Bíróság elemzése\n",
    "    if 'birosag' in df.columns:\n",
    "        plot_top_categories(df, 'birosag', top_n=30)\n",
    "    \n",
    "    # Jogterület elemzése\n",
    "    if 'JogTerulet' in df.columns:\n",
    "        plot_top_categories(df, 'JogTerulet', top_n=20)\n",
    "    \n",
    "    # Egyéb fontos kategorikus változók\n",
    "    for col in ['MeghozoBirosag', 'Kollegium', 'Azonosito']:\n",
    "        if col in df.columns:\n",
    "            plot_top_categories(df, col, top_n=15)\n",
    "else:\n",
    "    print(\"❌ Nincs adat a kategorikus változók elemzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word-analysis",
   "metadata": {},
   "source": [
    "## 6. Gyakori Szavak Elemzése\n",
    "\n",
    "A chunkokban leggyakrabban előforduló szavak elemzése."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-word-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns:\n",
    "    print(\"🔤 Gyakori szavak elemzése:\")\n",
    "    \n",
    "    # Mivel nagy lehet az adathalmaz, vegyünk egy mintát\n",
    "    sample_size = min(len(df), 10000)\n",
    "    sample_df = df.sample(n=sample_size, random_state=42)\n",
    "    print(f\"Szóelemzés mintanagysága: {sample_size} chunk\")\n",
    "    \n",
    "    # Egyszerű tokenizálás\n",
    "    def simple_tokenize(text):\n",
    "        return [word.lower() for word in text.split() if word.strip()]\n",
    "    \n",
    "    # Szavak gyakoriságának számítása\n",
    "    word_counts = Counter()\n",
    "    total_words = 0\n",
    "    \n",
    "    for text in sample_df['text'].astype(str):\n",
    "        tokens = simple_tokenize(text)\n",
    "        word_counts.update(tokens)\n",
    "        total_words += len(tokens)\n",
    "    \n",
    "    # Top 30 szó kiválasztása\n",
    "    top_words = word_counts.most_common(30)\n",
    "    word_freq = pd.DataFrame(top_words, columns=['szo', 'gyakorisag'])\n",
    "    \n",
    "    print(f\"Top 30 leggyakoribb szó (összes szó: {total_words:,}):\")\n",
    "    display(word_freq)\n",
    "    \n",
    "    # Vizualizáció\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='gyakorisag', y='szo', data=word_freq)\n",
    "    plt.title('Top 30 leggyakoribb szó a chunkokban')\n",
    "    plt.xlabel('Gyakoriság')\n",
    "    plt.ylabel('Szó')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Töltelékszavak aránya\n",
    "    stop_words = ['és', 'a', 'az', 'de', 'hogy', 'is', 'nem', 'van', 'lesz', 'volt', 'mint']\n",
    "    stop_word_count = sum(word_counts.get(word, 0) for word in stop_words)\n",
    "    stop_word_ratio = stop_word_count / total_words if total_words > 0 else 0\n",
    "    print(f\"\\nTöltelékszavak aránya: {stop_word_ratio:.2%}\")\n",
    "    print(f\"Töltelékszavak összesen: {stop_word_count:,}\")\n",
    "    \n",
    "    # Szóhossz eloszlás\n",
    "    word_lengths = [len(word) for word in word_counts.keys()]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(word_lengths, bins=range(1, max(word_lengths) + 2), alpha=0.7)\n",
    "    plt.title('Szóhossz eloszlása a chunkokban')\n",
    "    plt.xlabel('Szó hossza (karakter)')\n",
    "    plt.ylabel('Szavak száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ Nincs 'text' oszlop a gyakori szavak elemzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunk-structure-analysis",
   "metadata": {},
   "source": [
    "## 7. Chunk Szerkezet Elemzése\n",
    "\n",
    "A chunkok belső szerkezetének és tartalmának elemzése."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-chunk-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns:\n",
    "    print(\"🏗️ Chunk szerkezet elemzése:\")\n",
    "    \n",
    "    # Első chunk teljes tartalmának megjelenítése\n",
    "    first_chunk_text = df['text'].iloc[0] if len(df) > 0 else ''\n",
    "    print(\"\\nElső chunk teljes szövege:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(first_chunk_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Szöveg struktúra elemzés\n",
    "    sample_df = df.sample(min(1000, len(df)), random_state=42)\n",
    "    \n",
    "    # Mondatok és bekezdések elemzése\n",
    "    sample_df['mondat_szam'] = sample_df['text'].astype(str).apply(\n",
    "        lambda x: len([s for s in x.split('.') if s.strip()]) if '.' in x else 1\n",
    "    )\n",
    "    sample_df['bekezdes_szam'] = sample_df['text'].astype(str).apply(\n",
    "        lambda x: len([p for p in x.split('\\n') if p.strip()]) if '\\n' in x else 1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSzöveg struktúra statisztikák (minta: {len(sample_df)} chunk):\")\n",
    "    display(sample_df[['mondat_szam', 'bekezdes_szam']].describe())\n",
    "    \n",
    "    # Struktúra vizualizáció\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(x=sample_df['mondat_szam'], bins=30, kde=True)\n",
    "    plt.title('Mondatok száma chunkonként')\n",
    "    plt.xlabel('Mondatok száma')\n",
    "    plt.ylabel('Chunkok száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(x=sample_df['bekezdes_szam'], bins=30, kde=True)\n",
    "    plt.title('Bekezdések száma chunkonként')\n",
    "    plt.xlabel('Bekezdések száma')\n",
    "    plt.ylabel('Chunkok száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Szöveg komplexitás\n",
    "    sample_df['atlag_mondat_hossz'] = sample_df.apply(\n",
    "        lambda row: row['karakter_szam'] / max(row['mondat_szam'], 1), axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSzöveg komplexitás:\")\n",
    "    display(sample_df['atlag_mondat_hossz'].describe())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sample_df['atlag_mondat_hossz'], bins=50, alpha=0.7)\n",
    "    plt.title('Átlagos mondathossz eloszlása')\n",
    "    plt.xlabel('Karakterek mondatonként')\n",
    "    plt.ylabel('Chunkok száma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ Nincs 'text' oszlop a chunk szerkezet elemzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-correlation",
   "metadata": {},
   "source": [
    "## 8. Metadatok és Szöveg Kapcsolata\n",
    "\n",
    "A chunk metadatok és a szöveges tartalom közötti kapcsolatok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-metadata-text-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'text' in df.columns and 'karakter_szam' in df.columns:\n",
    "    print(\"🔗 Metadatok és szöveg kapcsolata:\")\n",
    "    \n",
    "    # Szöveghossz és kategorikus változók kapcsolata\n",
    "    categorical_cols = ['birosag', 'JogTerulet', 'MeghozoBirosag']\n",
    "    available_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    \n",
    "    for col in available_cols[:3]:  # Maximum 3 kategorikus változó\n",
    "        if df[col].nunique() <= 20:  # Csak ha nem túl sok egyedi érték\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            top_categories = df[col].value_counts().nlargest(10).index\n",
    "            df_filtered = df[df[col].isin(top_categories)]\n",
    "            \n",
    "            sns.boxplot(data=df_filtered, x=col, y='karakter_szam')\n",
    "            plt.title(f'Szöveghossz eloszlása - {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Karakterek száma')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    \n",
    "    # Korreláció numerikus változók között\n",
    "    numeric_cols = ['karakter_szam', 'szo_szam', 'HatarozatEve']\n",
    "    available_numeric = [col for col in numeric_cols if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    if len(available_numeric) > 1:\n",
    "        correlation_matrix = df[available_numeric].corr()\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Numerikus változók korrelációja')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nKorrelációs mátrix:\")\n",
    "        display(correlation_matrix)\n",
    "else:\n",
    "    print(\"❌ Nincs elég adat a metadatok és szöveg kapcsolat elemzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-check",
   "metadata": {},
   "source": [
    "## 9. Chunk Minőség Ellenőrzése\n",
    "\n",
    "A chunking minőségének és konzisztenciájának ellenőrzése."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-chunk-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(\"✅ Chunk minőség ellenőrzése:\")\n",
    "    \n",
    "    # Alapvető ellenőrzések\n",
    "    quality_issues = {}\n",
    "    \n",
    "    # 1. Üres vagy túl rövid chunkok\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        empty_chunks = df[df['karakter_szam'] < 50].shape[0]\n",
    "        quality_issues['Túl rövid chunkok (<50 karakter)'] = empty_chunks\n",
    "    \n",
    "    # 2. Túl hosszú chunkok\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "        long_chunks = df[df['karakter_szam'] > max_length].shape[0]\n",
    "        quality_issues[f'Túl hosszú chunkok (>{max_length} karakter)'] = long_chunks\n",
    "    \n",
    "    # 3. Hiányzó metadatok\n",
    "    required_cols = ['doc_id', 'chunk_id']\n",
    "    for col in required_cols:\n",
    "        if col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                quality_issues[f'Hiányzó {col}'] = missing\n",
    "    \n",
    "    # 4. Duplikált chunk_id-k\n",
    "    if 'chunk_id' in df.columns:\n",
    "        duplicates = df['chunk_id'].duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            quality_issues['Duplikált chunk_id-k'] = duplicates\n",
    "    \n",
    "    # 5. Hiányzó szöveg\n",
    "    if 'text' in df.columns:\n",
    "        empty_text = df[df['text'].astype(str).str.len() < 10].shape[0]\n",
    "        if empty_text > 0:\n",
    "            quality_issues['Túl rövid/üres szöveg'] = empty_text\n",
    "    \n",
    "    # Eredmények megjelenítése\n",
    "    if quality_issues:\n",
    "        print(\"\\n⚠️ Chunk minőségi problémák:\")\n",
    "        for issue, count in quality_issues.items():\n",
    "            print(f\"  {issue}: {count} db ({100*count/len(df):.1f}%)\")\n",
    "        \n",
    "        # Problémás chunkok részletei\n",
    "        print(\"\\nProblémás chunkok részletei:\")\n",
    "        problematic_mask = pd.Series(False, index=df.index)\n",
    "        \n",
    "        if 'karakter_szam' in df.columns:\n",
    "            problematic_mask |= (df['karakter_szam'] < 50)\n",
    "            problematic_mask |= (df['karakter_szam'] > max_length)\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            problematic_mask |= (df['text'].astype(str).str.len() < 10)\n",
    "        \n",
    "        if problematic_mask.any():\n",
    "            problematic_df = df[problematic_mask].head(5)\n",
    "            for i, (_, chunk) in enumerate(problematic_df.iterrows()):\n",
    "                print(f\"\\nProblémás chunk {i+1}:\")\n",
    "                print(f\"  chunk_id: {chunk.get('chunk_id', 'N/A')}\")\n",
    "                print(f\"  doc_id: {chunk.get('doc_id', 'N/A')}\")\n",
    "                if 'karakter_szam' in chunk:\n",
    "                    print(f\"  karakter_szam: {chunk['karakter_szam']}\")\n",
    "                if 'text' in chunk:\n",
    "                    text_preview = str(chunk['text'])[:150] + \"...\" if len(str(chunk['text'])) > 150 else str(chunk['text'])\n",
    "                    print(f\"  text: {text_preview}\")\n",
    "    else:\n",
    "        print(\"\\n✅ Nincsenek jelentős minőségi problémák\")\n",
    "        \n",
    "    # Összefoglaló statisztikák\n",
    "    print(\"\\n📊 Chunk minőség összefoglaló:\")\n",
    "    print(f\"  Összes chunk: {len(df)}\")\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        print(f\"  Átlagos karakterhossz: {df['karakter_szam'].mean():.0f}\")\n",
    "        print(f\"  Medián karakterhossz: {df['karakter_szam'].median():.0f}\")\n",
    "    if 'szo_szam' in df.columns:\n",
    "        print(f\"  Átlagos szószám: {df['szo_szam'].mean():.0f}\")\n",
    "        print(f\"  Medián szószám: {df['szo_szam'].median():.0f}\")\n",
    "    \n",
    "    # Chunk eloszlás bíróság szerint\n",
    "    if 'birosag' in df.columns:\n",
    "        chunks_per_court = df.groupby('birosag').size().sort_values(ascending=False)\n",
    "        print(f\"\\nChunkok eloszlása bíróság szerint (Top 5):\")\n",
    "        for court, count in chunks_per_court.head().items():\n",
    "            print(f\"  {court}: {count} chunk\")\n",
    "else:\n",
    "    print(\"❌ Nincs adat a chunk minőség ellenőrzéséhez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 10. Következtetések\n",
    "\n",
    "A chunk adatok kiértékelésének összefoglalása."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-conclusions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CHUNK ADATOK ELEMZÉS ÖSSZEFOGLALÓ ===\")\n",
    "print(\"\\n✅ Sikeresen elemezve:\")\n",
    "if df is not None:\n",
    "    print(f\"   📄 Chunkok: {len(df)} db\")\n",
    "    print(f\"   📊 Oszlopok: {len(df.columns)} db\")\n",
    "    \n",
    "    # Alapvető statisztikák\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        print(f\"   📏 Átlagos karakterhossz: {df['karakter_szam'].mean():.0f}\")\n",
    "        print(f\"   📏 Medián karakterhossz: {df['karakter_szam'].median():.0f}\")\n",
    "    \n",
    "    if 'szo_szam' in df.columns:\n",
    "        print(f\"   🔤 Átlagos szószám: {df['szo_szam'].mean():.0f}\")\n",
    "        print(f\"   🔤 Medián szószám: {df['szo_szam'].median():.0f}\")\n",
    "    \n",
    "    # Hiányzó értékek\n",
    "    missing_total = df.isnull().sum().sum()\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_ratio = missing_total / total_cells * 100\n",
    "    print(f\"   ❌ Hiányzó értékek: {missing_total}/{total_cells} ({missing_ratio:.2f}%)\")\n",
    "    \n",
    "    # Unikalitás\n",
    "    if 'chunk_id' in df.columns:\n",
    "        unique_chunks = df['chunk_id'].nunique()\n",
    "        print(f\"   🆔 Egyedi chunk_id: {unique_chunks}/{len(df)} ({100 * unique_chunks / len(df):.2f}%)\")\n",
    "\n",
    "print(\"\\n📋 Agents.md specifikáció ellenőrzés:\")\n",
    "if df is not None:\n",
    "    # Alapvető metadatok megléte\n",
    "    required_cols = ['doc_id', 'chunk_id', 'text']\n",
    "    missing_required = [col for col in required_cols if col not in df.columns]\n",
    "    if not missing_required:\n",
    "        print(\"   ✅ Alapvető metadatok jelen vannak\")\n",
    "    else:\n",
    "        print(f\"   ❌ Hiányzó alapvető metadatok: {missing_required}\")\n",
    "    \n",
    "    # Szöveghossz ellenőrzés\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "        long_chunks = df[df['karakter_szam'] > max_length].shape[0]\n",
    "        if long_chunks == 0:\n",
    "            print(\"   ✅ Nincsenek túl hosszú chunkok\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Túl hosszú chunkok: {long_chunks}/{len(df)} ({100*long_chunks/len(df):.1f}%)\")\n",
    "    \n",
    "    # Jogterületek\n",
    "    if 'JogTerulet' in df.columns:\n",
    "        valid_domains = df['JogTerulet'].notna().sum()\n",
    "        print(f\"   ⚖️ Érvényes jogterületek: {valid_domains}/{len(df)} ({100 * valid_domains / len(df):.2f}%)\")\n",
    "\n",
    "print(\"\\n💡 Ajánlások:\")\n",
    "if df is not None:\n",
    "    # Chunking problémák\n",
    "    if 'karakter_szam' in df.columns:\n",
    "        max_length = getattr(config, 'EMBEDDING_MAX_LENGTH', 512)\n",
    "        short_chunks = df[df['karakter_szam'] < 50].shape[0]\n",
    "        long_chunks = df[df['karakter_szam'] > max_length].shape[0]\n",
    "        \n",
    "        if short_chunks > len(df) * 0.1:  # 10% feletti arány\n",
    "            print(f\"   🔧 Túl sok rövid chunk: {short_chunks} db - chunking paraméterek finomhangolása szükséges\")\n",
    "        \n",
    "        if long_chunks > len(df) * 0.1:  # 10% feletti arány\n",
    "            print(f\"   ✂️ Túl sok hosszú chunk: {long_chunks} db - chunking paraméterek finomhangolása szükséges\")\n",
    "    \n",
    "    # Hiányzó metadatok\n",
    "    missing_cols = df.columns[df.isnull().sum() > len(df) * 0.5]  # 50% feletti hiány\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"   📝 Kritikus hiányzó metadatok: {list(missing_cols)} - javítás szükséges\")\n",
    "    \n",
    "    # Duplikált chunk_id-k\n",
    "    if 'chunk_id' in df.columns:\n",
    "        duplicates = df['chunk_id'].duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"   🆔 Duplikált chunk_id-k javítása: {duplicates} db\")\n",
    "\n",
    "print(\"\\n🎯 Chunk adatok elemzése kész - a retrieval rendszer használatra kész!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
