# CourtRankRL RunPod Setup Guide

## ğŸš€ **RunPod 5090 GPU-n valÃ³ futtatÃ¡s**

Ez az ÃºtmutatÃ³ segÃ­ti a CourtRankRL teljes pipeline futtatÃ¡sÃ¡t RunPod Jupyter notebook-ban.

---

## ğŸ“‹ **1. ElÅ‘feltÃ©telek**

### **RunPod Account**
1. RegisztrÃ¡lj a [runpod.io](https://runpod.io)-n
2. VÃ¡laszd a **"Secure Cloud"** opciÃ³t
3. Keresd meg a **RTX 5090** GPU template-et

### **GPU Template BeÃ¡llÃ­tÃ¡sok**
- **GPU**: RTX 5090 (24GB VRAM)
- **CPU**: AMD EPYC (16+ cores)
- **RAM**: 64GB+ system memory
- **Storage**: 50GB+ SSD storage
- **Container**: PyTorch 2.1+ (Python 3.11)
- **Internet**: High-speed internet (model letÃ¶ltÃ©shez)

---

## ğŸ“ **2. FÃ¡jlok FeltÃ¶ltÃ©se**

### **Projekt StruktÃºra**
```
/workspace/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”œâ”€â”€ hybrid_search.py
â”‚   â”‚   â””â”€â”€ grpo_reranker.py
â”‚   â””â”€â”€ data_loader/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ index/
â”‚       â”œâ”€â”€ faiss_index.bin      (211MB)
â”‚       â”œâ”€â”€ chunk_id_map.json    (111MB)
â”‚       â””â”€â”€ bm25_index.json      (9.6GB)
â””â”€â”€ notebooks/
    â”œâ”€â”€ RunPod_Full_Pipeline.py
    â””â”€â”€ README_RunPod_5090.md
```

### **FeltÃ¶ltÃ©si MÃ³dszerek**

#### **MÃ³dszer 1: RunPod Dashboard**
1. LÃ©pj be a RunPod dashboard-ba
2. VÃ¡laszd ki a pod-odat
3. Kattints a **"Files"** tab-ra
4. Upload-old a teljes projektet ZIP-ben

#### **MÃ³dszer 2: RunPod CLI**
```bash
# LokÃ¡lis gÃ©pen
cd /path/to/CourtRankRL
zip -r courtrankrl.zip .

# RunPod-on (ha mÃ¡r be vagy jelentkezve)
runpodctl upload courtrankrl.zip /workspace/
unzip courtrankrl.zip
```

#### **MÃ³dszer 3: Jupyter-ben**
```python
# Jupyter notebook-ban
import zipfile
import urllib.request

# ZIP feltÃ¶ltÃ©s URL-rÅ‘l
zip_url = "https://your-storage/courtrankrl.zip"
urllib.request.urlretrieve(zip_url, "courtrankrl.zip")

with zipfile.ZipFile("courtrankrl.zip", 'r') as zip_ref:
    zip_ref.extractall("/workspace")
```

---

## ğŸ–¥ï¸ **3. Jupyter Notebook BeÃ¡llÃ­tÃ¡s**

### **ElsÅ‘ CellÃ¡k FuttatÃ¡sa**

```python
# 1. Dependencies telepÃ­tÃ©se
!pip install --upgrade pip
!pip install faiss-cpu tqdm transformers accelerate torch sentencepiece

# 2. System check
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# 3. Projekt betÃ¶ltÃ©se
import sys
sys.path.append('/workspace')

from configs import config
from src.search.hybrid_search import HybridRetriever
from src.search.grpo_reranker import GRPOReranker

print("âœ… Project loaded successfully!")
```

### **Teljes Pipeline FuttatÃ¡sa**

```python
# Teljes pipeline script futtatÃ¡sa
exec(open('/workspace/notebooks/RunPod_Full_Pipeline.py').read())
```

---

## ğŸ¯ **4. Query FuttatÃ¡s**

### **EgyszerÅ± Query**
```python
from src.search.hybrid_search import HybridRetriever

retriever = HybridRetriever()
results = retriever.retrieve("csalÃ¡di jog", top_k=10, fusion_method="rrf")

for i, doc_id in enumerate(results, 1):
    print(f"{i}. {doc_id}")
```

### **CLI Parancs**
```bash
cd /workspace
python3 -m src.cli query "csalÃ¡di jog" --top-k 10
```

### **Batch Query**
```python
queries = [
    "csalÃ¡di jog",
    "ingatlan Ã¼gy",
    "szerzÅ‘dÃ©ses jogvita",
    "munkajogi vita",
    "kÃ¡rtÃ©rÃ­tÃ©si igÃ©ny"
]

for query in queries:
    results = retriever.retrieve(query, top_k=5)
    print(f"\n{query}: {len(results)} results")
```

---

## ğŸƒ **5. Training FuttatÃ¡s**

### **GRPO Training**
```python
from src.search.grpo_reranker import GRPOReranker

reranker = GRPOReranker()

# Training loop
for epoch in range(10):
    print(f"Epoch {epoch+1}/10")
    # Training logic here
    pass

# Policy mentÃ©se
torch.save(reranker.policy.state_dict(), config.RL_POLICY_PATH)
print("âœ… Training completed!")
```

### **CLI Training**
```bash
cd /workspace
python3 -m src.cli train
```

---

## ğŸ“Š **6. Monitoring & Debug**

### **GPU Memory Monitor**
```python
import torch

def print_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3

        print(f"GPU Memory: {allocated:.1f}GB / {total:.1f}GB")
        print(f"Memory Usage: {allocated/total*100:.1f}%")
        print(f"Reserved: {reserved:.1f}GB")
```

### **System Monitor**
```python
import psutil

def system_status():
    memory = psutil.virtual_memory()
    storage = psutil.disk_usage('/')

    print("ğŸ’¾ System Status:")
    print(f"   RAM: {memory.percent:.1f}% ({memory.available/1024**3:.1f}GB free)")
    print(f"   Storage: {storage.percent:.1f}% ({storage.free/1024**3:.1f}GB free)")
```

---

## ğŸš¨ **7. HibaelhÃ¡rÃ­tÃ¡s**

### **MemÃ³ria Hiba**
```python
# CsÃ¶kkentett batch size
config.EMBEDDING_BATCH_SIZE = 32  # Vagy 16, 8

# CsÃ¶kkentett max length
config.EMBEDDING_MAX_LENGTH = 512
```

### **CUDA Hiba**
```python
# CUDA verziÃ³ ellenÅ‘rzÃ©se
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA version: {torch.version.cuda}")

# GPU elÃ©rhetÅ‘sÃ©g
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
```

### **Model BetÃ¶ltÃ©s Hiba**
```python
# Fallback model
try:
    model = AutoModel.from_pretrained("Qwen/Qwen3-Embedding-0.6B")
except:
    print("ğŸ”„ Fallback to smaller model")
    model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
```

### **Index BetÃ¶ltÃ©s Hiba**
```python
# Index mÃ©ret ellenÅ‘rzÃ©se
import os
faiss_size = os.path.getsize("/workspace/data/index/faiss_index.bin") / 1024**3
bm25_size = os.path.getsize("/workspace/data/index/bm25_index.json") / 1024**3

print(f"FAISS index: {faiss_size:.1f} GB")
print(f"BM25 index: {bm25_size:.1f} GB")

if bm25_size > 8:
    print("âš ï¸ BM25 tÃºl nagy, csak FAISS hasznÃ¡lata")
```

---

## ğŸ“ˆ **8. Performance OptimalizÃ¡ciÃ³**

### **GPU Memory OptimalizÃ¡ciÃ³**
```python
# FP16 hasznÃ¡lat mindenhol
torch.set_default_dtype(torch.float16)

# Gradient checkpointing
model.gradient_checkpointing_enable()

# Mixed precision training
from torch.cuda.amp import GradScaler
scaler = GradScaler()
```

### **Batch Size OptimalizÃ¡ciÃ³**
```python
# Dinamikus batch size
def get_optimal_batch_size():
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory >= 24:  # 5090
            return 128
        elif gpu_memory >= 12:  # 4090
            return 64
        else:
            return 32
    return 16

config.EMBEDDING_BATCH_SIZE = get_optimal_batch_size()
```

---

## ğŸ‰ **9. TermelÃ©si HasznÃ¡lat**

### **Production Query**
```python
# Production ready query
def production_query(query: str, rerank: bool = True):
    retriever = HybridRetriever()

    if rerank:
        reranker = GRPOReranker()
        reranker.load_policy(config.RL_POLICY_PATH)

        # Baseline retrieval
        baseline = retriever.retrieve(query, top_k=100, fusion_method="rrf")

        # Reranking
        reranked = reranker.rerank(baseline)[:10]

        return reranked
    else:
        return retriever.retrieve(query, top_k=10, fusion_method="rrf")
```

### **Monitoring**
```python
# Log minden query
import logging
logging.basicConfig(filename='courtrankrl.log', level=logging.INFO)

def log_query(query, results):
    logging.info(f"Query: {query}, Results: {len(results)}")
    for doc_id in results:
        logging.info(f"  - {doc_id}")
```

---

## ğŸš€ **10. RunPod Specific Tips**

### **Cost Optimization**
- **Spot instances**: 50-70% olcsÃ³bb, de preemptible
- **Storage**: Csak a szÃ¼ksÃ©ges fÃ¡jlokat tartsd
- **Idle timeout**: 30 perc inaktivitÃ¡s utÃ¡n leÃ¡ll

### **File Management**
```bash
# FÃ¡jlok tisztÃ­tÃ¡sa
rm -rf /workspace/temp_files/
rm -rf /workspace/__pycache__/

# Csak a szÃ¼ksÃ©ges fÃ¡jlok megtartÃ¡sa
ls -lh /workspace/data/index/
```

### **Network Optimization**
```python
# Model cache
export HF_HOME=/workspace/.cache/huggingface
export TRANSFORMERS_CACHE=/workspace/.cache/transformers

# Offline mode (ha mÃ¡r letÃ¶ltÃ¶tted)
export TRANSFORMERS_OFFLINE=1
```

---

## ğŸ¯ **11. Next Steps**

1. **âœ… Setup Complete**: FÃ¡jlok feltÃ¶ltve, GPU elÃ©rhetÅ‘
2. **ğŸ”„ Run Pipeline**: Teljes pipeline tesztelÃ©se
3. **ğŸ“Š Query Testing**: KÃ¼lÃ¶nbÃ¶zÅ‘ jogi terÃ¼letek tesztelÃ©se
4. **ğŸƒ Training**: GRPO policy tanÃ­tÃ¡sa
5. **ğŸš€ Production**: TermelÃ©si deployment

**GratulÃ¡lok! A CourtRankRL kÃ©szen Ã¡ll a RunPod 5090 GPU-n valÃ³ hasznÃ¡latra!** ğŸ‰

---

## ğŸ“ **Support**

Ha problÃ©mÃ¡k merÃ¼lnek fel:
1. EllenÅ‘rizd a GPU elÃ©rhetÅ‘sÃ©get: `torch.cuda.is_available()`
2. NÃ©zd meg a memory hasznÃ¡latot: `nvidia-smi`
3. EllenÅ‘rizd a fÃ¡jlok elÃ©rhetÅ‘sÃ©gÃ©t: `ls -lh /workspace/data/index/`
4. PrÃ³bÃ¡ld a fallback modelt: `sentence-transformers/all-MiniLM-L6-v2`

**Sikeres RunPod deployment-et!** ğŸš€
