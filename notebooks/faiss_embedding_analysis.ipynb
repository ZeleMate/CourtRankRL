{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faiss-embedding-analysis-header",
   "metadata": {},
   "source": [
    "# FAISS Embeddingek Ki√©rt√©kel√©se - CourtRankRL Projekt\n",
    "\n",
    "Ez a notebook a google/embeddinggemma-300m modellel gener√°lt FAISS indexben t√°rolt embeddingeket elemzi. Az agents.md specifik√°ci√≥ alapj√°n k√©sz√≠tett ki√©rt√©kel√©si szempontokat vizsg√°lja a jelenlegi adatokkal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Plot st√≠lus be√°ll√≠t√°sa\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Projekt konfigur√°ci√≥ bet√∂lt√©se\n",
    "import sys\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "from configs import config\n",
    "\n",
    "print(\"CourtRankRL - FAISS Embedding Analysis\")\n",
    "print(f\"Embedding model: {config.QWEN3_MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {config.EMBEDDING_DIMENSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-faiss-data",
   "metadata": {},
   "source": [
    "## 1. FAISS Index √©s Embeddingek Bet√∂lt√©se\n",
    "\n",
    "A jelenlegi projektben tal√°lhat√≥ FAISS index √©s chunk ID mapping bet√∂lt√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-faiss-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS index bet√∂lt√©se\n",
    "faiss_path = config.FAISS_INDEX_PATH\n",
    "chunk_map_path = config.CHUNK_ID_MAP_PATH\n",
    "\n",
    "print(f\"FAISS index bet√∂lt√©se: {faiss_path}\")\n",
    "print(f\"Chunk ID mapping bet√∂lt√©se: {chunk_map_path}\")\n",
    "\n",
    "index = None\n",
    "chunk_id_map = None\n",
    "embeddings = None\n",
    "\n",
    "try:\n",
    "    if faiss_path.exists():\n",
    "        index = faiss.read_index(str(faiss_path))\n",
    "        print(f\"‚úÖ FAISS index bet√∂ltve: {index.ntotal} vektor, {index.d} dimenzi√≥\")\n",
    "        \n",
    "        # Embedding dimenzi√≥ ellen≈ërz√©se\n",
    "        if hasattr(index, 'd') and index.d != config.EMBEDDING_DIMENSION:\n",
    "            print(f\"‚ö†Ô∏è  Embedding dimenzi√≥ elt√©r√©s: index={index.d}, config={config.EMBEDDING_DIMENSION}\")\n",
    "        \n",
    "        # Embeddingek kivon√°sa (minden vektor)\n",
    "        embeddings = []\n",
    "        for i in range(index.ntotal):\n",
    "            embedding = index.reconstruct(i)\n",
    "            embeddings.append(embedding)\n",
    "        embeddings = np.array(embeddings)\n",
    "        print(f\"‚úÖ Embeddingek kivonva: {embeddings.shape}\")\n",
    "    else:\n",
    "        print(f\"‚ùå FAISS index nem tal√°lhat√≥: {faiss_path}\")\n",
    "        print(\"Futtassa a gemma_embedding_runpod.ipynb-t el≈ësz√∂r!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Hiba a FAISS index bet√∂lt√©se sor√°n: {e}\")\n",
    "    index = None\n",
    "\n",
    "# Chunk ID mapping bet√∂lt√©se\n",
    "try:\n",
    "    if chunk_map_path.exists():\n",
    "        with open(chunk_map_path, 'r', encoding='utf-8') as f:\n",
    "            chunk_id_map = json.load(f)\n",
    "        print(f\"‚úÖ Chunk ID map bet√∂ltve: {len(chunk_id_map)} mapping\")\n",
    "    else:\n",
    "        print(f\"‚ùå Chunk ID map nem tal√°lhat√≥: {chunk_map_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Hiba a chunk ID map bet√∂lt√©se sor√°n: {e}\")\n",
    "    chunk_id_map = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-chunks-data",
   "metadata": {},
   "source": [
    "## 2. Chunk Adatok Bet√∂lt√©se\n",
    "\n",
    "A chunks.jsonl f√°jl bet√∂lt√©se, hogy √∂sszekapcsoljuk az embeddingeket a sz√∂vegekkel √©s metadatokkal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-chunks-jsonl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunkok bet√∂lt√©se mintav√©telez√©ssel a teljes√≠tm√©ny √©rdek√©ben\n",
    "chunks_file = config.CHUNKS_JSONL\n",
    "sample_size = min(10000, index.ntotal if index else 0)  # Maximum 10k chunk elemz√©sre\n",
    "\n",
    "df = None\n",
    "\n",
    "if chunks_file.exists() and index is not None and chunk_id_map is not None:\n",
    "    try:\n",
    "        print(f\"üìä Mintav√©tel: {sample_size} chunk bet√∂lt√©se...\")\n",
    "        \n",
    "        chunks_list = []\n",
    "        chunk_ids = list(chunk_id_map.values())[:sample_size]\n",
    "        \n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    chunk = json.loads(line.strip())\n",
    "                    if chunk.get('chunk_id') in chunk_ids:\n",
    "                        chunks_list.append(chunk)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                if len(chunks_list) >= sample_size:\n",
    "                    break\n",
    "        \n",
    "        if chunks_list:\n",
    "            df = pd.DataFrame(chunks_list)\n",
    "            print(f\"‚úÖ Bet√∂lt√∂tt chunkok sz√°ma: {len(df)}\")\n",
    "            \n",
    "            # Embeddingek hozz√°rendel√©se a chunkokhoz\n",
    "            if embeddings is not None:\n",
    "                try:\n",
    "                    embedding_dict = {chunk_id_map[str(i)]: embeddings[i] for i in range(len(embeddings))}\n",
    "                    df = df.assign(embedding=df['chunk_id'].map(embedding_dict))\n",
    "                    valid_embeddings = df['embedding'].notna().sum()\n",
    "                    print(f\"‚úÖ Embeddingek hozz√°rendelve: {valid_embeddings} chunk\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Hiba az embeddingek hozz√°rendel√©se sor√°n: {e}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Nincsenek embeddingek a hozz√°rendel√©shez\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Nem tal√°lhat√≥ak megfelel≈ë chunkok\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hiba a chunkok bet√∂lt√©se sor√°n: {e}\")\n",
    "        df = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Hi√°nyz√≥ adatok a chunk bet√∂lt√©shez:\")\n",
    "    if not chunks_file.exists():\n",
    "        print(f\"  - Chunks f√°jl nem tal√°lhat√≥: {chunks_file}\")\n",
    "    if index is None:\n",
    "        print(\"  - FAISS index\")\n",
    "    if chunk_id_map is None:\n",
    "        print(\"  - Chunk ID map\")\n",
    "    df = None\n",
    "\n",
    "# Ellen≈ërz√©s\n",
    "if df is not None and not df.empty:\n",
    "    print(f\"\\nAdatok bet√∂ltve: {df.shape[0]} chunk, {df.shape[1]} oszlop\")\n",
    "    print(f\"Oszlopok: {df.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Nincs adat az elemz√©shez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-quality-check",
   "metadata": {},
   "source": [
    "## 3. Embedding Min≈ës√©g Ellen≈ërz√©se\n",
    "\n",
    "Az agents.md specifik√°ci√≥ szerint L2-normaliz√°lt embeddingek sz√ºks√©gesek a FAISS IP metrik√°hoz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-normalization",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'embedding' in df.columns and embeddings is not None:\n",
    "    print(\"üîç Embeddingek min≈ës√©gi ellen≈ërz√©se:\")\n",
    "    \n",
    "    # Embedding dimenzi√≥ ellen≈ërz√©se\n",
    "    first_embedding = df['embedding'].iloc[0]\n",
    "    if isinstance(first_embedding, np.ndarray):\n",
    "        print(f\"‚úÖ Embedding t√≠pusa: {type(first_embedding)}\")\n",
    "        print(f\"‚úÖ Embedding dimenzi√≥ja: {len(first_embedding)}\")\n",
    "        print(f\"‚úÖ Elv√°rt dimenzi√≥: {config.EMBEDDING_DIMENSION}\")\n",
    "        \n",
    "        # L2 normaliz√°l√°s ellen≈ërz√©se\n",
    "        norms = df['embedding'].apply(lambda x: np.linalg.norm(x) if isinstance(x, np.ndarray) else 1.0)\n",
    "        print(f\"\\nNorm√°k statisztik√°i:\")\n",
    "        print(norms.describe())\n",
    "        \n",
    "        # Normaliz√°l√°s ellen≈ërz√©se (agents.md spec szerint k√∂telez≈ë)\n",
    "        normalized_count = norms.apply(lambda x: abs(x - 1.0) < 0.01).sum()\n",
    "        print(f\"L2-normaliz√°lt embeddingek: {normalized_count}/{len(df)} ({100*normalized_count/len(df):.1f}%)\")\n",
    "        \n",
    "        if normalized_count < len(df):\n",
    "            print(\"‚ö†Ô∏è Nem minden embedding van L2-normaliz√°lva - ez probl√©m√°s lehet FAISS IP metrik√°n√°l\")\n",
    "        \n",
    "        # Hi√°nyz√≥ embeddingek\n",
    "        missing_embeddings = df['embedding'].isna().sum()\n",
    "        print(f\"Hi√°nyz√≥ embeddingek: {missing_embeddings}\")\n",
    "        \n",
    "        # Embeddingek k√∂z√∂tti t√°vols√°gok elemz√©se\n",
    "        valid_embeddings = df.dropna(subset=['embedding'])\n",
    "        if len(valid_embeddings) > 100:\n",
    "            X = np.vstack(valid_embeddings['embedding'].values)\n",
    "            \n",
    "            # V√©letlenszer≈±en kiv√°lasztott 1000 p√°r t√°vols√°ga\n",
    "            n_pairs = min(1000, len(X) * (len(X) - 1) // 2)\n",
    "            indices = np.random.choice(len(X), size=min(len(X), 100), replace=False)\n",
    "            \n",
    "            distances = []\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    dist = np.linalg.norm(X[indices[i]] - X[indices[j]])\n",
    "                    distances.append(dist)\n",
    "            \n",
    "            distances = np.array(distances)\n",
    "            print(f\"\\nEmbeddingek k√∂z√∂tti √°tlagos t√°vols√°g: {distances.mean():.4f} ¬± {distances.std():.4f}\")\n",
    "            print(f\"T√°vols√°g tartom√°ny: [{distances.min():.4f}, {distances.max():.4f}]\")\n",
    "            \n",
    "            # Embedding s≈±r≈±s√©g\n",
    "            print(f\"Embedding dimenzi√≥: {X.shape[1]}\")\n",
    "            print(f\"Adatpontok sz√°ma: {len(X)}\")\n",
    "            print(f\"Adatpontok s≈±r≈±s√©ge: {len(X) / X.shape[1]:.2f}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è T√∫l kev√©s embedding a min≈ës√©gi metrik√°khoz: {len(valid_embeddings)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Embedding t√≠pusa nem megfelel≈ë: {type(first_embedding)}\")\n",
    "else:\n",
    "    print(\"‚ùå Nincs embedding adat az elemz√©shez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca-analysis",
   "metadata": {},
   "source": [
    "## 4. PCA Dimenzi√≥cs√∂kkent√©s √©s Vizualiz√°ci√≥\n",
    "\n",
    "Az embeddingek 2D-s lek√©pez√©se PCA seg√≠ts√©g√©vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perform-pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'embedding' in df.columns and embeddings is not None:\n",
    "    # Hi√°nyz√≥ embeddingek elt√°vol√≠t√°sa\n",
    "    valid_df = df.dropna(subset=['embedding']).copy()\n",
    "    \n",
    "    if len(valid_df) > 100:  # Minimum 100 embedding PCA-hoz\n",
    "        print(f\"üìä PCA elemz√©s {len(valid_df)} embeddinggel...\")\n",
    "        \n",
    "        # Embeddingek NumPy t√∂mbb√© alak√≠t√°sa\n",
    "        X = np.vstack(valid_df['embedding'].values)\n",
    "        print(f\"PCA bemenet: {X.shape}\")\n",
    "        \n",
    "        # PCA futtat√°sa\n",
    "        pca = PCA(n_components=2)\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "        \n",
    "        print(f\"PCA els≈ë k√©t komponens varianci√°ja: {pca.explained_variance_ratio_}\")\n",
    "        print(f\"√ñsszes magyar√°zott variancia: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "        \n",
    "        # PCA eredm√©ny vizualiz√°ci√≥\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.scatter(X_reduced[:, 0], X_reduced[:, 1], s=2, alpha=0.6)\n",
    "        plt.title(\"EmbeddingGemma-300m - PCA 2D lek√©pez√©s\")\n",
    "        plt.xlabel(\"F≈ëkomponens 1\")\n",
    "        plt.ylabel(\"F≈ëkomponens 2\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Jogter√ºlet szerinti sz√≠nez√©s\n",
    "        if 'JogTerulet' in valid_df.columns:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            domains = valid_df['JogTerulet'].fillna('ismeretlen').values\n",
    "            scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                                c=valid_df['JogTerulet'].astype('category').cat.codes, \n",
    "                                s=8, alpha=0.6, cmap='tab10')\n",
    "            plt.title(\"EmbeddingGemma Embeddingek jogter√ºlet szerint sz√≠nezve\")\n",
    "            plt.xlabel(\"F≈ëkomponens 1\")\n",
    "            plt.ylabel(\"F≈ëkomponens 2\")\n",
    "            plt.legend(handles=scatter.legend_elements()[0], \n",
    "                      labels=valid_df['JogTerulet'].astype('category').cat.categories.tolist(), \n",
    "                      bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "        \n",
    "        # B√≠r√≥s√°g szerinti sz√≠nez√©s\n",
    "        if 'birosag' in valid_df.columns:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            courts = valid_df['birosag'].fillna('ismeretlen').values\n",
    "            scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                                c=valid_df['birosag'].astype('category').cat.codes, \n",
    "                                s=8, alpha=0.6, cmap='Set3')\n",
    "            plt.title(\"EmbeddingGemma Embeddingek b√≠r√≥s√°g szerint sz√≠nezve\")\n",
    "            plt.xlabel(\"F≈ëkomponens 1\")\n",
    "            plt.ylabel(\"F≈ëkomponens 2\")\n",
    "            plt.legend(handles=scatter.legend_elements()[0], \n",
    "                      labels=valid_df['birosag'].astype('category').cat.categories.tolist(), \n",
    "                      bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "        \n",
    "        # Embedding hossza szerinti sz√≠nez√©s\n",
    "        if 'karakter_szam' in valid_df.columns:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                                c=valid_df['karakter_szam'], s=8, alpha=0.6, cmap='viridis')\n",
    "            plt.title(\"EmbeddingGemma Embeddingek sz√∂veghossz szerint sz√≠nezve\")\n",
    "            plt.xlabel(\"F≈ëkomponens 1\")\n",
    "            plt.ylabel(\"F≈ëkomponens 2\")\n",
    "            plt.colorbar(scatter, label='Karakterek sz√°ma')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è T√∫l kev√©s embedding PCA-hoz: {len(valid_df)}\")\n",
    "else:\n",
    "    print(\"‚ùå Nincs embedding adat a PCA-hoz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-analysis",
   "metadata": {},
   "source": [
    "## 5. Metadatok √©s Embeddingek Kapcsolata\n",
    "\n",
    "Az embeddingek √©s a jogi metadatok k√∂z√∂tti kapcsolat elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-metadata-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'embedding' in df.columns and not df.empty:\n",
    "    valid_df = df.dropna(subset=['embedding'])\n",
    "    \n",
    "    if len(valid_df) > 0:\n",
    "        print(\"üìä Metadatok √©s embeddingek kapcsolata:\")\n",
    "        \n",
    "        # B√≠r√≥s√°g megoszl√°s\n",
    "        if 'birosag' in valid_df.columns:\n",
    "            print(f\"\\nB√≠r√≥s√°gok megoszl√°sa:\")\n",
    "            court_counts = valid_df['birosag'].value_counts()\n",
    "            print(f\"Top 10 b√≠r√≥s√°g: {court_counts.head(10).to_dict()}\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            court_counts.head(10).plot(kind='bar')\n",
    "            plt.title('Top 10 leggyakoribb b√≠r√≥s√°g az embeddingekkel rendelkez≈ë chunkokban')\n",
    "            plt.xlabel('B√≠r√≥s√°g')\n",
    "            plt.ylabel('Chunkok sz√°ma')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(axis='y')\n",
    "            plt.show()\n",
    "        \n",
    "        # Jogter√ºlet megoszl√°s\n",
    "        if 'JogTerulet' in valid_df.columns:\n",
    "            print(f\"\\nJogter√ºletek megoszl√°sa:\")\n",
    "            domain_counts = valid_df['JogTerulet'].value_counts()\n",
    "            print(f\"Top 10 jogter√ºlet: {domain_counts.head(10).to_dict()}\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            domain_counts.head(10).plot(kind='bar')\n",
    "            plt.title('Top 10 leggyakoribb jogter√ºlet az embeddingekkel rendelkez≈ë chunkokban')\n",
    "            plt.xlabel('Jogter√ºlet')\n",
    "            plt.ylabel('Chunkok sz√°ma')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(axis='y')\n",
    "            plt.show()\n",
    "        \n",
    "        # √âv szerinti megoszl√°s\n",
    "        if 'HatarozatEve' in valid_df.columns:\n",
    "            print(f\"\\nHat√°rozatok √©v szerinti megoszl√°sa:\")\n",
    "            year_counts = valid_df['HatarozatEve'].value_counts().sort_index()\n",
    "            print(f\"√âvek tartom√°ny: {year_counts.index.min()} - {year_counts.index.max()}\")\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            year_counts.plot(kind='line', marker='o')\n",
    "            plt.title('Hat√°rozatok eloszl√°sa √©v szerint')\n",
    "            plt.xlabel('√âv')\n",
    "            plt.ylabel('Chunkok sz√°ma')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        \n",
    "        # Sz√∂veghossz eloszl√°s\n",
    "        if 'text' in valid_df.columns:\n",
    "            valid_df['text_length'] = valid_df['text'].astype(str).apply(len)\n",
    "            print(f\"\\nSz√∂veghossz statisztik√°k:\")\n",
    "            print(valid_df['text_length'].describe())\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(valid_df['text_length'], bins=50, alpha=0.7)\n",
    "            plt.title('Chunk sz√∂veghossz eloszl√°sa')\n",
    "            plt.xlabel('Karakterek sz√°ma')\n",
    "            plt.ylabel('Chunkok sz√°ma')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nincs √©rv√©nyes embedding adat a metadatok elemz√©s√©hez\")\n",
    "else:\n",
    "    print(\"‚ùå Nincs adat a metadatok elemz√©s√©hez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faiss-index-analysis",
   "metadata": {},
   "source": [
    "## 6. FAISS Index Tulajdons√°gok\n",
    "\n",
    "Az index t√≠pus√°nak √©s konfigur√°ci√≥j√°nak elemz√©se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-faiss-properties",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index is not None:\n",
    "    print(\"üîç FAISS Index tulajdons√°gok:\")\n",
    "    \n",
    "    print(f\"Index t√≠pusa: {type(index).__name__}\")\n",
    "    print(f\"Vektorok sz√°ma: {index.ntotal}\")\n",
    "    print(f\"Dimenzi√≥: {index.d}\")\n",
    "    \n",
    "    # Index specifikus tulajdons√°gok\n",
    "    if hasattr(index, 'nlist'):\n",
    "        print(f\"IVF lista sz√°m: {index.nlist}\")\n",
    "    if hasattr(index, 'nprobe'):\n",
    "        print(f\"Keres√©si pr√≥b√°k: {index.nprobe}\")\n",
    "    if hasattr(index, 'metric_type'):\n",
    "        print(f\"Metrika t√≠pusa: {index.metric_type}\")\n",
    "    \n",
    "    # Index teljes√≠tm√©ny metrik√°k\n",
    "    print(f\"\\nIndex m√©rete (becs√ºlt): {index.ntotal * index.d * 4 / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Keres√©si sebess√©g becsl√©s (ha van adat)\n",
    "    if embeddings is not None and len(embeddings) > 0:\n",
    "        # Egyszer≈± keres√©si teszt\n",
    "        query_embedding = embeddings[0].reshape(1, -1)\n",
    "        k = 10\n",
    "        \n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        distances, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Keres√©si teljes√≠tm√©ny (1 query, top-{k}): {search_time*1000:.2f}ms\")\n",
    "        print(f\"√Åtlagos t√°vols√°g: {distances[0].mean():.4f}\")\n",
    "        print(f\"T√°vols√°g sz√≥r√°s: {distances[0].std():.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ FAISS index elemz√©s k√©sz\")\n",
    "else:\n",
    "    print(\"‚ùå Nincs FAISS index az elemz√©shez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 7. K√∂vetkeztet√©sek\n",
    "\n",
    "Az embedding ki√©rt√©kel√©s √∂sszefoglal√°sa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-conclusions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FAISS EMBEDDING ELEMZ√âS √ñSSZEFOGLAL√ì ===\")\n",
    "print(\"\\n‚úÖ Sikeresen elemezve:\")\n",
    "if index is not None:\n",
    "    print(f\"   üìä FAISS index: {index.ntotal} vektor, {index.d} dimenzi√≥\")\n",
    "if chunk_id_map is not None:\n",
    "    print(f\"   üó∫Ô∏è Chunk ID mapping: {len(chunk_id_map)} bejegyz√©s\")\n",
    "if df is not None:\n",
    "    print(f\"   üìÑ Chunk adatok: {len(df)} chunk bet√∂ltve\")\n",
    "    if 'embedding' in df.columns:\n",
    "        valid_count = df['embedding'].notna().sum()\n",
    "        print(f\"   üß† Embeddingek: {valid_count}/{len(df)} √©rv√©nyes\")\n",
    "\n",
    "print(\"\\nüìã Agents.md specifik√°ci√≥ ellen≈ërz√©s:\")\n",
    "if index is not None and index.d == config.EMBEDDING_DIMENSION:\n",
    "    print(\"   ‚úÖ Embedding dimenzi√≥ helyes\")\n",
    "else:\n",
    "    print(\"   ‚ùå Embedding dimenzi√≥ elt√©r√©s\")\n",
    "    \n",
    "# L2 normaliz√°l√°s ellen≈ërz√©se\n",
    "if df is not None and 'embedding' in df.columns:\n",
    "    valid_embeddings = df.dropna(subset=['embedding'])\n",
    "    if len(valid_embeddings) > 0:\n",
    "        norms = valid_embeddings['embedding'].apply(lambda x: np.linalg.norm(x))\n",
    "        normalized_count = norms.apply(lambda x: abs(x - 1.0) < 0.01).sum()\n",
    "        if normalized_count >= len(valid_embeddings) * 0.95:  # 95% k√ºsz√∂b\n",
    "            print(\"   ‚úÖ L2 normaliz√°l√°s megfelel≈ë\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è L2 normaliz√°l√°s hi√°nyos: {normalized_count}/{len(valid_embeddings)} ({100*normalized_count/len(valid_embeddings):.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Aj√°nl√°sok:\")\n",
    "if df is not None and 'embedding' in df.columns:\n",
    "    missing = df['embedding'].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"   üîÑ Hi√°nyz√≥ embeddingek √∫jragener√°l√°sa: {missing} chunk\")\n",
    "if index is None:\n",
    "    print(\"   üöÄ FAISS index gener√°l√°sa sz√ºks√©ges: gemma_embedding_runpod.ipynb\")\n",
    "\n",
    "print(\"\\nüéØ Elemz√©s k√©sz - a retrieval rendszer haszn√°latra k√©sz!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
