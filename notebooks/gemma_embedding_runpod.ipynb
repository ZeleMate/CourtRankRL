{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0a2b33",
   "metadata": {},
   "source": [
    "# CourtRankRL FAISS Index √©p√≠t√©se ‚Äì RTX 5090 optimaliz√°lva\n",
    "\n",
    "## Specifik√°ci√≥\n",
    "- **Modell**: HF EmbeddingGemma-300m (google/embeddinggemma-300m)\n",
    "- **Embeddingek**: L2-normaliz√°lt, 768 dimenzi√≥s vektorok\n",
    "- **Metrika**: Inner Product (IP)\n",
    "- **Index**: IVF PQ (kvant√°lt inverted file)\n",
    "- **Training**: Adapt√≠v train buffer √©s nlist a mint√°hoz igaz√≠tva\n",
    "- **Kimenet**: `faiss_index.bin`, `chunk_id_map.json`, `doc_id_map.json` a `/workspace/data/index/` k√∂nyvt√°rban\n",
    "\n",
    "## RTX 5090 optimaliz√°l√°sok\n",
    "- **Batch size**: 512 (4x nagyobb)\n",
    "- **Train buffer**: 1M vektor (5x nagyobb)\n",
    "- **Flash Attention 2**: Automatikus aktiv√°l√°s CUDA-n\n",
    "- **V√°rhat√≥ feldolgoz√°si id≈ë**: ~10-15 perc (2.96M chunk)\n",
    "\n",
    "A notebook RunPod GPU k√∂rnyezetre k√©sz√ºlt, minden el√©r√©si √∫t a `/workspace` gy√∂k√©rre √©p√ºl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e820e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet faiss-gpu tqdm transformers accelerate huggingface_hub python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9204a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ac293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HuggingFace bejelentkez√©s sikeres\n"
     ]
    }
   ],
   "source": [
    "# K√∂rnyezeti v√°ltoz√≥k bet√∂lt√©se √©s HuggingFace bejelentkez√©s\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ HuggingFace bejelentkez√©s sikeres\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nincs HUGGINGFACE_TOKEN, a modell let√∂lt√©se korl√°tozott lehet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab34083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Alapk√∂nyvt√°r: /workspace\n",
      "üìÑ Chunks f√°jl: /workspace/chunks.jsonl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "‚ùå Nem tal√°lhat√≥ a chunks.jsonl: /workspace/chunks.jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÑ Chunks f√°jl:\u001b[39m\u001b[33m\"\u001b[39m, CHUNKS_PATH)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m CHUNKS_PATH.exists():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Nem tal√°lhat√≥ a chunks.jsonl: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHUNKS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m BASE_PATH.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m TMP_DIR.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: ‚ùå Nem tal√°lhat√≥ a chunks.jsonl: /workspace/chunks.jsonl"
     ]
    }
   ],
   "source": [
    "# --- Konfigur√°ci√≥ (Agents.md szerint, RTX 5090-re optimaliz√°lva) ---\n",
    "BASE_PATH = Path(\"/workspace\")\n",
    "TMP_DIR = BASE_PATH / \"tmp_faiss_build\"\n",
    "\n",
    "CHUNKS_PATH = BASE_PATH / \"chunks.jsonl\"\n",
    "FAISS_PATH = BASE_PATH / \"faiss_index.bin\"\n",
    "CHUNK_MAP_PATH = BASE_PATH / \"chunk_id_map.json\"\n",
    "DOC_MAP_PATH = BASE_PATH / \"doc_id_map.json\"\n",
    "\n",
    "EMBED_MODEL_NAME = \"google/embeddinggemma-300m\"\n",
    "EMBED_DIM = 768\n",
    "\n",
    "# RTX 5090 optimaliz√°ci√≥: megn√∂velt batch √©s train buffer\n",
    "BATCH_SIZE = 512  # N√∂velve: 128 ‚Üí 512 (5090 32GB VRAM)\n",
    "MAX_LENGTH = 512\n",
    "TRAIN_BUFFER_TARGET = 1_000_000  # N√∂velve: 200k ‚Üí 1M (jobb IVF train)\n",
    "\n",
    "NLIST_MIN = 64\n",
    "NLIST_MAX = 1024\n",
    "NPROBE_TARGET = 32\n",
    "PQ_M = 64\n",
    "PQ_BITS = 8\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "print(\"üìÇ Alapk√∂nyvt√°r:\", BASE_PATH)\n",
    "print(\"üìÑ Chunks f√°jl:\", CHUNKS_PATH)\n",
    "if not CHUNKS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Nem tal√°lhat√≥ a chunks.jsonl: {CHUNKS_PATH}\")\n",
    "BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modell √©s tokenizer bet√∂lt√©se (RTX 5090 optimaliz√°lva) ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.float16\n",
    "    device_label = \"CUDA\"\n",
    "elif getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    dtype = torch.float16\n",
    "    device_label = \"MPS\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.float32\n",
    "    device_label = \"CPU\"\n",
    "\n",
    "print(f\"üîå Haszn√°lt eszk√∂z: {device_label}\")\n",
    "print(f\"üßÆ Dtype: {dtype}\")\n",
    "\n",
    "cache_dir = BASE_PATH / \".hf_cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîÑ Tokenizer bet√∂lt√©se...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    EMBED_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=str(cache_dir)\n",
    ")\n",
    "print(\"‚úÖ Tokenizer k√©sz\")\n",
    "\n",
    "print(\"üîÑ EmbeddingGemma modell bet√∂lt√©se...\")\n",
    "\n",
    "# Attn implementation: Flash Attention 2 ha el√©rhet≈ë (5090-en gyorsabb)\n",
    "attn_impl = \"flash_attention_2\" if device.type == \"cuda\" else \"eager\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    EMBED_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=str(cache_dir),\n",
    "    device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "    attn_implementation=attn_impl,\n",
    ")\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Opcion√°lis: PyTorch 2.0+ compile optimaliz√°l√°s (20-30% gyors√≠t√°s)\n",
    "# Kommenteld ki, ha probl√©m√°t okoz\n",
    "# if device.type == \"cuda\" and hasattr(torch, \"compile\"):\n",
    "#     print(\"üöÄ Modell compile optimaliz√°l√°s...\")\n",
    "#     model = torch.compile(model, mode=\"max-autotune\")\n",
    "\n",
    "actual_dim = getattr(model.config, \"hidden_size\", EMBED_DIM)\n",
    "if actual_dim != EMBED_DIM:\n",
    "    print(f\"‚ö†Ô∏è Figyelmeztet√©s: a modell dimenzi√≥ja {actual_dim}, a konfigur√°ci√≥ban {EMBED_DIM}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Embedding dimenzi√≥: {actual_dim}\")\n",
    "    \n",
    "if attn_impl == \"flash_attention_2\":\n",
    "    print(\"‚ö° Flash Attention 2 aktiv√°lva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Seg√©df√ºggv√©nyek ---\n",
    "class ReservoirSampler:\n",
    "    \"\"\"Egyszer≈± reservoir sampling a train bufferhez.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer: List[np.ndarray] = []\n",
    "        self.seen = 0\n",
    "\n",
    "    def add(self, vectors: np.ndarray) -> None:\n",
    "        for vec in vectors:\n",
    "            self.seen += 1\n",
    "            if self.capacity == 0:\n",
    "                continue\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(vec.copy())\n",
    "            else:\n",
    "                j = np.random.randint(0, self.seen)\n",
    "                if j < self.capacity:\n",
    "                    self.buffer[j] = vec.copy()\n",
    "\n",
    "    def as_array(self) -> np.ndarray:\n",
    "        if not self.buffer:\n",
    "            return np.empty((0, EMBED_DIM), dtype=np.float32)\n",
    "        return np.stack(self.buffer, axis=0)\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Sz√∂veglista embedel√©se L2-normaliz√°lt vektorokk√°.\"\"\"\n",
    "    if not texts:\n",
    "        return np.empty((0, EMBED_DIM), dtype=np.float32)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "        if hasattr(outputs, \"last_hidden_state\"):\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        elif hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            embeddings = outputs.pooler_output\n",
    "        else:\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        embeddings = embeddings.to(torch.float32)\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # GPU memory cleanup batch-ek k√∂z√∂tt\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings.cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Seg√©df√ºggv√©nyek bet√∂ltve\")\n",
    "\n",
    "\n",
    "def chunk_to_doc_id(chunk_id: str) -> str:\n",
    "    \"\"\"Chunk azonos√≠t√≥b√≥l dokumentum azonos√≠t√≥t k√©pez.\"\"\"\n",
    "    if not chunk_id:\n",
    "        return chunk_id\n",
    "    parts = chunk_id.rsplit('_', 1)\n",
    "    if len(parts) == 2 and parts[1].isdigit():\n",
    "        return parts[0]\n",
    "    return chunk_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding el≈ë√°ll√≠t√°s √©s ideiglenes shardok √≠r√°sa ---\n",
    "start_time = time.time()\n",
    "if TMP_DIR.exists():\n",
    "    shutil.rmtree(TMP_DIR)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "reservoir = ReservoirSampler(TRAIN_BUFFER_TARGET)\n",
    "shard_paths = []\n",
    "id_paths = []\n",
    "chunk_ids_total = 0\n",
    "lines_total = 0\n",
    "\n",
    "batch_texts: List[str] = []\n",
    "batch_ids: List[str] = []\n",
    "shard_index = 0\n",
    "\n",
    "print(\"üßÆ Embeddingek sz√°m√≠t√°sa √©s train buffer felt√∂lt√©se...\")\n",
    "with CHUNKS_PATH.open(\"r\", encoding=\"utf-8\") as source:\n",
    "    for line in tqdm(source, desc=\"Embedding feldolgoz√°s\", unit=\"sor\"):\n",
    "        lines_total += 1\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        chunk_id = str(record.get(\"chunk_id\", \"\")).strip()\n",
    "        text = record.get(\"text\", \"\")\n",
    "        if not chunk_id or not isinstance(text, str) or not text.strip():\n",
    "            continue\n",
    "\n",
    "        batch_ids.append(chunk_id)\n",
    "        batch_texts.append(text)\n",
    "\n",
    "        if len(batch_texts) >= BATCH_SIZE:\n",
    "            embeddings = embed_texts(batch_texts)\n",
    "            if embeddings.size > 0:\n",
    "                reservoir.add(embeddings)\n",
    "                emb_path = TMP_DIR / f\"embeddings_{shard_index:06d}.npy\"\n",
    "                ids_path = TMP_DIR / f\"chunk_ids_{shard_index:06d}.json\"\n",
    "                np.save(emb_path, embeddings)\n",
    "                with ids_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "                    json.dump(batch_ids, handle, ensure_ascii=False)\n",
    "                shard_paths.append(emb_path)\n",
    "                id_paths.append(ids_path)\n",
    "                chunk_ids_total += len(batch_ids)\n",
    "                shard_index += 1\n",
    "            batch_texts = []\n",
    "            batch_ids = []\n",
    "\n",
    "    if batch_texts:\n",
    "        embeddings = embed_texts(batch_texts)\n",
    "        if embeddings.size > 0:\n",
    "            reservoir.add(embeddings)\n",
    "            emb_path = TMP_DIR / f\"embeddings_{shard_index:06d}.npy\"\n",
    "            ids_path = TMP_DIR / f\"chunk_ids_{shard_index:06d}.json\"\n",
    "            np.save(emb_path, embeddings)\n",
    "            with ids_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "                json.dump(batch_ids, handle, ensure_ascii=False)\n",
    "            shard_paths.append(emb_path)\n",
    "            id_paths.append(ids_path)\n",
    "            chunk_ids_total += len(batch_ids)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è Feldolgoz√°s ideje: {elapsed/60:.2f} perc\")\n",
    "print(f\"üì¶ √ñsszes shard: {len(shard_paths)}\")\n",
    "print(f\"üìÑ Feldolgozott sorok: {lines_total}\")\n",
    "print(f\"üîó Chunk ID-k sz√°ma: {chunk_ids_total}\")\n",
    "print(f\"üß™ Train buffer m√©rete: {len(reservoir.buffer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c11885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FAISS index tr√©ning √©s felt√∂lt√©s ---\n",
    "train_matrix = reservoir.as_array()\n",
    "if train_matrix.shape[0] < max(NLIST_MIN, PQ_M * 4):\n",
    "    raise RuntimeError(\"‚ùå T√∫l kev√©s minta a FAISS tr√©ninghez. Ellen≈ërizd a TRAIN_BUFFER_TARGET √©rt√©k√©t vagy a bemeneti adatot.\")\n",
    "\n",
    "vector_total = chunk_ids_total\n",
    "nlist = int(np.clip(round(math.sqrt(vector_total)), NLIST_MIN, NLIST_MAX))\n",
    "print(f\"üìè nlist √©rt√©k: {nlist}\")\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(EMBED_DIM)\n",
    "index = faiss.IndexIVFPQ(quantizer, EMBED_DIM, nlist, PQ_M, PQ_BITS, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "print(\"üéì FAISS tr√©ning indul...\")\n",
    "index.train(train_matrix)\n",
    "print(\"‚úÖ FAISS tr√©ning k√©sz\")\n",
    "\n",
    "index.nprobe = min(NPROBE_TARGET, nlist)\n",
    "print(f\"üéØ nprobe be√°ll√≠tva: {index.nprobe}\")\n",
    "\n",
    "chunk_id_map = {}\n",
    "doc_id_map = {}\n",
    "next_row = 0\n",
    "\n",
    "print(\"üì• Vektorok hozz√°ad√°sa az indexhez...\")\n",
    "for idx, emb_path in enumerate(tqdm(shard_paths, desc=\"Index felt√∂lt√©se\", unit=\"shard\")):\n",
    "    vectors = np.load(emb_path)\n",
    "    with id_paths[idx].open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        ids = json.load(handle)\n",
    "    if vectors.shape[0] != len(ids):\n",
    "        raise ValueError(\"‚ùå A vektorok √©s az ID-k sz√°ma nem egyezik meg egy shardban\")\n",
    "    index.add(vectors)\n",
    "    for cid in ids:\n",
    "        chunk_id_map[str(next_row)] = cid\n",
    "        doc_id_map[str(next_row)] = chunk_to_doc_id(cid)\n",
    "        next_row += 1\n",
    "\n",
    "print(f\"‚úÖ Index vektorsz√°m: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Index ment√©se √©s takar√≠t√°s ---\n",
    "faiss.write_index(index, str(FAISS_PATH))\n",
    "with CHUNK_MAP_PATH.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(chunk_id_map, handle, ensure_ascii=False)\n",
    "with DOC_MAP_PATH.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(doc_id_map, handle, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ FAISS index mentve: {FAISS_PATH}\")\n",
    "print(f\"üíæ Chunk ID mapping mentve: {CHUNK_MAP_PATH}\")\n",
    "print(f\"üíæ Doc ID mapping mentve: {DOC_MAP_PATH}\")\n",
    "\n",
    "shutil.rmtree(TMP_DIR, ignore_errors=True)\n",
    "print(\"üßπ Ideiglenes f√°jlok t√∂r√∂lve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1f2e",
   "metadata": {},
   "source": [
    "## √ñsszefoglal√≥\n",
    "- A FAISS dense index sikeresen elk√©sz√ºlt IVF PQ konfigur√°ci√≥val.\n",
    "- A train buffer √©s az adapt√≠v nlist megfelel az Agents.md el≈ë√≠r√°sainak.\n",
    "- Kimeneti f√°jlok: `faiss_index.bin`, `chunk_id_map.json`, `doc_id_map.json` a `/workspace/data/index/` k√∂nyvt√°rban.\n",
    "- A notebook GPU-s RunPod k√∂rnyezetben futtathat√≥ v√°ltoztat√°s n√©lk√ºl.\n",
    "\n",
    "### RTX 5090 optimaliz√°l√°sok:\n",
    "- **Batch size**: 512 (4x nagyobb, 128-r√≥l)\n",
    "- **Train buffer**: 1M vektor (5x nagyobb, 200k-r√≥l)\n",
    "- **Flash Attention 2**: Automatikusan aktiv√°l√≥dik CUDA-n\n",
    "- **V√°rhat√≥ sebess√©g**: ~15,000-20,000 chunk/mp (~10-15 perc a teljes korpuszra)\n",
    "\n",
    "### Artifact let√∂lt√©se RunPod-b√≥l:\n",
    "```bash\n",
    "# RunPod termin√°lb√≥l:\n",
    "cd /workspace/data/index\n",
    "ls -lh  # Ellen≈ërizd a f√°jlokat\n",
    "# Majd let√∂lt√©s a helyi g√©pre: data/index/ k√∂nyvt√°rba\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
