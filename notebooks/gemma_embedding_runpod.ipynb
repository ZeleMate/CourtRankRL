{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36353e25",
      "metadata": {},
      "source": [
        "# EmbeddingGemma Embedding futtatás RunPod GPU-n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e3526d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install faiss-cpu tqdm transformers accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9ed343",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import math\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# HuggingFace bejelentkezés\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# HF token betöltése .env fájlból\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # .env fájl betöltése\n",
        "\n",
        "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"✅ HuggingFace bejelentkezés sikeres\")\n",
        "else:\n",
        "    print(\"⚠️  HF token nem található - egyes modellek korlátozottak lehetnek\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fccb68fe",
      "metadata": {},
      "source": [
        "## Paraméterek\n",
        "Állítsd be a bemeneti/ kimeneti elérési utakat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e0a893",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Paraméterek beállítása\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m project_root = Path(\u001b[34;43m__file__\u001b[39;49m).parent.parent\n\u001b[32m      4\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "# Paraméterek beállítása - RunPod önálló futtatáshoz\n",
        "# MINDEN elérési út és konfiguráció hardcoded a függetlenség érdekében\n",
        "\n",
        "# Elérési utak (RunPod munkakönyvtárhoz igazítva)\n",
        "chunks_path = Path(\"/workspace/data/processed/chunks.jsonl\")\n",
        "faiss_path = Path(\"/workspace/data/index/faiss_index.bin\")\n",
        "chunk_map_path = Path(\"/workspace/data/index/chunk_id_map.json\")\n",
        "\n",
        "# Modell konfiguráció\n",
        "model_name = \"google/embeddinggemma-300m\"\n",
        "\n",
        "# Memória optimalizált paraméterek\n",
        "batch_size = 32  # GPU memória problémák elkerülésére\n",
        "max_length = 512  # Csökkentett max token hossz\n",
        "\n",
        "print(\"RunPod önálló notebook konfiguráció:\")\n",
        "print(f\"Chunks path: {chunks_path}\")\n",
        "print(f\"FAISS path: {faiss_path}\")\n",
        "print(f\"Chunk map path: {chunk_map_path}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Max length: {max_length}\")\n",
        "print(\"Minden konfiguráció beállítva - notebook önállóan futtatható!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c858d5f",
      "metadata": {},
      "source": [
        "## Modell betöltése\n",
        "Feltételezzük, hogy GPU elérhető (`torch.cuda.is_available()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7c16fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Default érték az embedding_dim-nek\n",
        "embedding_dim = 768  # EmbeddingGemma-300m default dimenziója\n",
        "\n",
        "# Memória optimalizálás\n",
        "if device == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU memória előtte: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")\n",
        "\n",
        "    # Modell betöltése memória optimalizált módon\n",
        "    print(f\"Modell betöltés: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    try:\n",
        "        # Próbáljuk meg az accelerate-t használni (ha telepítve van)\n",
        "        model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            dtype=torch.float16,  # FP16 használata memória csökkentésére\n",
        "            low_cpu_mem_usage=True,    # Alacsony CPU memória használat\n",
        "            device_map=\"auto\"          # Automatikus eszköz elhelyezés\n",
        "        ).to(device)\n",
        "        print(\"Accelerate használata sikeres\")\n",
        "    except (ImportError, ValueError) as e:\n",
        "        # Fallback megoldás ha nincs accelerate\n",
        "        print(f\"Accelerate hiba, fallback megoldás: {e}\")\n",
        "        print(\"Modell betöltés CPU-ra, majd áthelyezés GPU-ra...\")\n",
        "        model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        model = model.to(device)\n",
        "        print(\"Modell sikeresen betöltve GPU-ra\")\n",
        "\n",
        "    model.eval()\n",
        "    try:\n",
        "        embedding_dim = model.config.hidden_size\n",
        "    except:\n",
        "        pass  # Már van default érték beállítva\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"GPU memória utána: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")\n",
        "\n",
        "print(f\"Model device: {device}\")\n",
        "print(f\"Embedding dimension: {embedding_dim}\")\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Batch size: {batch_size}, Max length: {max_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b6e09a9",
      "metadata": {},
      "source": [
        "## Embedding függvény\n",
        "Egyszerű batch feldolgozás GPU-n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee450ffa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed_batch(texts: List[str]) -> np.ndarray:\n",
        "    if not texts:\n",
        "        return np.zeros((0, embedding_dim), dtype=np.float32)\n",
        "\n",
        "    # Memória optimalizálás - kisebb batch-ek kezelése\n",
        "    if len(texts) > batch_size:\n",
        "        # Ha túl nagy a batch, feldaraboljuk\n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            sub_texts = texts[i:i + batch_size]\n",
        "            sub_embeddings = embed_batch(sub_texts)\n",
        "            all_embeddings.append(sub_embeddings)\n",
        "        return np.vstack(all_embeddings)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        if hasattr(outputs, 'last_hidden_state'):\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :].float()\n",
        "        else:\n",
        "            embeddings = outputs.pooler_output.float()\n",
        "\n",
        "    # Memória felszabadítás\n",
        "    del inputs, outputs\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    embeddings = embeddings.cpu().numpy()\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1.0\n",
        "    return embeddings / norms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fb9de8",
      "metadata": {},
      "source": [
        "## FAISS index létrehozása\n",
        "Az index paramétereit egyszerűre vesszük: nlist = sqrt(N), PQ m=64 (igazítva a dimenzióhoz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0eba25e",
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_ids = []\n",
        "index = None\n",
        "embedding_dim = None\n",
        "vector_count = 0\n",
        "\n",
        "with chunks_path.open('r', encoding='utf-8') as handle:\n",
        "    iterator = tqdm(handle, desc='Streaming embedding generálás')\n",
        "    batch_texts = []\n",
        "    batch_ids = []\n",
        "    for line in iterator:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            chunk = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "        chunk_id = str(chunk.get('chunk_id', '')).strip()\n",
        "        text = chunk.get('text', '')\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        if not chunk_id or not text.strip():\n",
        "            continue\n",
        "        batch_ids.append(chunk_id)\n",
        "        batch_texts.append(text)\n",
        "        if len(batch_texts) >= batch_size:\n",
        "            vectors = embed_batch(batch_texts)\n",
        "            if vectors.size > 0:\n",
        "                if index is None:\n",
        "                    embedding_dim = vectors.shape[1]\n",
        "                    index = faiss.IndexFlatIP(embedding_dim)\n",
        "                index.add(vectors.astype(np.float32, copy=False))\n",
        "                chunk_ids.extend(batch_ids)\n",
        "                vector_count += vectors.shape[0]\n",
        "            batch_texts.clear()\n",
        "            batch_ids.clear()\n",
        "    # Maradék batch kezelése\n",
        "    if batch_texts:\n",
        "        vectors = embed_batch(batch_texts)\n",
        "        if vectors.size > 0:\n",
        "            if index is None:\n",
        "                embedding_dim = vectors.shape[1]\n",
        "                index = faiss.IndexFlatIP(embedding_dim)\n",
        "            index.add(vectors.astype(np.float32, copy=False))\n",
        "            chunk_ids.extend(batch_ids)\n",
        "            vector_count += vectors.shape[0]\n",
        "\n",
        "# Ellenőrzés\n",
        "if index is None or index.ntotal == 0:\n",
        "    raise ValueError(\"Hiba: Nem sikerült embedding indexet létrehozni!\")\n",
        "print(f\"Feldolgozott chunk-ok száma: {len(chunk_ids)}\")\n",
        "print(f\"Vektorok száma az indexben: {index.ntotal}\")\n",
        "vector_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87539205",
      "metadata": {},
      "source": [
        "## FAISS paraméterek és index tréning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "addae807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# FAISS paraméterek és index létrehozása\n",
        "if index is None:\n",
        "    raise ValueError(\"Hiba: Az index nem lett inicializálva!\")\n",
        "    \n",
        "nlist = max(1, int(math.sqrt(vector_count)))\n",
        "pq_m = 64\n",
        "if embedding_dim % pq_m != 0:\n",
        "    for candidate in range(pq_m, 0, -1):\n",
        "        if embedding_dim % candidate == 0:\n",
        "            pq_m = candidate\n",
        "            break\n",
        "pq_bits = 8\n",
        "\n",
        "# Vektorok kinyerése az IndexFlatIP-ből\n",
        "print(\"📊 Vektorok kinyerése az IndexFlatIP-ből...\")\n",
        "try:\n",
        "    all_vectors = index.reconstruct_n(0, index.ntotal)\n",
        "except AttributeError as err:\n",
        "    raise AttributeError(\"Ez a FAISS build nem támogatja az 'xb' attribútumot; a reconstruct_n API-t kell használni a vektorok kinyeréséhez.\") from err\n",
        "if not isinstance(all_vectors, np.ndarray):\n",
        "    vector_to_array = getattr(faiss, \"vector_float_to_array\", getattr(faiss, \"vector_to_array\", None))\n",
        "    if vector_to_array is None:\n",
        "        raise RuntimeError(\"Nem található FAISS segédfüggvény a vektorok numpy tömbbé alakításához.\")\n",
        "    all_vectors = vector_to_array(all_vectors)\n",
        "    all_vectors = all_vectors.reshape(index.ntotal, embedding_dim)\n",
        "all_vectors = np.ascontiguousarray(all_vectors, dtype=np.float32)\n",
        "print(f\"✅ Vektorok kinyerve: {all_vectors.shape}\")\n",
        "\n",
        "# Kvantált index létrehozása\n",
        "quantizer = faiss.IndexFlatIP(embedding_dim)\n",
        "new_index = faiss.IndexIVFPQ(quantizer, embedding_dim, nlist, pq_m, pq_bits, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "# Index tréning és feltöltés\n",
        "new_index.train(all_vectors)\n",
        "new_index.add(all_vectors)\n",
        "\n",
        "# Csere\n",
        "index = new_index\n",
        "\n",
        "# Ellenőrzés\n",
        "print(\"Index created with\", len(chunk_ids), \"vectors\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88118295",
      "metadata": {},
      "source": [
        "## Eredmények mentése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed719b30",
      "metadata": {},
      "outputs": [],
      "source": [
        "faiss_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "faiss.write_index(index, str(faiss_path))\n",
        "chunk_id_map = {str(i): cid for i, cid in enumerate(chunk_ids)}\n",
        "with chunk_map_path.open('w', encoding='utf-8') as f:\n",
        "    json.dump(chunk_id_map, f)\n",
        "faiss_path, chunk_map_path"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}