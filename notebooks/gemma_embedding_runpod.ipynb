{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0a2b33",
   "metadata": {},
   "source": [
    "# CourtRankRL FAISS Index Ã©pÃ­tÃ©se â€“ RTX 5090 optimalizÃ¡lva\n",
    "\n",
    "## SpecifikÃ¡ciÃ³\n",
    "- **Modell**: HF EmbeddingGemma-300m (google/embeddinggemma-300m)\n",
    "- **Embeddingek**: L2-normalizÃ¡lt, 768 dimenziÃ³s vektorok\n",
    "- **Metrika**: Inner Product (IP)\n",
    "- **Index**: IVF PQ (kvantÃ¡lt inverted file)\n",
    "- **Training**: AdaptÃ­v train buffer Ã©s nlist a mintÃ¡hoz igazÃ­tva\n",
    "- **Kimenet**: `faiss_index.bin`, `chunk_id_map.json`, `doc_id_map.json` a `/workspace/data/index/` kÃ¶nyvtÃ¡rban\n",
    "\n",
    "## RTX 5090 optimalizÃ¡lÃ¡sok\n",
    "- **Batch size**: 512 (4x nagyobb)\n",
    "- **Train buffer**: 1M vektor (5x nagyobb)\n",
    "- **Flash Attention 2**: Automatikus aktivÃ¡lÃ¡s CUDA-n\n",
    "- **VÃ¡rhatÃ³ feldolgozÃ¡si idÅ‘**: ~10-15 perc (2.96M chunk)\n",
    "\n",
    "A notebook RunPod GPU kÃ¶rnyezetre kÃ©szÃ¼lt, minden elÃ©rÃ©si Ãºt a `/workspace` gyÃ¶kÃ©rre Ã©pÃ¼l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e820e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet faiss-gpu tqdm transformers accelerate huggingface_hub python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9204a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ac293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HuggingFace bejelentkezÃ©s sikeres\n"
     ]
    }
   ],
   "source": [
    "# KÃ¶rnyezeti vÃ¡ltozÃ³k betÃ¶ltÃ©se Ã©s HuggingFace bejelentkezÃ©s\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"âœ… HuggingFace bejelentkezÃ©s sikeres\")\n",
    "else:\n",
    "    print(\"âš ï¸ Nincs HUGGINGFACE_TOKEN, a modell letÃ¶ltÃ©se korlÃ¡tozott lehet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab34083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ AlapkÃ¶nyvtÃ¡r: /workspace\n",
      "ğŸ“„ Chunks fÃ¡jl: /workspace/chunks.jsonl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "âŒ Nem talÃ¡lhatÃ³ a chunks.jsonl: /workspace/chunks.jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“„ Chunks fÃ¡jl:\u001b[39m\u001b[33m\"\u001b[39m, CHUNKS_PATH)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m CHUNKS_PATH.exists():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâŒ Nem talÃ¡lhatÃ³ a chunks.jsonl: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHUNKS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m BASE_PATH.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m TMP_DIR.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: âŒ Nem talÃ¡lhatÃ³ a chunks.jsonl: /workspace/chunks.jsonl"
     ]
    }
   ],
   "source": [
    "# --- KonfigurÃ¡ciÃ³ (Agents.md szerint, RTX 5090-re optimalizÃ¡lva) ---\n",
    "BASE_PATH = Path(\"/workspace\")\n",
    "TMP_DIR = BASE_PATH / \"tmp_faiss_build\"\n",
    "\n",
    "CHUNKS_PATH = BASE_PATH / \"chunks.jsonl\"\n",
    "FAISS_PATH = BASE_PATH / \"faiss_index.bin\"\n",
    "CHUNK_MAP_PATH = BASE_PATH / \"chunk_id_map.json\"\n",
    "DOC_MAP_PATH = BASE_PATH / \"doc_id_map.json\"\n",
    "\n",
    "EMBED_MODEL_NAME = \"google/embeddinggemma-300m\"\n",
    "EMBED_DIM = 768\n",
    "\n",
    "# RTX 5090 optimalizÃ¡ciÃ³: megnÃ¶velt batch Ã©s train buffer\n",
    "BATCH_SIZE = 512  # NÃ¶velve: 128 â†’ 512 (5090 32GB VRAM)\n",
    "MAX_LENGTH = 512\n",
    "TRAIN_BUFFER_TARGET = 1_000_000  # NÃ¶velve: 200k â†’ 1M (jobb IVF train)\n",
    "\n",
    "NLIST_MIN = 64\n",
    "NLIST_MAX = 1024\n",
    "NPROBE_TARGET = 32\n",
    "PQ_M = 64\n",
    "PQ_BITS = 8\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "print(\"ğŸ“‚ AlapkÃ¶nyvtÃ¡r:\", BASE_PATH)\n",
    "print(\"ğŸ“„ Chunks fÃ¡jl:\", CHUNKS_PATH)\n",
    "if not CHUNKS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"âŒ Nem talÃ¡lhatÃ³ a chunks.jsonl: {CHUNKS_PATH}\")\n",
    "BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modell Ã©s tokenizer betÃ¶ltÃ©se (RTX 5090 optimalizÃ¡lva) ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.float16\n",
    "    device_label = \"CUDA\"\n",
    "elif getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    dtype = torch.float16\n",
    "    device_label = \"MPS\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.float32\n",
    "    device_label = \"CPU\"\n",
    "\n",
    "print(f\"ğŸ”Œ HasznÃ¡lt eszkÃ¶z: {device_label}\")\n",
    "print(f\"ğŸ§® Dtype: {dtype}\")\n",
    "\n",
    "cache_dir = BASE_PATH / \".hf_cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ”„ Tokenizer betÃ¶ltÃ©se...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    EMBED_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=str(cache_dir)\n",
    ")\n",
    "print(\"âœ… Tokenizer kÃ©sz\")\n",
    "\n",
    "print(\"ğŸ”„ EmbeddingGemma modell betÃ¶ltÃ©se...\")\n",
    "\n",
    "# Attn implementation: Flash Attention 2 ha elÃ©rhetÅ‘ (5090-en gyorsabb)\n",
    "attn_impl = \"flash_attention_2\" if device.type == \"cuda\" else \"eager\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    EMBED_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=str(cache_dir),\n",
    "    device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "    attn_implementation=attn_impl,\n",
    ")\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# OpcionÃ¡lis: PyTorch 2.0+ compile optimalizÃ¡lÃ¡s (20-30% gyorsÃ­tÃ¡s)\n",
    "# Kommenteld ki, ha problÃ©mÃ¡t okoz\n",
    "# if device.type == \"cuda\" and hasattr(torch, \"compile\"):\n",
    "#     print(\"ğŸš€ Modell compile optimalizÃ¡lÃ¡s...\")\n",
    "#     model = torch.compile(model, mode=\"max-autotune\")\n",
    "\n",
    "actual_dim = getattr(model.config, \"hidden_size\", EMBED_DIM)\n",
    "if actual_dim != EMBED_DIM:\n",
    "    print(f\"âš ï¸ FigyelmeztetÃ©s: a modell dimenziÃ³ja {actual_dim}, a konfigurÃ¡ciÃ³ban {EMBED_DIM}\")\n",
    "else:\n",
    "    print(f\"âœ… Embedding dimenziÃ³: {actual_dim}\")\n",
    "    \n",
    "if attn_impl == \"flash_attention_2\":\n",
    "    print(\"âš¡ Flash Attention 2 aktivÃ¡lva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SegÃ©dfÃ¼ggvÃ©nyek ---\n",
    "class ReservoirSampler:\n",
    "    \"\"\"EgyszerÅ± reservoir sampling a train bufferhez.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer: List[np.ndarray] = []\n",
    "        self.seen = 0\n",
    "\n",
    "    def add(self, vectors: np.ndarray) -> None:\n",
    "        for vec in vectors:\n",
    "            self.seen += 1\n",
    "            if self.capacity == 0:\n",
    "                continue\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(vec.copy())\n",
    "            else:\n",
    "                j = np.random.randint(0, self.seen)\n",
    "                if j < self.capacity:\n",
    "                    self.buffer[j] = vec.copy()\n",
    "\n",
    "    def as_array(self) -> np.ndarray:\n",
    "        if not self.buffer:\n",
    "            return np.empty((0, EMBED_DIM), dtype=np.float32)\n",
    "        return np.stack(self.buffer, axis=0)\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"SzÃ¶veglista embedelÃ©se L2-normalizÃ¡lt vektorokkÃ¡.\"\"\"\n",
    "    if not texts:\n",
    "        return np.empty((0, EMBED_DIM), dtype=np.float32)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "        if hasattr(outputs, \"last_hidden_state\"):\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        elif hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            embeddings = outputs.pooler_output\n",
    "        else:\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        embeddings = embeddings.to(torch.float32)\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # GPU memory cleanup batch-ek kÃ¶zÃ¶tt\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings.cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"âœ… SegÃ©dfÃ¼ggvÃ©nyek betÃ¶ltve\")\n",
    "\n",
    "\n",
    "def chunk_to_doc_id(chunk_id: str) -> str:\n",
    "    \"\"\"Chunk azonosÃ­tÃ³bÃ³l dokumentum azonosÃ­tÃ³t kÃ©pez.\"\"\"\n",
    "    if not chunk_id:\n",
    "        return chunk_id\n",
    "    parts = chunk_id.rsplit('_', 1)\n",
    "    if len(parts) == 2 and parts[1].isdigit():\n",
    "        return parts[0]\n",
    "    return chunk_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding elÅ‘Ã¡llÃ­tÃ¡s Ã©s ideiglenes shardok Ã­rÃ¡sa ---\n",
    "start_time = time.time()\n",
    "if TMP_DIR.exists():\n",
    "    shutil.rmtree(TMP_DIR)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "reservoir = ReservoirSampler(TRAIN_BUFFER_TARGET)\n",
    "shard_paths = []\n",
    "id_paths = []\n",
    "chunk_ids_total = 0\n",
    "lines_total = 0\n",
    "\n",
    "batch_texts: List[str] = []\n",
    "batch_ids: List[str] = []\n",
    "shard_index = 0\n",
    "\n",
    "print(\"ğŸ§® Embeddingek szÃ¡mÃ­tÃ¡sa Ã©s train buffer feltÃ¶ltÃ©se...\")\n",
    "with CHUNKS_PATH.open(\"r\", encoding=\"utf-8\") as source:\n",
    "    for line in tqdm(source, desc=\"Embedding feldolgozÃ¡s\", unit=\"sor\"):\n",
    "        lines_total += 1\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        chunk_id = str(record.get(\"chunk_id\", \"\")).strip()\n",
    "        text = record.get(\"text\", \"\")\n",
    "        if not chunk_id or not isinstance(text, str) or not text.strip():\n",
    "            continue\n",
    "\n",
    "        batch_ids.append(chunk_id)\n",
    "        batch_texts.append(text)\n",
    "\n",
    "        if len(batch_texts) >= BATCH_SIZE:\n",
    "            embeddings = embed_texts(batch_texts)\n",
    "            if embeddings.size > 0:\n",
    "                reservoir.add(embeddings)\n",
    "                emb_path = TMP_DIR / f\"embeddings_{shard_index:06d}.npy\"\n",
    "                ids_path = TMP_DIR / f\"chunk_ids_{shard_index:06d}.json\"\n",
    "                np.save(emb_path, embeddings)\n",
    "                with ids_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "                    json.dump(batch_ids, handle, ensure_ascii=False)\n",
    "                shard_paths.append(emb_path)\n",
    "                id_paths.append(ids_path)\n",
    "                chunk_ids_total += len(batch_ids)\n",
    "                shard_index += 1\n",
    "            batch_texts = []\n",
    "            batch_ids = []\n",
    "\n",
    "    if batch_texts:\n",
    "        embeddings = embed_texts(batch_texts)\n",
    "        if embeddings.size > 0:\n",
    "            reservoir.add(embeddings)\n",
    "            emb_path = TMP_DIR / f\"embeddings_{shard_index:06d}.npy\"\n",
    "            ids_path = TMP_DIR / f\"chunk_ids_{shard_index:06d}.json\"\n",
    "            np.save(emb_path, embeddings)\n",
    "            with ids_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "                json.dump(batch_ids, handle, ensure_ascii=False)\n",
    "            shard_paths.append(emb_path)\n",
    "            id_paths.append(ids_path)\n",
    "            chunk_ids_total += len(batch_ids)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"â±ï¸ FeldolgozÃ¡s ideje: {elapsed/60:.2f} perc\")\n",
    "print(f\"ğŸ“¦ Ã–sszes shard: {len(shard_paths)}\")\n",
    "print(f\"ğŸ“„ Feldolgozott sorok: {lines_total}\")\n",
    "print(f\"ğŸ”— Chunk ID-k szÃ¡ma: {chunk_ids_total}\")\n",
    "print(f\"ğŸ§ª Train buffer mÃ©rete: {len(reservoir.buffer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c11885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FAISS index trÃ©ning Ã©s feltÃ¶ltÃ©s ---\n",
    "train_matrix = reservoir.as_array()\n",
    "if train_matrix.shape[0] < max(NLIST_MIN, PQ_M * 4):\n",
    "    raise RuntimeError(\"âŒ TÃºl kevÃ©s minta a FAISS trÃ©ninghez. EllenÅ‘rizd a TRAIN_BUFFER_TARGET Ã©rtÃ©kÃ©t vagy a bemeneti adatot.\")\n",
    "\n",
    "vector_total = chunk_ids_total\n",
    "nlist = int(np.clip(round(math.sqrt(vector_total)), NLIST_MIN, NLIST_MAX))\n",
    "print(f\"ğŸ“ nlist Ã©rtÃ©k: {nlist}\")\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(EMBED_DIM)\n",
    "index = faiss.IndexIVFPQ(quantizer, EMBED_DIM, nlist, PQ_M, PQ_BITS, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "print(\"ğŸ“ FAISS trÃ©ning indul...\")\n",
    "index.train(train_matrix)\n",
    "print(\"âœ… FAISS trÃ©ning kÃ©sz\")\n",
    "\n",
    "index.nprobe = min(NPROBE_TARGET, nlist)\n",
    "print(f\"ğŸ¯ nprobe beÃ¡llÃ­tva: {index.nprobe}\")\n",
    "\n",
    "chunk_id_map = {}\n",
    "doc_id_map = {}\n",
    "next_row = 0\n",
    "\n",
    "print(\"ğŸ“¥ Vektorok hozzÃ¡adÃ¡sa az indexhez...\")\n",
    "for idx, emb_path in enumerate(tqdm(shard_paths, desc=\"Index feltÃ¶ltÃ©se\", unit=\"shard\")):\n",
    "    vectors = np.load(emb_path)\n",
    "    with id_paths[idx].open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        ids = json.load(handle)\n",
    "    if vectors.shape[0] != len(ids):\n",
    "        raise ValueError(\"âŒ A vektorok Ã©s az ID-k szÃ¡ma nem egyezik meg egy shardban\")\n",
    "    index.add(vectors)\n",
    "    for cid in ids:\n",
    "        chunk_id_map[str(next_row)] = cid\n",
    "        doc_id_map[str(next_row)] = chunk_to_doc_id(cid)\n",
    "        next_row += 1\n",
    "\n",
    "print(f\"âœ… Index vektorszÃ¡m: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Index mentÃ©se Ã©s takarÃ­tÃ¡s ---\n",
    "faiss.write_index(index, str(FAISS_PATH))\n",
    "with CHUNK_MAP_PATH.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(chunk_id_map, handle, ensure_ascii=False)\n",
    "with DOC_MAP_PATH.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(doc_id_map, handle, ensure_ascii=False)\n",
    "\n",
    "print(f\"ğŸ’¾ FAISS index mentve: {FAISS_PATH}\")\n",
    "print(f\"ğŸ’¾ Chunk ID mapping mentve: {CHUNK_MAP_PATH}\")\n",
    "print(f\"ğŸ’¾ Doc ID mapping mentve: {DOC_MAP_PATH}\")\n",
    "\n",
    "shutil.rmtree(TMP_DIR, ignore_errors=True)\n",
    "print(\"ğŸ§¹ Ideiglenes fÃ¡jlok tÃ¶rÃ¶lve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1f2e",
   "metadata": {},
   "source": [
    "## Ã–sszefoglalÃ³\n",
    "- A FAISS dense index sikeresen elkÃ©szÃ¼lt IVF PQ konfigurÃ¡ciÃ³val.\n",
    "- A train buffer Ã©s az adaptÃ­v nlist megfelel az Agents.md elÅ‘Ã­rÃ¡sainak.\n",
    "- Kimeneti fÃ¡jlok: `faiss_index.bin`, `chunk_id_map.json`, `doc_id_map.json` a `/workspace/data/index/` kÃ¶nyvtÃ¡rban.\n",
    "- A notebook GPU-s RunPod kÃ¶rnyezetben futtathatÃ³ vÃ¡ltoztatÃ¡s nÃ©lkÃ¼l.\n",
    "\n",
    "### RTX 5090 optimalizÃ¡lÃ¡sok:\n",
    "- **Batch size**: 512 (4x nagyobb, 128-rÃ³l)\n",
    "- **Train buffer**: 1M vektor (5x nagyobb, 200k-rÃ³l)\n",
    "- **Flash Attention 2**: Automatikusan aktivÃ¡lÃ³dik CUDA-n\n",
    "- **VÃ¡rhatÃ³ sebessÃ©g**: ~15,000-20,000 chunk/mp (~10-15 perc a teljes korpuszra)\n",
    "\n",
    "### Artifact letÃ¶ltÃ©se RunPod-bÃ³l:\n",
    "```bash\n",
    "# RunPod terminÃ¡lbÃ³l:\n",
    "cd /workspace/data/index\n",
    "ls -lh  # EllenÅ‘rizd a fÃ¡jlokat\n",
    "# Majd letÃ¶ltÃ©s a helyi gÃ©pre: data/index/ kÃ¶nyvtÃ¡rba\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
