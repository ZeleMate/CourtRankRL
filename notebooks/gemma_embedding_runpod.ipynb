{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CourtRankRL Embedding Generation - Sentence Transformers (RTX 5090 GPU)\n",
        "\n",
        "## Specifikáció\n",
        "- **Modell**: google/embeddinggemma-300m via Sentence Transformers (>=5.1.0)\n",
        "- **Input**: chunks.jsonl (processed court decisions)\n",
        "- **Output**: embeddings.npy (float32, L2-normalized) és embedding_chunk_ids.json\n",
        "- **Környezet**: RunPod RTX 5090 GPU (24GB VRAM)\n",
        "- **Batch size**: 512 (GPU optimalizált)\n",
        "- **Kritikus**: CSAK Sentence Transformers - AutoModel ZERO VECTOR-t produkál!\n",
        "\n",
        "## Prompt-ok (automatikus Sentence Transformers-ben)\n",
        "- **Document chunks**: `prompt_name=\"document\"` → \"title: none | text: {chunk_text}\"\n",
        "- **Query**: `prompt_name=\"query\"` → \"task: search result | query: {query_text}\"\n",
        "- **Normalization**: `normalize_embeddings=True` (automatikus L2-normalizálás)\n",
        "\n",
        "## Memória kezelés\n",
        "- Shard-okba írás nagy dataset-ekhez\n",
        "- Konszolidáció a végén\n",
        "- GPU memória monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd5c1a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# KRITIKUS: Csak Sentence Transformers - AutoModel ZERO VECTOR-t produkál!\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "from typing import List\n",
        "import torch\n",
        "\n",
        "# --- Konfiguráció ---\n",
        "BASE_PATH = Path(\"/workspace\")\n",
        "CHUNKS_PATH = BASE_PATH / \"chunks.jsonl\"\n",
        "EMBEDDINGS_PATH = BASE_PATH / \"embeddings.npy\"\n",
        "CHUNK_IDS_PATH = BASE_PATH / \"embedding_chunk_ids.json\"\n",
        "\n",
        "# GPU konfiguráció RTX 5090-hez\n",
        "BATCH_SIZE = 512\n",
        "MAX_SEQ_LENGTH = 2048  # EmbeddingGemma default\n",
        "SHARD_SIZE = 100_000  # Shard-okba írás\n",
        "\n",
        "# HF token (környezeti változóból)\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError(\"❌ HF_TOKEN környezeti változó hiányzik!\")\n",
        "\n",
        "print(f\"🚀 RTX 5090 Embedding Generation indul...\")\n",
        "print(f\"📂 Base path: {BASE_PATH}\")\n",
        "print(f\"📄 Input chunks: {CHUNKS_PATH}\")\n",
        "print(f\"📄 Output embeddings: {EMBEDDINGS_PATH}\")\n",
        "print(f\"📄 Output chunk IDs: {CHUNK_IDS_PATH}\")\n",
        "\n",
        "# --- Chunks betöltés ---\n",
        "def load_chunks(chunks_path: Path) -> List[dict]:\n",
        "    \"\"\"Chunks betöltése JSONL-ből\"\"\"\n",
        "    chunks = []\n",
        "    print(f\"📥 Chunks betöltése: {chunks_path}\")\n",
        "    \n",
        "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            if line_num % 100_000 == 0:\n",
        "                print(f\"  📊 {line_num:,} chunks feldolgozva...\")\n",
        "            try:\n",
        "                chunk = json.loads(line.strip())\n",
        "                chunks.append(chunk)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"⚠️ Hibás JSON {line_num}-ban: {e}\")\n",
        "                continue\n",
        "    \n",
        "    print(f\"✅ {len(chunks):,} chunks betöltve\")\n",
        "    return chunks\n",
        "\n",
        "chunks = load_chunks(CHUNKS_PATH)\n",
        "chunk_texts = [chunk['text'] for chunk in chunks]\n",
        "chunk_ids = [chunk['chunk_id'] for chunk in chunks]\n",
        "\n",
        "print(f\"📊 Összes chunk: {len(chunk_texts):,}\")\n",
        "print(f\"💾 Memória használat: {psutil.virtual_memory().used / 1024**3:.1f}GB\")\n",
        "\n",
        "# --- Modell betöltés (CSAK Sentence Transformers!) ---\n",
        "print(\"🤖 EmbeddingGemma modell betöltése (Sentence Transformers)...\")\n",
        "\n",
        "try:\n",
        "    model = SentenceTransformer(\n",
        "        \"google/embeddinggemma-300m\",\n",
        "        token=HF_TOKEN,\n",
        "        trust_remote_code=True,\n",
        "        cache_dir=\"/tmp/hf_cache\"\n",
        "    )\n",
        "    print(\"✅ Modell betöltve (Sentence Transformers)\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Modell betöltési hiba: {e}\")\n",
        "    raise\n",
        "\n",
        "# GPU beállítások\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "    print(f\"✅ GPU elérhető: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"💾 GPU memória: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️ GPU nem elérhető - CPU használata (lassú!)\")\n",
        "\n",
        "# --- Embedding generálás ---\n",
        "def generate_embeddings(model, texts: List[str], batch_size: int, device) -> np.ndarray:\n",
        "    \"\"\"Batch-es embedding generálás\"\"\"\n",
        "    print(f\"🔄 Embedding generálás indul: {len(texts):,} texts\")\n",
        "    \n",
        "    all_embeddings = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Batch-ek feldolgozása\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        batch_start = time.time()\n",
        "        \n",
        "        try:\n",
        "            # KRITIKUS: Sentence Transformers encode() használata\n",
        "            # Automatikus prompt: \"title: none | text: {chunk}\"\n",
        "            # Automatikus L2-normalizálás\n",
        "            batch_embeddings = model.encode(\n",
        "                batch_texts,\n",
        "                prompt_name=\"document\",  # Document prompt\n",
        "                normalize_embeddings=True,  # L2-normalizálás\n",
        "                batch_size=batch_size,\n",
        "                convert_to_numpy=True,\n",
        "                device=device,\n",
        "                show_progress_bar=True\n",
        "                # dtype=torch.float32 ELTÁVOLÍTVA - nem támogatott paraméter!\n",
        "            )\n",
        "            \n",
        "            all_embeddings.append(batch_embeddings)\n",
        "            \n",
        "            # GPU memória monitoring\n",
        "            if device.type == 'cuda':\n",
        "                mem_used = torch.cuda.memory_allocated() / 1024**3\n",
        "                print(f\"  📊 Batch {i//batch_size + 1} kész | GPU memória: {mem_used:.1f}GB\")\n",
        "            \n",
        "            batch_time = time.time() - batch_start\n",
        "            print(f\"  ⏱️ Batch idő: {batch_time:.2f}s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Batch hiba {i//batch_size + 1}-nál: {e}\")\n",
        "            raise\n",
        "    \n",
        "    # Konszolidáció\n",
        "    embeddings = np.vstack(all_embeddings)\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"✅ Embedding generálás kész: {embeddings.shape}\")\n",
        "    print(f\"⏱️ Teljes idő: {total_time:.2f}s\")\n",
        "    print(f\"⚡ Sebesség: {len(texts)/total_time:.1f} texts/sec\")\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Sanity check: Ellenőrizzük hogy nincsenek zero vector-ok\n",
        "def validate_embeddings(embeddings: np.ndarray, chunk_ids: List[str]) -> tuple:\n",
        "    \"\"\"Embedding validáció és tisztítás\"\"\"\n",
        "    print(\"🔍 Embedding validáció...\")\n",
        "    \n",
        "    # NaN/Inf ellenőrzés\n",
        "    finite_mask = np.isfinite(embeddings).all(axis=1)\n",
        "    print(f\"  📊 Finite embeddings: {finite_mask.sum():,}/{len(embeddings):,}\")\n",
        "    \n",
        "    # Zero norma ellenőrzés\n",
        "    norms = np.linalg.norm(embeddings, axis=1)\n",
        "    nonzero_mask = norms > 1e-6\n",
        "    print(f\"  📊 Non-zero norm embeddings: {nonzero_mask.sum():,}/{len(embeddings):,}\")\n",
        "    \n",
        "    # Kombinált maszk\n",
        "    valid_mask = finite_mask & nonzero_mask\n",
        "    num_invalid = (~valid_mask).sum()\n",
        "    \n",
        "    if num_invalid > 0:\n",
        "        print(f\"⚠️ {num_invalid:,} invalid embedding kiszűrve\")\n",
        "        embeddings = embeddings[valid_mask]\n",
        "        chunk_ids = [cid for cid, keep in zip(chunk_ids, valid_mask) if keep]\n",
        "    else:\n",
        "        print(\"✅ Minden embedding valid\")\n",
        "    \n",
        "    return embeddings, chunk_ids\n",
        "\n",
        "# Generálás\n",
        "embeddings = generate_embeddings(model, chunk_texts, BATCH_SIZE, device)\n",
        "\n",
        "# Validáció és tisztítás\n",
        "embeddings, chunk_ids = validate_embeddings(embeddings, chunk_ids)\n",
        "\n",
        "# Végső dtype biztosítás\n",
        "if embeddings.dtype != np.float32:\n",
        "    embeddings = embeddings.astype(np.float32)\n",
        "\n",
        "print(f\"🎯 Végső alak: {embeddings.shape}\")\n",
        "print(f\"💾 Memória használat: {embeddings.nbytes / 1024**3:.2f}GB\")\n",
        "\n",
        "# --- Mentés shard-okba (nagy dataset-ekhez) ---\n",
        "def save_sharded_embeddings(embeddings: np.ndarray, chunk_ids: List[str], \n",
        "                           shard_size: int, output_path: Path, ids_path: Path):\n",
        "    \"\"\"Shard-okba mentés memória optimalizáláshoz\"\"\"\n",
        "    print(f\"💾 Shard-okba mentés (shard size: {shard_size:,})...\")\n",
        "    \n",
        "    os.makedirs(output_path.parent, exist_ok=True)\n",
        "    os.makedirs(ids_path.parent, exist_ok=True)\n",
        "    \n",
        "    # Shard-ok létrehozása\n",
        "    for i in range(0, len(embeddings), shard_size):\n",
        "        shard_idx = i // shard_size\n",
        "        end_idx = min(i + shard_size, len(embeddings))\n",
        "        \n",
        "        shard_embeddings = embeddings[i:end_idx]\n",
        "        shard_ids = chunk_ids[i:end_idx]\n",
        "        \n",
        "        shard_path = output_path.parent / f\"embeddings_shard_{shard_idx}.npy\"\n",
        "        ids_shard_path = output_path.parent / f\"chunk_ids_shard_{shard_idx}.json\"\n",
        "        \n",
        "        np.save(shard_path, shard_embeddings)\n",
        "        with open(ids_shard_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(shard_ids, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        print(f\"  💾 Shard {shard_idx} mentve: {len(shard_embeddings):,} embeddings\")\n",
        "    \n",
        "    # Konszolidált fájlok létrehozása\n",
        "    print(\"🔄 Konszolidáció shard-okból...\")\n",
        "    \n",
        "    # Embeddings összevonása\n",
        "    consolidated_embeddings = []\n",
        "    consolidated_ids = []\n",
        "    \n",
        "    shard_idx = 0\n",
        "    while True:\n",
        "        shard_path = output_path.parent / f\"embeddings_shard_{shard_idx}.npy\"\n",
        "        ids_shard_path = output_path.parent / f\"chunk_ids_shard_{shard_idx}.json\"\n",
        "        \n",
        "        if not shard_path.exists():\n",
        "            break\n",
        "            \n",
        "        shard_emb = np.load(shard_path)\n",
        "        with open(ids_shard_path, 'r', encoding='utf-8') as f:\n",
        "            shard_ids = json.load(f)\n",
        "        \n",
        "        consolidated_embeddings.append(shard_emb)\n",
        "        consolidated_ids.extend(shard_ids)\n",
        "        \n",
        "        # Tisztítás\n",
        "        os.remove(shard_path)\n",
        "        os.remove(ids_shard_path)\n",
        "        \n",
        "        shard_idx += 1\n",
        "    \n",
        "    # Végső konszolidáció\n",
        "    final_embeddings = np.vstack(consolidated_embeddings)\n",
        "    print(f\"✅ Konszolidált: {final_embeddings.shape}\")\n",
        "    \n",
        "    # Mentés\n",
        "    np.save(output_path, final_embeddings)\n",
        "    with open(ids_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(consolidated_ids, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"💾 Konszolidált fájlok mentve\")\n",
        "\n",
        "# Mentés\n",
        "save_sharded_embeddings(embeddings, chunk_ids, SHARD_SIZE, EMBEDDINGS_PATH, CHUNK_IDS_PATH)\n",
        "\n",
        "# --- Végső validáció ---\n",
        "print(\"🔍 Végső validáció...\")\n",
        "\n",
        "# Betöltés ellenőrzés\n",
        "loaded_embeddings = np.load(EMBEDDINGS_PATH)\n",
        "with open(CHUNK_IDS_PATH, 'r', encoding='utf-8') as f:\n",
        "    loaded_ids = json.load(f)\n",
        "\n",
        "# Ellenőrzések\n",
        "assert loaded_embeddings.shape[0] == len(loaded_ids), \"ID és embedding szám nem egyezik\"\n",
        "assert loaded_embeddings.shape[1] == 768, f\"Embedding dim nem 768: {loaded_embeddings.shape[1]}\"\n",
        "assert loaded_embeddings.dtype == np.float32, f\"Dtype nem float32: {loaded_embeddings.dtype}\"\n",
        "\n",
        "# NaN/Inf ellenőrzés\n",
        "assert np.isfinite(loaded_embeddings).all(), \"NaN/Inf az embeddingekben!\"\n",
        "assert np.all(np.linalg.norm(loaded_embeddings, axis=1) > 0), \"Zero-norm embeddings!\"\n",
        "\n",
        "print(\"✅ Végső validáció sikeres\")\n",
        "print(f\"📊 Végső statisztikák:\")\n",
        "print(f\"  • Embeddings: {loaded_embeddings.shape}\")\n",
        "print(f\"  • Chunk IDs: {len(loaded_ids)}\")\n",
        "print(f\"  • Memória: {loaded_embeddings.nbytes / 1024**3:.2f}GB\")\n",
        "\n",
        "# --- Összefoglaló ---\n",
        "print(\"=\"*80)\n",
        "print(\"🎉 EMBEDDING GENERÁLÁS SIKERES!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"📄 Kimeneti fájlok:\")\n",
        "print(f\"  • {EMBEDDINGS_PATH}\")\n",
        "print(f\"  • {CHUNK_IDS_PATH}\")\n",
        "print(f\"📊 Vektorok: {loaded_embeddings.shape[0]:,}\")\n",
        "print(f\"🎯 Dimenzió: {loaded_embeddings.shape[1]}\")\n",
        "print(f\"💾 Fájlméret: {EMBEDDINGS_PATH.stat().st_size / 1024**3:.2f}GB\")\n",
        "print(f\"⏱️ Futtatási idő: {time.time() - time.time():.2f}s\")  # TODO: Track total time\n",
        "\n",
        "print(\"\n",
        "🚀 Következő lépés: Töltsd le az artifact-okat és futtasd a faiss_index_builder.ipynb-t\")\n",
        "print(\"💡 Tipp: Ellenőrizd hogy minden embedding L2-normalizált és finite!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487faf93",
      "metadata": {},
      "source": [
        "## Használat\n",
        "\n",
        "\n",
        "\n",
        "## Kritikus megjegyzések\n",
        "\n",
        "1. **CSAK Sentence Transformers!** AutoModel használata zero-vector-t produkál\n",
        "2. **normalize_embeddings=True** kötelező az L2-normalizáláshoz\n",
        "3. **float32** használata - EmbeddingGemma nem támogatja float16-ot\n",
        "4. **prompt_name=\"document\"** automatikusan hozzáadja a megfelelő prompt-ot\n",
        "5. **Validáció** minden embedding-re - zero vector-ok azonnali detektálása\n",
        "6. **dtype paraméter ELTÁVOLÍTVA** - Sentence Transformers nem támogatja ezt a paramétert"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
