{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36353e25",
      "metadata": {},
      "source": [
        "# EmbeddingGemma Embedding futtat√°s RunPod GPU-n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e3526d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/zelenyianszkimate/Documents/CourtRankRL/.venv/bin/python: No module named pip\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install faiss-cpu tqdm transformers accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9ed343",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import math\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# HuggingFace bejelentkez√©s\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# HF token bet√∂lt√©se .env f√°jlb√≥l\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # .env f√°jl bet√∂lt√©se\n",
        "\n",
        "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"‚úÖ HuggingFace bejelentkez√©s sikeres\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  HF token nem tal√°lhat√≥ - egyes modellek korl√°tozottak lehetnek\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fccb68fe",
      "metadata": {},
      "source": [
        "## Param√©terek\n",
        "√Åll√≠tsd be a bemeneti/ kimeneti el√©r√©si utakat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e0a893",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Param√©terek be√°ll√≠t√°sa\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m project_root = Path(\u001b[34;43m__file__\u001b[39;49m).parent.parent\n\u001b[32m      4\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "# Param√©terek be√°ll√≠t√°sa - RunPod √∂n√°ll√≥ futtat√°shoz\n",
        "# MINDEN el√©r√©si √∫t √©s konfigur√°ci√≥ hardcoded a f√ºggetlens√©g √©rdek√©ben\n",
        "\n",
        "# El√©r√©si utak (RunPod munkak√∂nyvt√°rhoz igaz√≠tva)\n",
        "chunks_path = Path(\"/workspace/data/processed/chunks.jsonl\")\n",
        "faiss_path = Path(\"/workspace/data/index/faiss_index.bin\")\n",
        "chunk_map_path = Path(\"/workspace/data/index/chunk_id_map.json\")\n",
        "\n",
        "# Modell konfigur√°ci√≥\n",
        "model_name = \"google/embeddinggemma-300m\"\n",
        "\n",
        "# Mem√≥ria optimaliz√°lt param√©terek\n",
        "batch_size = 32  # GPU mem√≥ria probl√©m√°k elker√ºl√©s√©re\n",
        "max_length = 512  # Cs√∂kkentett max token hossz\n",
        "\n",
        "print(\"RunPod √∂n√°ll√≥ notebook konfigur√°ci√≥:\")\n",
        "print(f\"Chunks path: {chunks_path}\")\n",
        "print(f\"FAISS path: {faiss_path}\")\n",
        "print(f\"Chunk map path: {chunk_map_path}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Max length: {max_length}\")\n",
        "print(\"Minden konfigur√°ci√≥ be√°ll√≠tva - notebook √∂n√°ll√≥an futtathat√≥!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c858d5f",
      "metadata": {},
      "source": [
        "## Modell bet√∂lt√©se\n",
        "Felt√©telezz√ºk, hogy GPU el√©rhet≈ë (`torch.cuda.is_available()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7c16fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Default √©rt√©k az embedding_dim-nek\n",
        "embedding_dim = 768  # EmbeddingGemma-300m default dimenzi√≥ja\n",
        "\n",
        "# Mem√≥ria optimaliz√°l√°s\n",
        "if device == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU mem√≥ria el≈ëtte: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")\n",
        "\n",
        "    # Modell bet√∂lt√©se mem√≥ria optimaliz√°lt m√≥don\n",
        "    print(f\"Modell bet√∂lt√©s: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    try:\n",
        "        # Pr√≥b√°ljuk meg az accelerate-t haszn√°lni (ha telep√≠tve van)\n",
        "        model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            dtype=torch.float16,  # FP16 haszn√°lata mem√≥ria cs√∂kkent√©s√©re\n",
        "            low_cpu_mem_usage=True,    # Alacsony CPU mem√≥ria haszn√°lat\n",
        "            device_map=\"auto\"          # Automatikus eszk√∂z elhelyez√©s\n",
        "        ).to(device)\n",
        "        print(\"Accelerate haszn√°lata sikeres\")\n",
        "    except (ImportError, ValueError) as e:\n",
        "        # Fallback megold√°s ha nincs accelerate\n",
        "        print(f\"Accelerate hiba, fallback megold√°s: {e}\")\n",
        "        print(\"Modell bet√∂lt√©s CPU-ra, majd √°thelyez√©s GPU-ra...\")\n",
        "        model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        model = model.to(device)\n",
        "        print(\"Modell sikeresen bet√∂ltve GPU-ra\")\n",
        "\n",
        "    model.eval()\n",
        "    try:\n",
        "        embedding_dim = model.config.hidden_size\n",
        "    except:\n",
        "        pass  # M√°r van default √©rt√©k be√°ll√≠tva\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"GPU mem√≥ria ut√°na: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")\n",
        "\n",
        "print(f\"Model device: {device}\")\n",
        "print(f\"Embedding dimension: {embedding_dim}\")\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Batch size: {batch_size}, Max length: {max_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b6e09a9",
      "metadata": {},
      "source": [
        "## Embedding f√ºggv√©ny\n",
        "Egyszer≈± batch feldolgoz√°s GPU-n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee450ffa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed_batch(texts: List[str]) -> np.ndarray:\n",
        "    if not texts:\n",
        "        return np.zeros((0, embedding_dim), dtype=np.float32)\n",
        "\n",
        "    # Mem√≥ria optimaliz√°l√°s - kisebb batch-ek kezel√©se\n",
        "    if len(texts) > batch_size:\n",
        "        # Ha t√∫l nagy a batch, feldaraboljuk\n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            sub_texts = texts[i:i + batch_size]\n",
        "            sub_embeddings = embed_batch(sub_texts)\n",
        "            all_embeddings.append(sub_embeddings)\n",
        "        return np.vstack(all_embeddings)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        if hasattr(outputs, 'last_hidden_state'):\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :].float()\n",
        "        else:\n",
        "            embeddings = outputs.pooler_output.float()\n",
        "\n",
        "    # Mem√≥ria felszabad√≠t√°s\n",
        "    del inputs, outputs\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    embeddings = embeddings.cpu().numpy()\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1.0\n",
        "    return embeddings / norms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fb9de8",
      "metadata": {},
      "source": [
        "## FAISS index l√©trehoz√°sa\n",
        "Az index param√©tereit egyszer≈±re vessz√ºk: nlist = sqrt(N), PQ m=64 (igaz√≠tva a dimenzi√≥hoz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0eba25e",
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_ids = []\n",
        "index = None\n",
        "embedding_dim = None\n",
        "vector_count = 0\n",
        "\n",
        "with chunks_path.open('r', encoding='utf-8') as handle:\n",
        "    iterator = tqdm(handle, desc='Streaming embedding gener√°l√°s')\n",
        "    batch_texts = []\n",
        "    batch_ids = []\n",
        "    for line in iterator:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            chunk = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "        chunk_id = str(chunk.get('chunk_id', '')).strip()\n",
        "        text = chunk.get('text', '')\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        if not chunk_id or not text.strip():\n",
        "            continue\n",
        "        batch_ids.append(chunk_id)\n",
        "        batch_texts.append(text)\n",
        "        if len(batch_texts) >= batch_size:\n",
        "            vectors = embed_batch(batch_texts)\n",
        "            if vectors.size > 0:\n",
        "                if index is None:\n",
        "                    embedding_dim = vectors.shape[1]\n",
        "                    index = faiss.IndexFlatIP(embedding_dim)\n",
        "                index.add(vectors.astype(np.float32, copy=False))\n",
        "                chunk_ids.extend(batch_ids)\n",
        "                vector_count += vectors.shape[0]\n",
        "            batch_texts.clear()\n",
        "            batch_ids.clear()\n",
        "    # Marad√©k batch kezel√©se\n",
        "    if batch_texts:\n",
        "        vectors = embed_batch(batch_texts)\n",
        "        if vectors.size > 0:\n",
        "            if index is None:\n",
        "                embedding_dim = vectors.shape[1]\n",
        "                index = faiss.IndexFlatIP(embedding_dim)\n",
        "            index.add(vectors.astype(np.float32, copy=False))\n",
        "            chunk_ids.extend(batch_ids)\n",
        "            vector_count += vectors.shape[0]\n",
        "\n",
        "# Ellen≈ërz√©s\n",
        "if index is None or index.ntotal == 0:\n",
        "    raise ValueError(\"Hiba: Nem siker√ºlt embedding indexet l√©trehozni!\")\n",
        "print(f\"Feldolgozott chunk-ok sz√°ma: {len(chunk_ids)}\")\n",
        "print(f\"Vektorok sz√°ma az indexben: {index.ntotal}\")\n",
        "vector_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87539205",
      "metadata": {},
      "source": [
        "## FAISS param√©terek √©s index tr√©ning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "addae807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# FAISS param√©terek √©s index l√©trehoz√°sa\n",
        "if index is None:\n",
        "    raise ValueError(\"Hiba: Az index nem lett inicializ√°lva!\")\n",
        "    \n",
        "nlist = max(1, int(math.sqrt(vector_count)))\n",
        "pq_m = 64\n",
        "if embedding_dim % pq_m != 0:\n",
        "    for candidate in range(pq_m, 0, -1):\n",
        "        if embedding_dim % candidate == 0:\n",
        "            pq_m = candidate\n",
        "            break\n",
        "pq_bits = 8\n",
        "\n",
        "# Vektorok kinyer√©se az IndexFlatIP-b≈ël\n",
        "print(\"üìä Vektorok kinyer√©se az IndexFlatIP-b≈ël...\")\n",
        "try:\n",
        "    all_vectors = index.reconstruct_n(0, index.ntotal)\n",
        "except AttributeError as err:\n",
        "    raise AttributeError(\"Ez a FAISS build nem t√°mogatja az 'xb' attrib√∫tumot; a reconstruct_n API-t kell haszn√°lni a vektorok kinyer√©s√©hez.\") from err\n",
        "if not isinstance(all_vectors, np.ndarray):\n",
        "    vector_to_array = getattr(faiss, \"vector_float_to_array\", getattr(faiss, \"vector_to_array\", None))\n",
        "    if vector_to_array is None:\n",
        "        raise RuntimeError(\"Nem tal√°lhat√≥ FAISS seg√©df√ºggv√©ny a vektorok numpy t√∂mbb√© alak√≠t√°s√°hoz.\")\n",
        "    all_vectors = vector_to_array(all_vectors)\n",
        "    all_vectors = all_vectors.reshape(index.ntotal, embedding_dim)\n",
        "all_vectors = np.ascontiguousarray(all_vectors, dtype=np.float32)\n",
        "print(f\"‚úÖ Vektorok kinyerve: {all_vectors.shape}\")\n",
        "\n",
        "# Kvant√°lt index l√©trehoz√°sa\n",
        "quantizer = faiss.IndexFlatIP(embedding_dim)\n",
        "new_index = faiss.IndexIVFPQ(quantizer, embedding_dim, nlist, pq_m, pq_bits, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "# Index tr√©ning √©s felt√∂lt√©s\n",
        "new_index.train(all_vectors)\n",
        "new_index.add(all_vectors)\n",
        "\n",
        "# Csere\n",
        "index = new_index\n",
        "\n",
        "# Ellen≈ërz√©s\n",
        "print(\"Index created with\", len(chunk_ids), \"vectors\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88118295",
      "metadata": {},
      "source": [
        "## Eredm√©nyek ment√©se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed719b30",
      "metadata": {},
      "outputs": [],
      "source": [
        "faiss_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "faiss.write_index(index, str(faiss_path))\n",
        "chunk_id_map = {str(i): cid for i, cid in enumerate(chunk_ids)}\n",
        "with chunk_map_path.open('w', encoding='utf-8') as f:\n",
        "    json.dump(chunk_id_map, f)\n",
        "faiss_path, chunk_map_path"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}