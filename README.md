# CourtRankRL ‚Äì Magyar b√≠r√≥s√°gi hat√°rozatok hibrid visszakeres√©se RL‚Äëalap√∫ √∫jrarangsorol√°ssal

## √Åttekint√©s

Compute‚Äëlight, lok√°lisan futtathat√≥ pipeline magyar b√≠r√≥s√°gi hat√°rozatokra. A rendszer Doclinggel feldolgozza a DOCX f√°jlokat, chunkol, BM25S √©s FAISS indexet √©p√≠t, hibrid (sparse+dense) visszakeres√©st v√©gez RRF f√∫zi√≥val. A lek√©rdez√©sek kimenete kiz√°r√≥lag azonos√≠t√≥kb√≥l √°ll√≥ lista (doc_id), magyar nyelv≈± k√≠s√©r≈ësz√∂veg n√©lk√ºl.

**Architekt√∫ra:**
- **Lok√°lis pipeline:** BM25S + EmbeddingGemma FAISS + hybrid retrieval (M3 MacBook Air 16GB RAM)
- **Cloud-only GRPO:** Reinforcement learning reranking Qwen3-4B-Instruct modellel (RunPod GPU)

F≈ë komponensek (high‚Äëlevel)
- Docling feldolgoz√°s √©s minim√°l normaliz√°l√°s
- Chunkol√°s Docling intelligens szegment√°l√°s√°val
- BM25S (sparse) index native tokenizer-rel
- FAISS (dense) index EmbeddingGemma-300m modellel (L2-normalized, IP metric)
- Hibrid visszakeres√©s RRF f√∫zi√≥val
- GRPO √∫jrarangsorol√°s (cloud-only, TRL GRPOTrainer, QLoRA adapters)

## Telep√≠t√©s

1) UV k√∂rnyezet be√°ll√≠t√°sa
- `uv sync` (telep√≠ti a f√ºgg≈ës√©geket √©s virtu√°lis k√∂rnyezetet)
- `source .venv/bin/activate` (aktiv√°lja a virtu√°lis k√∂rnyezetet, ha sz√ºks√©ges; uv automatikusan kezeli)

2) K√∂rnyezeti v√°ltoz√≥k (`.env` a projekt gy√∂ker√©ben)
- `HUGGINGFACE_TOKEN=hf_...`  (EmbeddingGemma bet√∂lt√©shez √©s query embedding-hez)

Megjegyz√©s: a projekt minden felhaszn√°l√≥i kimenete magyar nyelv≈±; a query v√°lasz kiz√°r√≥lag azonos√≠t√≥k list√°ja.

## Gyors haszn√°lat

### üñ•Ô∏è Lok√°lis futtat√°s (CLI)

```bash
# 1. Build pipeline (Docling ‚Üí chunking ‚Üí BM25S)
uv run courtrankrl build

# 2. FAISS index gener√°l√°s (RunPod GPU-n)
# Futtassa: notebooks/gemma_embedding_runpod.ipynb

# 3. Keres√©s (baseline only, agents.md szerint)
uv run courtrankrl query "csal√°di jogi √ºgy"

# 4. GRPO slate export (cloud training el≈ëk√©sz√≠t√©s)
uv run courtrankrl train

# 5. GRPO training (RunPod GPU-n)
# Futtassa: notebooks/grpo_train_runpod.ipynb
```

**Megjegyz√©s:** GRPO reranking csak cloud-on (agents.md spec szerint). Lok√°lis query csak baseline-t ad vissza.

### ‚òÅÔ∏è RunPod Cloud GPU futtat√°s

A projekt **k√©t cloud notebook-ot** tartalmaz RunPod GPU-ra optimaliz√°lva:

#### 1. FAISS Embedding Index (`gemma_embedding_runpod.ipynb`)
```bash
# GPU: A100/H100/RTX 5090 (24GB+ VRAM aj√°nlott)
# Input: /workspace/chunks.jsonl
# Output: /workspace/faiss_index.bin, /workspace/chunk_id_map.json

# Model: google/embeddinggemma-300m
# Optimaliz√°ci√≥k: FP16, Flash Attention 2, PyTorch compile
```

#### 2. GRPO Training (`grpo_train_runpod.ipynb`)
```bash
# GPU: RTX 5090 (24GB VRAM) - optimaliz√°lt konfigur√°ci√≥
# Input: /workspace/training_slates.jsonl (98 query √ó 20 chunk/slate)
# Output: /workspace/artifacts/grpo_policy/ (LoRA adapters + metrics)

# Model: Qwen/Qwen3-4B-Instruct-2507 (4-bit) + QLoRA (rank=64, alpha=128)
# Training: TRL GRPOTrainer GRPO algoritmus
#   - Loss: dapo (eliminates length bias)
#   - Reward scaling: batch (robust - PPO Lite)
#   - Hardware: batch_size=2, grad_accumulation=2, 6 generations/prompt
#   - Training time: ~45-60 perc (500 steps)
```

**El≈ëny√∂k RunPod-on:**
- ‚ö° **GPU gyors√≠t√°s**: 4B parameter model training
- üîÑ **Streaming feldolgoz√°s**: 3M+ chunk biztons√°gos kezel√©se
- üì¶ **√ñn√°ll√≥ notebookok**: K√∂rnyezeti v√°ltoz√≥kb√≥l konfigur√°lhat√≥
- üß† **Mem√≥ria optimaliz√°lt**: 4-bit quantization, bf16 compute

## Futtat√°s ‚Äì R√©szletes build l√©p√©sek

### 1. Lok√°lis build pipeline

```bash
uv run courtrankrl build
```

**Mit csin√°l:**
- Docling parsing: `data/raw/` DOCX ‚Üí plain text
- Normaliz√°l√°s √©s metadata extraction (court, domain, year)
- Intelligens chunking (Docling capabilites)
- BM25S index √©p√≠t√©s native tokenizer-rel
- Kimenetek: `data/processed/chunks.jsonl`, `data/index/bm25/bm25s_model/`

### 2. Cloud FAISS embedding gener√°l√°s (k√∂telez≈ë)

```bash
# RunPod GPU-n futtassa: notebooks/gemma_embedding_runpod.ipynb
# Bemenet: chunks.jsonl
# Kimenetek: faiss_index.bin, chunk_id_map.json
# T√∂ltse le lok√°lisan: data/index/ k√∂nyvt√°rba
```

### 3. Cloud GRPO training (opcion√°lis)

```bash
# 1. Slate export lok√°lisan
uv run courtrankrl train

# 2. RunPod GPU-n futtassa: notebooks/grpo_train_runpod.ipynb
# Bemenet: training_slates.jsonl
# Kimenetek: grpo_policy/ (LoRA adapters + metrics.json)
```

## Lek√©rdez√©s (hibrid baseline)

```bash
# Alap√©rtelmezett keres√©s (RRF fusion)
uv run courtrankrl query "k√°rt√©r√≠t√©s szivatty√∫ √ºgy"

# T√∂bb tal√°lat k√©r√©se
uv run courtrankrl query "szerz≈ëd√©ses jog" --top-k 20

# P√©lda csal√°di jogi √ºgyre
uv run courtrankrl query "csal√°di jogi √ºgy"
```

**HybridRetriever m≈±k√∂d√©s:**
1. Query embedding: EmbeddingGemma-300m (MPS acceleration)
2. BM25S sparse retrieval: chunk-level ‚Üí document-level max-score aggregation
3. FAISS dense retrieval: IndexIVFFlat (100% exact distances, ~9GB RAM for 3M vectors), L2-normalized IP search
4. Fusion: RRF (Reciprocal Rank Fusion) - robusztus, param√©ter-mentes algoritmus
5. Kimenet: Top-k document ID lista

**El≈ëfelt√©tel:** FAISS index l√©tezik (`gemma_embedding_runpod.ipynb`)

**Megjegyz√©s:** GRPO reranking cloud-only (agents.md spec). Lok√°lis query csak baseline-t ad vissza.

## GRPO √∫jrarangsorol√°s (cloud-only, agents.md szerint)

### Slate export (lok√°lisan)
```bash
uv run courtrankrl train
# Output: data/models/grpo_policy/training_slates.jsonl
```

### GRPO training (RunPod GPU-n)
```bash
# Futtassa: notebooks/grpo_train_runpod.ipynb
# Qrels form√°tum: data/qrels/baseline_qrels.tsv
# - Header: query_id\tdoc_id\trelevance
# - Doc IDs: chunks.jsonl doc_id mez≈ëb≈ël (NEM chunk_id!)
# - Relevance: {0, 1, 2}
```

**GRPO konfigur√°ci√≥ (RTX 5090 optimaliz√°lt):**
- Model: Qwen/Qwen3-4B-Instruct-2507 (4-bit) + QLoRA (rank=64, alpha=128, 7 target modules)
- Dataset: 98 query (teljes), 20 chunk/slate, teljes chunk sz√∂veg
- Trainer: TRL GRPOTrainer GRPO algoritmus (loss_type="dapo", scale_rewards="batch")
- Reward: nDCG@10 difference + entropy bonus (0.01), clipping [-1.0, 1.0]
- Hardware: RTX 5090 - batch_size=2, grad_accumulation=2, 6 generations, 500 steps
- Training time: ~45-60 perc
- Output: LoRA adapter weights + metrics.json

**Megjegyz√©s:** Lok√°lis inference nem t√°mogatott (4B model t√∫l nagy 16GB RAM-hoz).

## Artefaktumok √©s el√©r√©si utak

### Lok√°lis artifactok
- Chunks: `data/processed/chunks.jsonl`
- Processed docs: `data/processed/processed_docs.jsonl`
- BM25S index: `data/index/bm25/bm25s_model/` (corpus, vocab, params, indices)
- BM25 stats: `data/index/bm25/bm25_stats.json`
- BM25 chunk IDs: `data/index/bm25/chunk_ids.json`
- Token cache: `data/index/bm25/token_cache/` (token_ids.npy, vocab.json)

### Cloud-r√≥l let√∂ltend≈ë artifactok
- FAISS index: `data/index/faiss_index.bin` (gemma_embedding_runpod.ipynb)
- Chunk ID map: `data/index/chunk_id_map.json` (gemma_embedding_runpod.ipynb)
- GRPO adapters: `data/models/grpo_policy/` (grpo_train_runpod.ipynb)
- GRPO metrics: `data/models/grpo_policy/metrics.json` (grpo_train_runpod.ipynb)

### Qrels
- Format: `data/qrels/baseline_qrels.tsv` (TSV, header, doc_id-k)

## Konfigur√°ci√≥ (`configs/config.py`)

### Retrieval
- **BM25S**: `BM25_K1=1.5`, `BM25_B=0.75`, `BM25_USE_NUMBA`, `BM25_THREADS`
- **Hybrid**: `TOP_K_BASELINE=100`, `TOP_K_RERANKED=20`, `RRF_K=60`
- **FAISS**: `FAISS_NLIST_MIN=64`, `FAISS_NLIST_MAX=1024`

### Memory
- **Batch sizes**: `CHUNK_WRITE_BATCH_SIZE=200`
- **Soft limit**: `MEMORY_SOFT_LIMIT_BYTES=12GB`

### GRPO (RTX 5090 cloud-only)
- **Slate config**: `GRPO_SLATE_SIZE=20` (chunk-based)
- **LoRA config**: `GRPO_LORA_RANK=64`, `GRPO_LORA_ALPHA=128`
- **Training config**: `GRPO_MAX_STEPS=500`, `GRPO_BATCH_SIZE=2`, `GRPO_NUM_GENERATIONS=6`
- **Paths**: `SLATE_EXPORT_PATH`, `QRELS_FILE`

## Hibaelh√°r√≠t√°s

### Lok√°lis probl√©m√°k
- **FAISS index hi√°nyzik**: Futtassa `gemma_embedding_runpod.ipynb` RunPod-on, t√∂ltse le az artifactokat
- **MPS acceleration**: M3 MacBook Air automatikusan haszn√°lja, ha el√©rhet≈ë
- **BM25 index build lass√∫**: √Åll√≠tsa `BM25_USE_NUMBA=True` √©s `BM25_THREADS=-1`
- **Mem√≥ria hiba**: Cs√∂kkentse `CHUNK_WRITE_BATCH_SIZE` vagy `sample_size` √©rt√©k√©t

### Cloud probl√©m√°k
- **CUDA OOM (embedding)**: Cs√∂kkentse `BATCH_SIZE` √©rt√©k√©t 512‚Üí256‚Üí128
- **CUDA OOM (GRPO)**: N√∂velje `GRADIENT_ACCUMULATION_STEPS` 4‚Üí8
- **Slow training**: Ellen≈ërizze Flash Attention 2 aktiv√°l√°s√°t
- **Qrels format error**: Ellen≈ërizze TSV header-t √©s doc_id-kat (nem chunk_id!)

## Szakdolgozati elemz√©s

```bash
# Adatelemz≈ë notebook futtat√°sa
jupyter notebook notebooks/data_analysis.ipynb
```

**Tartalom:**
- Sz√∂veghossz √©s strukt√∫ra elemz√©s (chunk/document szint)
- Metaadat eloszl√°sok (b√≠r√≥s√°g, jogter√ºlet, √©v)
- Sz√≥k√©szlet √©s nyelvi jellemz≈ëk (top words, Zipf-t√∂rv√©ny)
- FAISS embedding elemz√©s (norma, dimenzi√≥-szint≈± stats)
- Professzion√°lis √°br√°k √©s t√°bl√°zatok

## Nyelvi ir√°nyelv

- **agents.md √©s README.md**: Angol
- **Minden m√°s (CLI, notebook output, kommentek)**: Magyar
- **Query output**: Csak doc_id lista, magyar√°z√≥ sz√∂veg n√©lk√ºl

---

**K√©sz√≠tette:** Zelenyi√°nszki M√°t√©  
**Implement√°ci√≥:** Python 3.11, Hugging Face (EmbeddingGemma, Qwen3), FAISS, BM25S, TRL, Docling  
**Optimaliz√°lva:** M3 MacBook Air 16GB RAM (lok√°lis), RunPod GPU (cloud)
